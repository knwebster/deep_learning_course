{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 8: Normalising flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Change of variables formula](#changeofvariables)\n",
    "\n",
    "[3. NICE / RealNVP](#nicerealnvp)\n",
    "\n",
    "[4. Affine Coupling Layer (*)](#affine_coupling_layer)\n",
    "\n",
    "[5. Glow](#glow)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "So far in this module we have covered many of the fundamental building blocks of deep learning: from mathematical neurons and multilayer perceptrons, to optimisation and regularisation of deep learning models, and important architectures such as convolutional and recurrent neural networks.\n",
    "\n",
    "In the remaining weeks of the module, we will use these building blocks to focus our attention on the probabilistic approach to deep learning. This is a branch of deep learning that aims to make use of tools from probability theory to account for noise and uncertainty in the data. Probabilistic deep learning models make direct use of probability distributions and latent random variables in the model architecture.\n",
    "\n",
    "In this week of the course we will develop normalising flow deep learning models. Normalising flows are a class of generative models, that were first popularised in the context of variational inference by [Rezende & Mohamed 2015](#Rezende15), and in the context of density estimation by [Dinh et al 2015](#Dinh15). In this week, we will focus on using normalising flows to estimate continuous data distributions.\n",
    "\n",
    "When trained as a density estimator, this type of model is able to produce new instances that could plausibly have come from the same dataset that it is trained on, as well as tell you whether a given example instance is likely. However, for complex datasets the data distribution can be very difficult to model, so this is a highly nontrivial task in general. This is where the power of deep learning models can be leveraged to learn highly multimodal and complicated data distributions, and this type of model has been successfully applied to domains such as image generation ([Ho et al 2019](#Ho19)), noise modelling ([Abdelhamed et al 2019](#Abdelhamed19)), audio synthesis ([Prenger et al 2019](#Prenger19)), and video generation ([Kumar et al 2019](#Kumar19)).\n",
    "\n",
    "In this week, we'll be going through a popular class of normalising flow models, which are known as the NICE ([Dinh 2015](#Dinh15)), RealNVP ([Dinh et al 2017](#Dinh17)) and Glow ([Kingma et al 2018](#Kingma18)) models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"changeofvariables\"></a>\n",
    "## Change of variables formula\n",
    "\n",
    "The approach taken by normalising flows to solve the density estimation task is to take an initial, simple density, and transform it - possibly using a series of parameterised transformations - to produce a rich and complex distribution. \n",
    "\n",
    "If these transformations are smooth and invertible, then we are able to evaluate the density of the complex transformed distribution. This property is important, because it then allows to train such a model using maximum likelihood. This is the idea behind normalising flows. \n",
    "\n",
    "We'll start this week by reviewing the change of variables formula, which forms the mathematical basis of normalising flows.\n",
    "\n",
    "#### Statement of the formula\n",
    "Let $Z := (z_1,\\ldots,z_D)\\in\\mathbb{R}^D$ be a $D$-dimensional continuous random variable, and suppose that $f:\\mathbb{R}^D\\mapsto\\mathbb{R}^D$ is a smooth, invertible transformation. Now consider the change of variables $X = f(Z)$, with $X=(x_1,\\ldots,x_D)$, and denote the probability density functions of the random variables $Z$ and $X$ by $p_Z$ and $p_X$ respectively.\n",
    "\n",
    "The change of variables formula states that\n",
    "\n",
    "$$\n",
    "p_X(\\mathbf{x}) = p_Z(\\mathbf{z})\\cdot\\left|\\det J_f(\\mathbf{z}) \\right|^{-1},\\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}, \\mathbf{z}\\in\\mathbb{R}^D$, and $J_f(\\mathbf{z})\\in\\mathbb{R}^{D\\times D}$ is the **Jacobian** of the transformation $f$, given by the matrix of partial derivatives\n",
    "\n",
    "$$\n",
    "J_f(\\mathbf{z}) = \\left[ \n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial f_1}{\\partial z_1}(\\mathbf{z}) & \\cdots & \\frac{\\partial f_1}{\\partial z_D}(\\mathbf{z})\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f_D}{\\partial z_1}(\\mathbf{z}) & \\cdots & \\frac{\\partial f_D}{\\partial z_d}(\\mathbf{z})\\\\\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "and $\\left|\\det J_f(\\mathbf{z}) \\right|$ is the absolute value of the determinant of the Jacobian matrix. Note that (1) can also be written in the log-form\n",
    "\n",
    "$$\n",
    "\\log p_X(\\mathbf{x}) = \\log p_Z(\\mathbf{z}) - \\log \\hspace{0.1ex}\\left|\\det J_f(\\mathbf{z}) \\right|. \\tag{2}\n",
    "$$\n",
    "\n",
    "Furthermore, we can equivalently consider the transformation $Z = f^{-1}(X)$. Then the change of variables formulae can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_Z(\\mathbf{z}) &= p_X(\\mathbf{x})\\cdot\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right|^{-1},\\tag{3}\\\\\n",
    "\\log p_Z(\\mathbf{z}) &= \\log p_X(\\mathbf{x}) - \\log \\hspace{0.1ex}\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right|.\\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### A simple example\n",
    "We will demonstrate the change of variables formula with a simple example. Let $Z=(z_1, z_2)$ be a 2-dimensional random variable that is uniformly distributed on the unit square $[0, 1]^2 =: \\Omega_Z$. We also define the transformation $f:\\mathbb{R}^2 \\mapsto \\mathbb{R}^2$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(z_1, z_2) = (\\lambda z_1, \\mu z_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for some nonzero $\\lambda, \\mu\\in\\mathbb{R}$. The random variable $X=(x_1, x_2)$ is given by $X = f(Z)$. \n",
    "\n",
    "<center><img src=\"figures/change_of_variables.png\" alt=\"Change of variables example in 2D\" style=\"width: 750px;\"/></center>\n",
    "<center>Linearly transformed uniformly distributed random variable</center>\n",
    "<br>\n",
    "\n",
    "Since $\\int_{\\Omega_Z}p_Z(\\mathbf{z})d\\mathbf{z} = 1$ and $\\mathbf{z}$ is uniformly distributed, we have that \n",
    "\n",
    "$$\n",
    "p_Z(\\mathbf{z}) = 1 \\quad\\text{for}\\quad \\mathbf{z}\\in\\Omega_Z.\n",
    "$$\n",
    "\n",
    "The random variable $X$ is uniformly distributed on the region $\\Omega_X = f(\\Omega_Z)$ as shown in the figure above (for the case $\\lambda, \\mu>0$). Since again $\\int_{\\Omega_X}p_X(\\mathbf{x})d\\mathbf{x} = 1$, the probability density function for $X$ must be given by \n",
    "\n",
    "$$\n",
    "p_X(\\mathbf{x}) = \\frac{1}{|\\Omega_X|} = \\frac{1}{|\\lambda\\mu |}\\quad\\text{for}\\quad \\mathbf{x}\\in\\Omega_X.\n",
    "$$\n",
    "\n",
    "This result corresponds to the equations $(1)-(4)$ above. In this simple example, the transformation $f$ is linear, and the Jacobian matrix is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_f(\\mathbf{z}) = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\lambda & 0\\\\\n",
    "0 & \\mu\n",
    "\\end{array}\n",
    "\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The absolute value of the determinant is $\\left|\\det J_{f^{-1}}(\\mathbf{x}) \\right| = |\\lambda\\mu | \\ne 0$. Equation $(1)$ then implies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_X(\\mathbf{x}) &= p_Z(\\mathbf{z})\\cdot\\left|\\det J_f(\\mathbf{z}) \\right|^{-1}\\\\\n",
    "&= \\frac{1}{|\\lambda\\mu|}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Writing in the log-form as in equation $(2)$ gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_X(\\mathbf{x}) &= \\log p_Z(\\mathbf{z}) - \\log \\hspace{0.1ex}\\left|\\det J_f(\\mathbf{z}) \\right|\\\\\n",
    "&= \\log (1) - \\log |\\lambda\\mu|\\\\\n",
    "&= - \\log |\\lambda\\mu|.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Sketch of proof in 1-D\n",
    "We now provide a sketch of the proof of the change of variables formula in one dimension. Let $Z$ and $X$ be random variables such that $X = f(Z)$, where $f : \\mathbb{R}\\mapsto\\mathbb{R}$ is a $C^k$ diffeomorphism with $k\\ge 1$. The change of variables formula in one dimension can be written\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(z)\\cdot\\left| \\frac{d}{dz}f(z) \\right|^{-1},\\qquad\\text{(cf. equation $(1)$)}\n",
    "$$\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(z)\\cdot\\left| \\frac{d}{dx}f^{-1}(x) \\right|.\\qquad\\text{(cf. equation $(3)$)}\n",
    "$$\n",
    "\n",
    "_Sketch of proof._ For $f$ to be invertible, it must be strictly monotonic. That means that for all $x^{(1)}, x^{(2)}\\in\\mathbb{R}$ with $x^{(1)} < x^{(2)}$, we have $f(x^{(1)}) < f(x^{(2)})$ (strictly monotonically increasing) or $f(x^{(1)}) > f(x^{(2)})$ (strictly monotonically decreasing).\n",
    "\n",
    "<center><img src=\"figures/change_of_variables_monotonic.png\" alt=\"Monotonic functions\" style=\"width: 600px;\"/></center>\n",
    "<center>Sketch of monotonic functions: (a) strictly increasing, (b) strictly decreasing</center>\n",
    "<br>\n",
    "\n",
    "Suppose first that $f$ is strictly increasing. Also let $F_X$ and $F_Z$ be the cumulative distribution functions of the random variables $X$ and $Z$ respectively. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_X(x) &= P(X \\le x)\\\\\n",
    "&= P(f(Z) \\le x)\\\\\n",
    "&= P(Z \\le f^{-1}(x))\\qquad\\text{(since $f$ is monotonically increasing)}\\\\\n",
    "&= F_Z(f^{-1}(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By differentiating on both sides with respect to $x$, we obtain the probability density function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_X(x) &= \\frac{d}{dx}F_X(x)\\\\\n",
    "&= \\frac{d}{dx} F_Z(f^{-1}(x))\\\\\n",
    "&= \\frac{d}{dz}F_Z(z)\\cdot\\frac{d}{dx}f^{-1}(x)\\\\\n",
    "&= p_Z(z)\\frac{d}{dx}f^{-1}(x) \\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now suppose first that $f$ is strictly decreasing. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_X(x) &= P(X \\le x)\\\\\n",
    "&= P(f(Z) \\le x)\\\\\n",
    "&= P(Z \\ge f^{-1}(x))\\qquad\\text{(since $f$ is monotonically decreasing)}\\\\\n",
    "&= 1 - F_Z(f^{-1}(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Again differentiating on both sides with respect to $x$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_X(x) &= \\frac{d}{dx}F_X(x)\\\\\n",
    "&= -\\frac{d}{dx} F_Z(f^{-1}(x))\\\\\n",
    "&= -F_Z'(f^{-1}(x))\\frac{d}{dx}f^{-1}(x)\\\\\n",
    "&= -p_Z(z)\\frac{d}{dx}f^{-1}(x) \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now note that the inverse of a strictly monotonically increasing (resp. decreasing) function is again strictly monotonically increasing (resp. decreasing). This implies that the quantity $\\frac{d}{dx} f^{-1}(x)$ is positive in $(5)$ and negative in $(6)$, and so these two equations can be combined into the single equation:\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(z)\\left|\\frac{d}{dx}f^{-1}(x)\\right|\n",
    "$$\n",
    "\n",
    "which completes the proof.\n",
    "\n",
    "#### Application to normalising flows\n",
    "Normalising flows are a class of models that exploit the change of variables formula to estimate an unknown target data density. \n",
    "\n",
    "Suppose we have data samples $\\mathcal{D}:=\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\}$, with each $\\mathbf{x}^{(i)}\\in\\mathbb{R}^D$, and assume that these samples are generated i.i.d. from the underlying distribution $p_X$. \n",
    "\n",
    "A normalising flow models the distribution $p_X$ using a random variable $Z$ (also of dimension $D$) with a simple distribution $p_Z$ (e.g. an isotropic Gaussian), such that the random variable $X$ can be written as a change of variables $X = f_\\theta(Z)$, where $\\theta$ is a parameter vector that parameterises the smooth invertible function $f_\\theta$. \n",
    "\n",
    "The function $f_\\theta$ is modelled using a neural network with parameters $\\theta$, which we want to learn from the data. An important point is that this neural network must be designed to be invertible, which is not the case in general with deep learning models. In practice, we often construct the neural network by composing multiple simpler blocks together. \n",
    "\n",
    "We use the principle of maximum likelihood to learn the optimal parameters $\\theta$; that is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{ML} &:= \\arg \\max_{\\theta} P(\\mathcal{D}; \\theta)\\\\\n",
    "&= \\arg \\max_{\\theta} \\log P(\\mathcal{D}; \\theta).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In order to compute $\\log P(\\mathcal{D}; \\theta)$ we can use the change of variables formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathcal{D}; \\theta) &= \\prod_{\\mathbf{x}\\in\\mathcal{D}}  p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot\\left|\\hspace{0.1ex}\\det J_{f_\\theta^{-1}}(\\mathbf{x}) \\hspace{0.1ex}\\right|\\\\\n",
    "\\log P(\\mathcal{D}; \\theta) &= \\sum_{x\\in\\mathcal{D}} \\log p_Z(f_\\theta^{-1}(\\mathbf{x})) + \\log \\hspace{0.1ex}\\left|\\hspace{0.1ex}\\det J_{f_\\theta^{-1}}(\\mathbf{x}) \\hspace{0.1ex}\\right|\\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term $p_Z(f_\\theta^{-1}(\\mathbf{x}))$ can be computed for a given data point $\\mathbf{x}\\in\\mathcal{D}$ since the neural network $f_\\theta$ is designed to be invertible, and the distribution $p_Z$ is known. The term $\\det J_{f_\\theta^{-1}}(\\mathbf{x})$ is also computable, although this also highlights another important aspect of normalising flow models: they should be designed such that the determinant of the Jacobian can be efficiently computed.\n",
    "\n",
    "The log-likelihood $(7)$ is usually optimised as usual in minibatches, with gradient-based optimisation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"nicerealnvp\"></a>\n",
    "## NICE / RealNVP\n",
    "\n",
    "#### NICE\n",
    "NICE stands for \"nonlinear independent components estimation\", and is a deep learning architecture framework for density estimation tasks. A key motivation for the proposed framework given in the abstract of the original paper ([Dinh 2015](#Dinh15)) is as follows:\n",
    "\n",
    "> It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables.\n",
    "\n",
    "As with many normalising flow examples, a typical choice for a base distribution would be an isotropic Gaussian, which is then transformed by the deep learning model. An important aspect is the efficient calculation of the Jacobian determinant of the transformation. \n",
    "\n",
    "In this section, we will describe the NICE architecture, and the RealNVP architecture that is built upon it. We will follow the exposition of the original papers, and think of the forward transformation as acting on the data input example. Note however that this is in contrast to the usual bijector convention of using the forward transformation for sampling, and the inverse transformation for computing log probs.\n",
    "\n",
    "#### Affine coupling layer\n",
    "The basic building block of the NICE architecture is the affine coupling layer. Given an input $\\mathbf{x}\\in\\mathbb{R}^D$, we split it into two blocks $(\\mathbf{x}_{1:d}, \\mathbf{x}_{d+1:D})$, where $d<D$ (usually $d\\approx D / 2$), and apply a transformation of the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d},\\tag{1}\\\\\n",
    "\\mathbf{z}_{d+1:D} &= \\mathbf{x}_{d+1:D} + t(\\mathbf{x}_{1:d}),\\tag{2}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $t:\\mathbb{R}^d\\mapsto\\mathbb{R}^{D-d}$ is an arbitrarily complex function, such as a neural network. It is easy to see that the coupling layer as above has an identity Jacobian matrix, and is trivially invertible:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d},\\\\\n",
    "\\mathbf{x}_{d+1:D} &= \\mathbf{z}_{d+1:D} - t(\\mathbf{z}_{1:d}).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Several coupling layers can be composed together to obtain a more complex, layered transformation. Note that a coupling layer leaves part of its input unchanged, and so the roles of the two subsets should be interchanged in alternating layers. \n",
    "\n",
    "If we examine the Jacobian, we can see that at least three coupling layers are needed to allow all dimensions to influence each other (this is left as an exercise for the reader). In the NICE paper, networks were composed with four coupling layers.\n",
    "\n",
    "#### RealNVP\n",
    "RealNVP stands for real-valued, non-volume preserving ([Dinh et al 2017](#Dinh17)). It was a follow-up work to the NICE paper, in which the affine coupling layer was modified as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d},\\tag{3}\\\\\n",
    "\\mathbf{z}_{d+1:D} &= \\mathbf{x}_{d+1:D}\\odot \\exp(s(\\mathbf{x}_{1:d})) + t(\\mathbf{x}_{1:d}),\\tag{4}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $s$ and $t$ stand for scale and translation, and are both functions that map from $\\mathbb{R}^d$ to $\\mathbb{R}^{D-d}$. The name RealNVP emphasises the fact that the transformation $(3)-(4)$ is no longer volume-preserving, as is the case with $(1)-(2)$, due to the additional scaling provided by the term $\\exp(s(\\mathbf{x}_{1:d}))$. We use the network output $s(\\mathbf{x}_{1:d})$ as a log-scale parameter for numerical stability.\n",
    "\n",
    "As before, the inverse transformation is no more complex than the forward propagation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d},\\tag{5}\\\\\n",
    "\\mathbf{x}_{d+1:D} &= (\\mathbf{z}_{d+1:D} - t(\\mathbf{z}_{1:d})) \\odot \\exp(-s(\\mathbf{z}_{1:d})).\\tag{6}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<center><img src=\"figures/affine_coupling_layer.png\" alt=\"RealNVP: forward pass\" style=\"width: 800px;\"/></center>\n",
    "<center>The forward and inverse passes of the RealNVP affine coupling layer</center>\n",
    "<br>\n",
    "\n",
    "Now, the Jacobian is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\mathbb{I}_d & \\mathbf{0}\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{d+1:D}}{\\partial \\mathbf{x}_{1:d}} & \\text{diag}\\,(\\exp (s(\\mathbf{x}_{1:d})))\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{D\\times D}\n",
    "$$\n",
    "\n",
    "and the log of the absolute value of the Jacobian determinant is easily calculated as $\\sum_j s(\\mathbf{x}_{1:d})_j$.\n",
    "\n",
    "#### Spatial and channel-wise masking\n",
    "Observe that the partitioning $\\mathbf{x}\\rightarrow (\\mathbf{x}_{1:d}, \\mathbf{x}_{d+1:D})$ can be implemented using a binary mask $b\\in\\{0, 1\\}^{n_h\\times n_w\\times c}$, so that the forward pass $(3)-(4)$ can be written\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})).\\tag{7}\n",
    "$$\n",
    "\n",
    "Similarly, the inverse pass $(5)-(6)$ can be written\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = b\\odot \\mathbf{z} + (1-b)\\odot((\\mathbf{z}-t(b\\odot\\mathbf{z}))\\odot \\exp(-s(b\\odot \\mathbf{z}))).\\tag{8}\n",
    "$$\n",
    "\n",
    "RealNVP implements two types of masking for image data $\\mathbf{x}\\in\\mathbb{R}^{n_h\\times n_w\\times c}$: spatial checkerboard and channel-wise masking. A spatial checkerboard mask applies the same partitioning to every channel dimension, as illustrated in the following figure.\n",
    "\n",
    "<center><img src=\"figures/checkerboard_mask.png\" alt=\"Checkerboard masking\" style=\"width: 600px;\"/></center>\n",
    "<center>Spatial checkerboard masking in RealNVP. (a) A layer input $\\mathbf{h}\\in\\mathbb{R}^{6\\times 6\\times 4}$ without masking, and (b) multiplied elementwise by a spatial checkerboard mask $b_s\\in\\{0, 1\\}^{6\\times 6}$, which is broadcast along the channel dimension</center>\n",
    "<br>\n",
    "\n",
    "A channel mask instead operates along the channel dimension, and applies the same partitioning at every spatial location, as in the following figure.\n",
    "\n",
    "<center><img src=\"figures/channel_mask.png\" alt=\"Channel masking\" style=\"width: 600px;\"/></center>\n",
    "<center>Channel masking in RealNVP. (a) A layer input $\\mathbf{h}\\in\\mathbb{R}^{6\\times 6\\times 4}$ without masking, and (b) multiplied elementwise by a channel mask $b_c\\in\\{0, 1\\}^{4}$, which is broadcast across the spatial dimensions</center>\n",
    "<br>\n",
    "\n",
    "As in the NICE framework, we want to ensure that all dimensions are able to interact with each other. The RealNVP architecture consists of three layers of alternating checkerboard masks, where the partitions are permuted. \n",
    "\n",
    "<center><img src=\"figures/alternating_masks.png\" alt=\"Alternating masks\" style=\"width: 900px;\"/></center>\n",
    "<center>Three affine coupling layers, with alternating masks in between layers</center>\n",
    "<br>\n",
    "\n",
    "#### Squeeze operation\n",
    "In the RealNVP architecture, after the three affine coupling layers with checkerboard masking there is a squeeze operation, where the spatial dimensions of the layer are divided into $2\\times 2\\times c$ subsquares, and reshaped into $1\\times 1\\times 4c$. The figure below illustrates this operation for a single channel:\n",
    "\n",
    "<center><img src=\"figures/squeeze.png\" alt=\"Squeeze operation\" style=\"width: 600px;\"/></center>\n",
    "<center>The squeeze operation. The spatial dimensions are halved, and the channel dimension is quadrupled</center>\n",
    "<br>\n",
    "\n",
    "Following the squeeze operation, there are three more affine coupling layers, this time using channel masking, and again permuting the partitions between each layer.\n",
    "\n",
    "#### Multiscale architecture\n",
    "The final component of the RealNVP framework is the multiscale architecture. With the squeeze operation, the spatial dimensions are downsampled, but the channel dimensions are increased. In order to reduce the overall layer sizes in the deeper layers, dimensions are factored out as latent variables at regular intervals.\n",
    "\n",
    "After one of the blocks of coupling-squeeze-coupling described above, half of the dimensions are factored out as latent variables, while the other half is further processed through subsequent layers. \n",
    "\n",
    "<center><img src=\"figures/factor_out_latent_variables.png\" alt=\"Multiscale architecture\" style=\"width: 800px;\"/></center>\n",
    "<center>Example showing how latent variables are factored out in the multiscale architecture. A layer input $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{8\\times 8\\times 2}$ will be reshaped to a $4\\times4\\times8$-shaped tensor after the coupling-squeeze-coupling block. Half of this tensor is absorbed into the base distribution as a latent variable $\\mathbf{z}^{(k+1)}\\in\\mathbb{R}^{4\\times 4\\times 4}$ and the remainder $\\mathbf{h}^{(k+1)}\\in\\mathbb{R}^{4\\times 4\\times 4}$ is processed through further layers of the network</center>\n",
    "<br>\n",
    "\n",
    "The complete RealNVP model has multiple levels of the multiscale architecture. This results in latent variables that represent different scales of features in the model. After a number of these levels, the final scale does not use the squeezing operation, and instead applies four affine coupling layers with alternating checkerboard masks to produce the final latent variable.\n",
    "\n",
    "<center><img src=\"figures/realnvp.png\" alt=\"Multiscale architecture\" style=\"width: 800px;\"/></center>\n",
    "<center>The end-to-end RealNVP architecture. Each scale consists of a block of 3 coupling layers (with checkerboard mask), squeeze, 3 coupling layers (with channel mask), followed by half of the dimensions factored out as a latent variable. The final scale consists only of 4 coupling layers (with checkerboard mask) to produce the final latent variable</center>\n",
    "<br>\n",
    "\n",
    "The following summarises the forward pass $\\mathbf{z} = f(\\mathbf{x})$ of the overall architecture with $L$ scales. The functions $f^{(1)},\\ldots,f^{(L-1)}$ consist of the coupling-squeeze-coupling block, whereas the function $f^{(L)}$ consists of 4 coupling layers with checkerboard masks.\n",
    "\n",
    ">\n",
    ">$$\\begin{align}\\mathbf{h}^{(0)}&=\\mathbf{x}\\\\ (\\mathbf{z}^{(k+1)}, \\mathbf{h}^{(k+1)})&=f^{(k+1)}(\\mathbf{h}^{(k)}),\\qquad k=0,\\ldots, L-2\\\\ \\mathbf{z}^{(L)}&= f^{(L)}(\\mathbf{h}^{(L-1)})\\\\\n",
    "\\mathbf{z} &= (\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(L)})\\end{align}$$\n",
    ">\n",
    "\n",
    "The latent variables factored out at each scale are reshaped and concatenated to produce a single latent variable $\\mathbf{z} = (\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(L)})$, which is assumed to be distributed according to a known base distribution (e.g. a diagonal Gaussian).\n",
    "\n",
    "As a final note, the architecture described in this section was further developed with the Glow model ([Kingma and Dhariwal 2018](#Kingma18)), where the checkerboard and channel-wise masking was replaced with 1x1 convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"affine_coupling_layer\"></a>\n",
    "## Affine Coupling Layer\n",
    "\n",
    "In this section we will build a partial implementation of the RealNVP architecture. In particular, we will write a custom layer to implement the affine coupling layer, using a binary mask:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} &= b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})) & \\text{(forward pass)}\\\\\n",
    "\\mathbf{x} &= b\\odot \\mathbf{z} + (1-b)\\odot((\\mathbf{z}-t(b\\odot\\mathbf{z}))\\odot \\exp(-s(b\\odot \\mathbf{z}))) & \\text{(inverse pass)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our affine coupling layer will take `shift_and_log_scale_fn` and `mask` arguments, which represent the $s$ and $t$ functions and $b$ mask respectively. In addition, we will implement an `inverse` method for the inverse pass (the `call` method implements the forward pass). \n",
    "\n",
    "The forward pass will be used during training (the forward pass as above maps $\\mathbf{x}\\mapsto\\mathbf{z}$), and so our custom layer also makes use of the in-built `add_loss` method (see [this guide](https://keras.io/guides/making_new_layers_and_models_via_subclassing/#the-addloss-method)) that will add the layer's contribution to the negative log likelihood loss. Recall that the Jacobian of a composition of functions is the product of Jacobians, so the layer's contribution is the negative log Jacobian determinant of this layer transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AffineCouplingLayer class\n",
    "\n",
    "from keras.layers import Layer\n",
    "\n",
    "class AffineCouplingLayer(Layer):\n",
    "\n",
    "    def __init__(self, shift_and_log_scale_fn, mask, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.shift_and_log_scale_fn = shift_and_log_scale_fn\n",
    "        self.b = ops.cast(mask, 'float32')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.event_dims = list(range(1, len(input_shape)))\n",
    "\n",
    "    def call(self, x):\n",
    "        t, log_s = self.shift_and_log_scale_fn(self.b * x)\n",
    "        z = self.b * x + (1 - self.b) * (x * ops.exp(log_s) + t) \n",
    "        \n",
    "        self.add_loss(-ops.mean(ops.sum(log_s * (1 - self.b), axis=self.event_dims)))\n",
    "        return z\n",
    "\n",
    "    def inverse(self, z):\n",
    "        t, log_s = self.shift_and_log_scale_fn(self.b * z)\n",
    "        x = self.b * z + (1 - self.b) * ((z - t) * ops.exp(-log_s))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a `shift_and_log_scale_fn` with the following structure:\n",
    "\n",
    "<center><img src=\"figures/shift_and_log_scale_fn.png\" alt=\"Shift and log-scale network\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example shift_and_log_scale_fn\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "def get_shift_and_log_scale_fn(input_size, hidden_units=[32, 32], activation='relu'):\n",
    "    inputs = Input(shape=[input_size])\n",
    "    h1 = inputs\n",
    "    h2 = inputs\n",
    "    for units in hidden_units:\n",
    "        h1 = Dense(units, activation=activation)(h1)\n",
    "        h2 = Dense(units, activation=activation)(h2)\n",
    "    shift = Dense(input_size)(h1)\n",
    "    log_scale = Dense(input_size, activation='tanh')(h2)\n",
    "    return Model(inputs=inputs, outputs=[shift, log_scale])\n",
    "\n",
    "shift_and_log_scale = get_shift_and_log_scale_fn(input_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a binary mask\n",
    "\n",
    "mask = ops.array([1, 1, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AffineCouplingLayer\n",
    "\n",
    "aff = AffineCouplingLayer(shift_and_log_scale, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AffineCouplingLayer\n",
    "\n",
    "x = keras.random.normal((2, 8))\n",
    "print(x)\n",
    "z = aff(x)  \n",
    "print(z)\n",
    "print(aff.inverse(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the loss contribution\n",
    "\n",
    "aff.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Verify the log Jacobian determinant value that is returned above by computing it directly using the `shift_and_log_scale` Model and the input Tensor `x` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two moons dataset\n",
    "We will now create a normalising flow using the `AffineCouplingLayer` and train it on a two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "\n",
    "train_data = make_moons(n_samples=50000, noise=0.05)[0].astype(np.float32)\n",
    "val_data = make_moons(n_samples=1000, noise=0.05)[0].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(train_data[:1000, 0], train_data[:1000, 1], alpha=0.2)\n",
    "plt.title(\"Two moons data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation DataLoaders\n",
    "\n",
    "import torch\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(torch.from_numpy(train_data), torch.from_numpy(train_data))\n",
    "val_ds = torch.utils.data.TensorDataset(torch.from_numpy(val_data), torch.from_numpy(val_data))\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train the normalising flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our normalising flow model will consist of a number of affine coupling layers, with the mask reversed between each layer. The model maps the data $\\mathbf{x}$ to the latent variable $\\mathbf{z}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coupling layer chain\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[2]),\n",
    "])\n",
    "num_layers = 8\n",
    "mask = ops.array([1, 0])\n",
    "\n",
    "for l in range(num_layers):\n",
    "    shift_and_log_scale = get_shift_and_log_scale_fn(2, hidden_units=[256, 256], \n",
    "                                                     activation='relu')\n",
    "    aff = AffineCouplingLayer(shift_and_log_scale, mask)\n",
    "    mask = 1 - mask\n",
    "    model.add(aff)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the negative log-likelihood of the data under our model $f_{\\theta}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\log P(\\mathcal{D}; \\theta) &= \\sum_{x\\in\\mathcal{D}} -\\log p_Z(f_\\theta(\\mathbf{x})) - \\log \\hspace{0.1ex}\\left|\\hspace{0.1ex}\\det J_{f_\\theta}(\\mathbf{x}) \\hspace{0.1ex}\\right|,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where in our example $f_\\theta$ is mapping from $X$ to $Z$. \n",
    "\n",
    "Each affine coupling layer adds its (negative) log Jacobian determinant contribution to the negative log-likelihood loss. We will write a custom loss function to take care of the final $-\\log p_Z(f_\\theta(\\mathbf{x}))$ term (see [here](https://keras.io/api/losses/#creating-custom-losses) for more information on creating custom loss functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom loss function\n",
    "\n",
    "def normal_log_pdf_loss(y_true, y_pred):\n",
    "    event_dims = list(range(1, len(y_pred.shape)))\n",
    "    const = 0.5 * ops.log(2. * np.pi)\n",
    "    return ops.sum(const + ops.square(y_pred)/2., axis=event_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inverse mapping Z -> X\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "z_sample = Input(shape=[2])\n",
    "h = z_sample\n",
    "for layer in reversed(model.layers):\n",
    "    h = layer.inverse(h)\n",
    "inverse_model = Model(inputs=z_sample, outputs=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SaveSamples(Callback):\n",
    "    \n",
    "    def __init__(self, plot_folder='./plots', **kwargs):\n",
    "        super(SaveSamples, self).__init__(**kwargs)\n",
    "        self.plot_folder = plot_folder\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def plot(self, num):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        ax = plt.gca()\n",
    "        plt.xlim([-1.5, 2.5])\n",
    "        plt.ylim([-1, 1.5])\n",
    "        ax.set_aspect('equal')\n",
    "        samples = ops.convert_to_numpy(inverse_model(keras.random.normal([2000, 2])))\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./plots/{:05d}.png\".format(num))\n",
    "        plt.close()\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not os.path.exists(self.plot_folder):\n",
    "            os.makedirs(self.plot_folder)\n",
    "        self.iteration = 0\n",
    "        self.plot(self.iteration + 1)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.iteration += 1\n",
    "        if self.iteration % 30 == 0:\n",
    "            self.plot((self.iteration // 30) + 1)\n",
    "        \n",
    "save_samples = SaveSamples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass our custom loss function directly into the `compile` method as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "\n",
    "model.compile(loss=normal_log_pdf_loss, optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "history = model.fit(train_dl, validation_data=val_dl, epochs=20, callbacks=[save_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.ylabel(\"NLL\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a movie file\n",
    "\n",
    "! ffmpeg -i ./plots/%05d.png -c:v libx264 -vf fps=10 -pix_fmt yuv420p -start_number 00000 samples.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"samples.mp4\", embed=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model transformations\n",
    "\n",
    "noise = keras.random.normal([2000, 2])\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15,10))\n",
    "h = noise\n",
    "axs[0, 0].scatter(ops.convert_to_numpy(h)[:, 0], ops.convert_to_numpy(h)[:, 1], alpha=0.1)\n",
    "axs[0, 0].set_title(\"Noise samples\")\n",
    "\n",
    "for i, (layer, ax) in enumerate(zip(reversed(model.layers), axs.flat[1:])):\n",
    "    h = layer.inverse(h)\n",
    "    ax.scatter(ops.convert_to_numpy(h)[:, 0], ops.convert_to_numpy(h)[:, 1], alpha=0.1)\n",
    "    ax.set_title(f\"After {i+1} steps of flow\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "filelist = [ f for f in os.listdir('./plots') if f.endswith(\".png\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./plots', f))\n",
    "if os.path.exists('samples.mp4'):\n",
    "    os.remove('samples.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Try re-running the two moons example again, but using a bi-modal base distribution. Is the flow able to more easily approximate the two moons distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"glow\"></a>\n",
    "## Glow\n",
    "\n",
    "Glow ([Kingma et al 2018](#Kingma18)) is short for Generative Flow, and this model builds directly from the RealNVP model. It uses the same multiscale architecture setup, where the latent variables are factored out at different levels with the same mechanism as before. \n",
    "\n",
    "However, Glow replaces the coupling-squeeze-coupling blocks with a different computation block. The Glow model aims to both simplify the RealNVP architecture, as well as to generalise a part of it. \n",
    "\n",
    "As we'll see, Glow still retains the affine coupling layer as the core computation of the model, and also still uses the squeeze operation, and the channel-wise masking. But it removes the checkerboard mask entirely, and also uses a different transformation instead of the alternating mask mechanism of RealNVP.\n",
    "\n",
    "<center><img src=\"figures/glow_schematic.png\" alt=\"Glow multiscale architecture\" style=\"width: 800px;\"/></center>\n",
    "<center>Glow uses the same multiscale architecture as in RealNVP, but replaces the coupling-squeeze-coupling block</center>\n",
    "<br>\n",
    "\n",
    "The computation block used in the Glow model is shown in the diagram below.\n",
    "\n",
    "<center><img src=\"figures/glow_step_arrows_labels.png\" alt=\"Glow copmutation block\" style=\"width: 400px;\"/></center>\n",
    "<center>The computation block used in the Glow model: a squeeze layer, followed by $K$ steps of flow, each consisting of an actnorm layer, an invertible 1x1 convolution, and an affine coupling layer</center>\n",
    "<br>\n",
    "\n",
    "The squeeze and affine couple layers are the same as in the RealNVP model. The affine coupling layer uses a fixed channel-wise mask. The input to the computation block is first passed through the squeeze layer, which halves each of the spatial dimensions of the input, and multiplies the size of the channel dimension by four.\n",
    "\n",
    "This is followed by a number of steps of flow, indicated in the diagram above by a green block. A single step of flow consists of an actnorm layer, followed by an invertible 1x1 convolution layer - both of which are new to the Glow model - followed by an affine coupling layer.\n",
    "\n",
    "Within each level of the Glow model, the squeeze operation is followed by $K$ steps of flow, before half of the neurons are factored out as latent variables as part of the multi scale architecture that we've seen before.\n",
    "\n",
    "#### Activation normalisation (actnorm)\n",
    "\n",
    "Actnorm is actually a replacement for the batch normalisation layers that are used within the RealNVP model. Recall that batchnorm is used in RealNVP in the shift and scale networks $s$ and $t$ that are used to parameterise the transformation inside the affine coupling layers. It's also applied to the output of the whole affine coupling layer, and this is where we need to compute the log-Jacobian determinant of the transformation, shown in the following table.\n",
    "\n",
    "| Transformation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| log-Jacobian determinant |\n",
    "| :--- | --- |\n",
    "| $\\hat{h}_j^{(k)} = \\frac{\\large h_j^{(k)} - \\mu_{jm}^{(k)}}{\\sqrt{\\left( \\sigma_{jm}^{(k)} \\right)^2 + \\epsilon}} $ | $z_j^{(k)} = \\gamma_j^{(k)}\\hat{h}_j^{(k)} + \\beta_j^{(k)} $ |\n",
    "| $ -\\frac{1}{2} \\sum_j \\log \\left( \\left( \\sigma_{jm}^{(k)}\\right)^2 + \\epsilon \\right)$ | $\\sum_j \\log \\left| \\gamma_j^{(k)} \\right|$ |\n",
    "\n",
    "<center>Batch normalisation transformations and log-Jacobian determinants</center>\n",
    "\n",
    "The potential problem with batch normalisation is that it relies on minibatch statistics being reasonable estimates of the dataset statistics. If the minibatch statistics are high variance, then this can impact performance. In the case of the original Glow implementation, practical computing constraints meant that the the authors needed to reduce the minibatch size to one, and batchnorm then became less effective.\n",
    "\n",
    "The solution in the Glow architecture is to introduce actnorm, or activation normalisation, as an alternative to batchnorm. The transformation of actnorm is a simple affine transformation per feature, parameterised by scale and shift parameters gamma and beta:\n",
    "\n",
    "$$\n",
    "z_j^{(k)} = \\gamma^{(k)}_j h^{(k)}_j + \\beta^{(k)}_j,\n",
    "$$\n",
    "\n",
    "where the subscript $j$ is indexing the feature dimension and $k$ indexes the layer.\n",
    "\n",
    "Actnorm does not compute minibatch statistics, which means that the transformation is more stable, but also less flexible, because it's not aware of the statistics of the given input minibatch, and so it's not able to normalise the input activations towards a target mean and variance.\n",
    "\n",
    "However, the parameters $\\gamma^{(k)}_j$ and $\\beta^{(k)}_j$ are initialised based on the statistics of a sample minibatch. In particular, they're initialised such that the output activations of the layer have zero mean and unit variance on the given sample minibatch. This is an example of data-dependent initialisation, and is trying to initialise these parameters in a good place for the particular dataset, even though the post-activations will drift away from zero mean and unit variance during training.\n",
    "\n",
    "Clearly this transformation is trivially invertible so long as all the gammas are nonzero, and in practice these are parameterised to ensure this.\n",
    "\n",
    "The log-Jacobian determinant is given by \n",
    "\n",
    "$$\n",
    "\\sum_j \\log \\left| \\gamma_j^{(k)} \\right|,\n",
    "$$\n",
    "\n",
    "which is a straightforward and efficient computation.\n",
    "\n",
    "Note that the expressions above assume rank-one inputs, so that in particular $\\mathbf{h}^{(k)}$ would be a vector. When using the Glow model on image inputs, the inputs $\\mathbf{h}^{(k)}$ will be 3-dimensional tensors with shape $(h, w, c)$, and the parameters $\\gamma^{(k)}_j \\in\\mathbb{R}^c$ and $\\beta^{(k)}_j \\in\\mathbb{R}^c$ are shared across every spatial location. In this case, the log-Jacobian determinant is multiplied by $hw$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invertible 1x1 convolution\n",
    "\n",
    "Recall the following affine coupling layer sequence in RealNVP, that consists of alternating binary channel-wise masks.\n",
    "\n",
    "<center><img src=\"figures/alternating_masks.png\" alt=\"Alternating masks\" style=\"width: 900px;\"/></center>\n",
    "<center>Three affine coupling layers, with alternating channel masks in between layers</center>\n",
    "<br>\n",
    "\n",
    "The forward transformation in one of the affine coupling layers can be written as\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})),\n",
    "$$\n",
    "\n",
    "where $b$ is the channel mask, that zeros out half of the neurons in the channel dimension. RealNVP composes three affine coupling layers together like this, reversing the binary mask each time. \n",
    "\n",
    "Instead, we could equivalently think of using a fixed binary mask, and permuting the neurons in the layer in the channel dimension after each affine coupling layer. The permutation matrix $W\\in\\mathbb{R}^{c\\times c}$ would have the following block structure.\n",
    "\n",
    "$$\n",
    "W = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\mathbf{0} & \\mathbf{I} \\\\\n",
    "\\mathbf{I} & \\mathbf{0}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This same permutation matrix would then be applied at each spatial location. This is a special case of a convolutional layer, where the convolutional kernel has spatial dimensions of 1 by 1, where there is no bias term in the convolutional layer, and where the number of output channels is the same as the number of input channels.\n",
    "\n",
    "So an equivalent formulation to the sequence of affine coupling layers with an alternating binary channel mask that we looked at before in RealNVP, is where we instead use a 1x1 convolution operation.\n",
    "\n",
    "The main contribution of the Glow model is to generalise the permutation of the channel dimensions by using a learned 1x1 convolution. This convolutional layer will have the same number of input and output channels, and doesn't use a bias term.\n",
    "\n",
    "So then we can write the transformation as a 2-dimensional convolution as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k+1)} = \\text{conv2d}(\\mathbf{h}^{(k)}; \\mathbf{W}^{(k)}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(k)}$ is the convolutional kernel. The log Jacobian determinant will then be\n",
    "\n",
    "$$\n",
    "h\\cdot w \\cdot \\log~ \\left|~ \\text{det} (\\mathbf{W}^{(k)})~\\right|,\n",
    "$$\n",
    "\n",
    "where $h$ and $w$ are the sizes of the height and width dimensions respectively.\n",
    "\n",
    "The problem here is that it can be expensive to compute the determinant of the convolutional kernel $\\mathbf{W}^{(k)}$. This computation scales as $\\mathcal{O}(c^3)$, where $c$ is the number of channels.\n",
    "\n",
    "A solution to this problem is to use the LU decomposition of the $c\\times c$ matrix $\\mathbf{W}^{(k)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{(k)} = \\mathbf{P}^{(k)} \\mathbf{L}^{(k)} \\left( \\mathbf{U}^{(k)} + \\text{diag}\\left(\\mathbf{s}^{(k)}\\right) \\right).\n",
    "$$\n",
    "\n",
    "In the above, $\\mathbf{P}^{(k)}$ is some permutation matrix, $\\mathbf{L}^{(k)}$ is lower triangular with ones on the diagonal, $\\mathbf{U}^{(k)}$ is upper triangular, and we have separated out the diagonal elements so that $\\mathbf{U}^{(k)}$ is upper triangular with zeros on the diagonal, and the diagonal elements are written as the diagonalisation of a vector $\\mathbf{s}^{(k)}$.\n",
    "\n",
    "In practical implementations, we can directly parameterise the convolutional kernel as in the above expression. The log Jacobian determinant can now be written as $h$ times $w$ times the sum of the log of the absolute values of the elements of the vector $\\mathbf{s}^{(k)}$. This computation now scales as $\\mathcal{O}(c)$, so is much more efficient.\n",
    "\n",
    "The Glow paper initialises the convolutional kernel $\\mathbf{W}^{(k)}$ as a random rotation matrix, and computes the LU decomposition in order to initialise the parameterisation you see above. This will set the permutation matrix $\\mathbf{P}^{(k)}$, which remains fixed throughout the training. The matrices $\\mathbf{L}^{(k)}$ and $\\mathbf{U}^{(k)}$ as well as the vector $\\mathbf{s}^{(k)}$ are all learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine Coupling Layer\n",
    "\n",
    "The third layer in a single step of flow is the affine coupling layer with a (fixed) binary channel-wise mask $b$. The forward transformation and the log-Jacobian determinant are shown again below.\n",
    "\n",
    "<center><img src=\"figures/realnvp_forward.png\" alt=\"RealNVP: forward pass\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "| Transformation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| log-Jacobian determinant |\n",
    "| :--- | --- |\n",
    "| $\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x}))$ | $\\sum_j \\log \\left| s_j \\right| $ |\n",
    "\n",
    "----\n",
    "\n",
    "The actnorm, invertible 1x1 convolution and affine coupling layers are the three layers that make up a single step of flow of the Glow model. Within each level of the Glow architecture, the inputs are first passed through the squeeze layer, and through $K$ steps of flow, before half of the neurons are factored out as latent variables. The remaining neurons continue on for further processing. This is the same multi scale architecture we saw in the RealNVP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Abdelhamed19\"></a>\n",
    "* Abdelhamed, A., Brubaker, M. A. & Brown, M. S. (2019), \"Noise flow: Noise modeling with conditional normalizing flows\", in *Proceedings of the IEEE International Conference on Computer Vision*, 3165–3173.\n",
    "<a class=\"anchor\" id=\"Dinh15\"></a>\n",
    "* Dinh, L., Krueger, D. & Bengio, Y. (2015),\"NICE: Non-linear Independent Components Estimation\", in *3rd International Conference on Learning Representations, (ICLR)*, San Diego, CA, USA, May 7-9, 2015.\n",
    "<a class=\"anchor\" id=\"Dinh17\"></a>\n",
    "* Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2017), \"Density estimation using Real NVP\",  in *5th International Conference on Learning Representations, (ICLR)*, Toulon, France, April 24-26, 2017.\n",
    "<a class=\"anchor\" id=\"Ho19\"></a>\n",
    "* Ho, J., Chen, X., Srinivas, A., Duan, Y., & Abbeel, P. (2019), \"Flow++: Improving flow-based generative models with variational dequantization and architecture design\", in *Proceedings of the 36th International Conference on Machine Learning, ICML*.\n",
    "<a class=\"anchor\" id=\"Kingma18\"></a>\n",
    "* Kingma, D. P. & Dhariwal, P. (2018), \"Glow: Generative Flow with Invertible 1x1 Convolutions\", in *Advances in Neural Information Processing Systems*, **31**, 10215--10224.\n",
    "<a class=\"anchor\" id=\"Kumar19\"></a>\n",
    "* Kumar, M., Babaeizadeh, M., Erhan, D., Finn, C., Levine, S., Dinh, L. & Kingma, D. (2019), \"VideoFlow: A Flow-Based Generative Model for Video\", in *Workshop on Invertible Neural Nets and Normalizing Flows*, ICML, 2019.\n",
    "<a class=\"anchor\" id=\"Prenger19\"></a>\n",
    "* Prenger, R., Valle, R., & Catanzaro, B. (2019), \"Waveglow: A flow-based generative network for speech synthesis\", in *Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)*, IEEE, 3617-3621.\n",
    "<a class=\"anchor\" id=\"Rezende15\"></a>\n",
    "* Rezende, D. & Mohamed, S. (2015), \"Variational Inference with Normalizing Flows\", in *Proceedings of Machine Learning Research*, **37**, 1530-1538."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

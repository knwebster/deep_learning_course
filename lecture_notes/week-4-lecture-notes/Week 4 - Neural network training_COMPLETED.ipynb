{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 4: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Optimisers](#optimisers)\n",
    "\n",
    "[3. Weight regularisation and early stopping](#regularisation)\n",
    "\n",
    "[4. Keras regularisers, Dropout layers and callbacks (\\*)](#keras_regularisation)\n",
    "\n",
    "[5. Weight initialisation](#weight_init)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In the last week of the module we studied the error backpropagation algorithm, which is the central algorithm used to compute gradients in the training of neural networks. We also saw how to use the automatic differentiation tools in TensorFlow and PyTorch, as well as introduced data pipelines with both frameworks. Finally, we saw two commonly used techniques used in deep learning models, which are dropout and batch normalization. \n",
    "\n",
    "In this week of the course, we will continue to look at neural network training, with a focus on the practical techniques that are used to successfully train these models. We will study various optimisation algorithms that are used, as well as regularisation techniques and weight initialisation strategies. \n",
    "\n",
    "We will also learn how to implement all of these techniques in Keras, as well as making use of Callback objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"optimisers\"></a>\n",
    "## Optimisers\n",
    "\n",
    "Recall the two main steps to training neural networks:\n",
    "\n",
    "1. Computation of the (stochastic) gradient of the loss function with respect to the model parameters\n",
    "2. Use of the computed gradient to update the parameters\n",
    "\n",
    "Now that we have seen how gradients of the loss with respect to the parameters can be efficiently computed using the backpropagation algorithm (step 1), we will take a look at several popular gradient-based optimisation algorithms used in deep learning (step 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent\n",
    "We have already seen how stochastic gradient descent (SGD, [Robbins & Monro 1951](#Robbins51)) can be applied to optimise neural network parameters. \n",
    "\n",
    "Recall that SGD computes stochastic gradients by computing the loss on a minibatch of samples:\n",
    "\n",
    "$$\n",
    "L(\\theta_t; \\mathcal{D}_m) = \\frac{1}{M} \\sum_{x_i, y_i\\in\\mathcal{D}_m} l(y_i, f_{\\theta_t}(x_i)),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}_m$ is a randomly sampled minibatch of training data points, $M = |\\mathcal{D}_m|$ is the size of the minibatch (typically much smaller than $|\\mathcal{D}_{train}|$). We then use the gradient $\\nabla\\tilde{L}(\\theta_t)$ to update the parameters according to the SGD update rule\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta_t} L(\\theta_t; \\mathcal{D}_m),\\qquad t\\in\\mathbb{N}_0.\n",
    "$$\n",
    "\n",
    "In Keras, an SGD optimizer object can be instantiated from the `keras.optimizers` module. \n",
    "\n",
    "Optimiser objects like this one can be passed directly into the `optimizer` keyword argument in `model.compile` instead of the string reference `'sgd'`. This is useful, for example if you want to change the learning rate default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SGD optimiser\n",
    "\n",
    "sgd = keras.optimizers.SGD()\n",
    "\n",
    "ops.convert_to_numpy(sgd.learning_rate)  # Default learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is stored as a Keras Variable, hence the need for `ops.convert_to_numpy` to inspect its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sgd optimiser and change the learning rate \n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate=0.005)\n",
    "\n",
    "ops.convert_to_numpy(sgd.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent reduces redudancy in the gradient computation, and is faster than full (batch) gradient descent. However some challenges remain: \n",
    "\n",
    "* Convergence can still be very slow with SGD\n",
    "* Setting the learning correctly can be difficult, involving trial and error\n",
    "* Different weights might operate on different scales, and require different rates of learning\n",
    "\n",
    "Several optimisation algorithms have been proposed to help treat these problems.\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "One common tweak to accelerate the slow convergence of SGD is to add momentum ([Qian 1999](#Qian99)):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{g}_t :=&~ \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m),\\\\\n",
    "\\mathbf{v}_{t+1} =&~ \\beta \\mathbf{v}_t + \\eta\\mathbf{g}_t\\\\\n",
    "\\theta_{t+1} =&~ \\theta_t - \\mathbf{v}_{t+1},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\beta\\ge0$ is the momentum term, and as before, $\\eta>0$ is the learning rate. When $\\beta=0$ then we recover plain SGD, but with $\\beta>0$ (a typical value is around 0.9), this gives the gradient a short term memory which often accelerates convergence.\n",
    "\n",
    "Momentum can be added when created an SDG optimizer using the `momentum` keyword argument. The default value is `0.0` (plain SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SGD optimiser with momentum\n",
    "\n",
    "sgd_with_momentum = keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "print(ops.convert_to_numpy(sgd_with_momentum.learning_rate))\n",
    "print(sgd_with_momentum.momentum)  # momentum is stored as a float value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov momentum\n",
    "\n",
    "A common variant of momentum is to use Nesterov momentum ([Nesterov 1983](#Nesterov83)), which computes the gradient correction after the accumulated gradient, instead of before:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{g}_t &= \\nabla_\\theta L(\\theta_t - \\beta\\mathbf{v}_t; \\mathcal{D}_m),\\\\\n",
    "\\mathbf{v}_{t+1} &= \\beta \\mathbf{v}_t + \\eta\\mathbf{g}_t\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\mathbf{v}_{t+1},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The accumulated gradient approximates the next value of the parameters, and so by evaluating the gradient $\\nabla_\\theta\\tilde{L}(\\theta_t - \\beta\\mathbf{v}_t )$, this gives the optimiser a sense of 'look-ahead'.\n",
    "\n",
    "<center><img src=\"figures/nesterov_momentum.png\" alt=\"Nesterov momentum\" style=\"width: 550px;\"/></center>\n",
    "<br>\n",
    "\n",
    "Nesterov momentum can be added to an SGD optimizer using the `nesterov` keyword argument. The default value is `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SGD optimiser with momentum\n",
    "\n",
    "sgd_with_momentum = keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "\n",
    "print(sgd_with_momentum.momentum)\n",
    "print(sgd_with_momentum.nesterov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad\n",
    "\n",
    "The Adagrad optimiser ([Duchi et al 2011](#Duchi11)) adapts the learning rate for each parameter, to account for different weights learning on different scales. Parameters that receive a gradient less frequently have larger updates, making Adagrad well suited to sparse data, where most of the features are zero in the data. It is used, for example, in [Pennington et al 2014](#Pennington14) to train GloVe word embedding vectors.\n",
    "\n",
    "The update rule is\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m),\n",
    "$$\n",
    "\n",
    "where $G_t\\in\\mathbb{R}^{p\\times p}$ is a diagonal matrix where the diagonal element $(G_t)_{ii}$ is the sum of squares of gradients with respect to $\\theta_i$ up to time step $t$. In the above, the division and square root operations are performed element-wise, and $\\odot$ is the Hadamard product.\n",
    "\n",
    "Note that the resulting learning rates per parameter are monotonically decreasing, and eventually the algorithm effectively stops learning.\n",
    "\n",
    "The Adagrad optimiser can be instantiated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Adagrad optimiser\n",
    "\n",
    "adagrad = keras.optimizers.Adagrad(\n",
    "    learning_rate=0.001, epsilon=1e-07,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, all keyword arguments shown are the default settings.\n",
    "\n",
    "#### RMSprop\n",
    "\n",
    "RMSprop is an unpublished optimisation method that aims to resolve the vanishing learning rates of Adagrad (it appeared in Geoff Hinton's Coursera course [in lecture 6e](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)). It uses a decaying average of past squared gradients. \n",
    "\n",
    "The update rule is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\mathbf{g}^2]_t &= \\rho \\mathbb{E}[\\mathbf{g}^2]_{t-1} + (1 - \\rho)(\\nabla_\\theta L(\\theta_t; \\mathcal{D}_m))^2\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\mathbb{E}[\\mathbf{g}^2]_t + \\epsilon}} \\odot \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As before, the division and square root are performed element-wise, and $\\odot$ is the Hadamard product. The $\\rho$ term is typically set similar to momentum, around 0.9.\n",
    "\n",
    "The RMSprop can be instantiated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RMSprop optimiser\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, epsilon=1e-07\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the above are the default settings. Momentum can also be added to the `RMSprop` optimiser with the `momentum` keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RMSprop optimiser with momentum\n",
    "\n",
    "rmsprop_with_momentum = keras.optimizers.RMSprop(momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSprop optimiser is also the default optimiser that is chosen if `model.compile` is called without the `optimizer` keyword argument.\n",
    "\n",
    "#### Adam\n",
    "\n",
    "The Adam optimiser ([Kingma 2015](#Kingma15)) is a popular optimisation algorithm, that also computes adaptive learning rates per parameter. It estimates first and second moments of the gradients, and the name stands for <ins>Ada</ins>ptive <ins>m</ins>oment estimation.\n",
    "\n",
    "The update rule is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\mathbf{g}]_t &= \\beta_1\\mathbb{E}[\\mathbf{g}]_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m),\\\\\n",
    "\\mathbb{E}[\\mathbf{g}^2]_t &= \\beta_2\\mathbb{E}[\\mathbf{g}^2]_{t-1} + (1 - \\beta_2) (\\nabla_\\theta L(\\theta_t; \\mathcal{D}_m))^2,\\\\\n",
    "\\mathbf{m}_t &= \\mathbb{E}[\\mathbf{g}]_t / (1 - \\beta_1),\\\\\n",
    "\\mathbf{v}_t &= \\mathbb{E}[\\mathbf{g}^2]_t / (1 - \\beta_2),\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\mathbf{v}_t + \\epsilon}}\\odot \\mathbf{m}_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The $\\mathbf{m}_t$ and $\\mathbf{v}_t$ terms correct for an initial bias towards zero. Typical values are $\\beta_1 \\approx 0.9$, $\\beta_2 \\approx 0.999$ and $\\epsilon \\approx 10^{-7}$.\n",
    "\n",
    "The Adam optimiser can be instantiated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Adam optimiser\n",
    "\n",
    "adam = keras.optimizers.Adam()\n",
    "\n",
    "print(ops.convert_to_numpy(adam.learning_rate))\n",
    "print(adam.beta_1)\n",
    "print(adam.beta_2)\n",
    "print(adam.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a non-exhaustive list of optimisers that have been developed and are actively used in deep learning research and practice. See [here](https://keras.io/api/optimizers/) for a complete of the optimisers that are available to use in Keras.\n",
    "\n",
    "The code below will create a demonstration optimization run using one of each of the optimizers described above for the [Beale function](http://benchmarkfcns.xyz/benchmarkfcns/bealefcn.html), a common test function used to evaluate optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beale(x, y):\n",
    "    return (1.5 - x + x * y)**2 + (2.25 - x + x * (y**2))**2 + (2.625 - x + x * (y**3))**2\n",
    "\n",
    "def grad_beale(x, y):\n",
    "    ddx = 2*(1.5 - x + x * y)*(y - 1) + 2*(2.25 - x + x * (y**2))*((y**2) - 1) + 2*(2.625 - x + x * (y**3))*((y**3)-1)\n",
    "    ddy = 2*(1.5 - x + x * y)*(x) + 2*(2.25 - x + x * (y**2))*(2*y*x) + 2*(2.625 - x + x * (y**3))*(3*(y**2)*x)\n",
    "    return [ddx, ddy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the gradient is easy enough to calculate by hand as above. \n",
    "\n",
    "The cell below will run the optimization routine for 100 iterations for each optimizer, and plot the trajectories over the contour plot of the Beale function. Feel free to interrupt the cell execution and restart it to try a different random initial condition. You can also try changing the learning rates and other parameters to see the effect on convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_init = keras.random.normal(())\n",
    "y_init = keras.random.normal(())\n",
    "\n",
    "test_fn = beale\n",
    "grad_fn = grad_beale\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
    "Z = test_fn(X, Y)\n",
    "levels = np.exp(np.linspace(0, 10, 25)) - 1\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contour(X, Y, Z, levels, alpha=0.6, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(ops.convert_to_numpy(x_init), ops.convert_to_numpy(y_init))\n",
    "plt.scatter(3, 0.5, marker='*', label='Optimum')\n",
    "optimizers_config = [\n",
    "    {\"name\": \"SGD\", \"kwargs\": {\"learning_rate\": 0.01}, \"label\": \"SGD\"},\n",
    "    {\"name\": \"SGD\", \"kwargs\": {\"learning_rate\": 0.001, \"momentum\": 0.9, \"nesterov\": True}, \"label\": \"SGD-NAG\"},\n",
    "    {\"name\": \"Adam\", \"kwargs\": {\"learning_rate\": 0.1}, \"label\": \"Adam\"},\n",
    "    {\"name\": \"Adagrad\", \"kwargs\": {\"learning_rate\": 0.1}, \"label\": \"Adagrad\"},\n",
    "    {\"name\": \"RMSprop\", \"kwargs\": {\"learning_rate\": 0.05}, \"label\": \"RMSprop\"}\n",
    "]\n",
    "optimizers, states, plot_data = [], [], []\n",
    "for optimizer in optimizers_config:\n",
    "    optimizers.append(getattr(keras.optimizers, optimizer['name'])(**optimizer['kwargs']))\n",
    "    states.append((keras.Variable(x_init, name='x_{}'.format(optimizer['name'])), \n",
    "                   keras.Variable(y_init, name='y_{}'.format(optimizer['name']))))\n",
    "    opt_plot, = plt.plot([ops.convert_to_numpy(x_init)], [ops.convert_to_numpy(y_init)], label=optimizer['label'])\n",
    "    plot_data.append(opt_plot)\n",
    "plt.title(f\"Test optimization run with {len(optimizers)} optimizers. \"\n",
    "          f\"Initial conditions x: {ops.convert_to_numpy(x_init):.4f}, y: {ops.convert_to_numpy(y_init):.4f}\", fontsize=14)\n",
    "\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    try:\n",
    "        for optimizer, state, data in zip(optimizers, states, plot_data):\n",
    "            grads = grad_fn(state[0], state[1])\n",
    "            optimizer.apply_gradients(zip(grads, state))\n",
    "            data.set_xdata(np.append(data.get_xdata(), state[0].numpy()))\n",
    "            data.set_ydata(np.append(data.get_ydata(), state[1].numpy()))\n",
    "        plt.text(0.01, 0.01, f'Iteration {i+1}', horizontalalignment='left', \n",
    "                 verticalalignment='bottom', transform = plt.gca().transAxes, \n",
    "                 fontsize=12, bbox=dict(facecolor='white', alpha=1.))\n",
    "        plt.legend(fontsize=12)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no single answer for which optimiser is best to use, as it very often depends on the data and the model. Optimisers with adaptive learning rates tend to convergence faster in most situations, and are more frequently chosen than plain SGD in most situations. However, it is interesting to note that SGD has been shown to generalise better ([Hardt et al 2015](#Hardt15)), and techniques such as switching from Adam to SGD during training have been proposed ([Keskar & Socher 2017](#Keskar17)). Further methods include annealed learning rates, cyclic learning rates ([Smith 2015](#Smith15)) and decaying momentum ([Chen & Kyrillidis 2019](#Chen19)). Many advances have been made in recent years, and neural network optimisation is still very much an active research area.\n",
    "\n",
    "*Exercise.* Change the Beale function above for a different [test function](https://en.wikipedia.org/wiki/Test_functions_for_optimization) in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"regularisation\"></a>\n",
    "## Weight regularisation and early stopping\n",
    "\n",
    "Deep learning models are typically very over-parameterised, often with millions of parameters over many layers in the model. They are universal approximators (see e.g. [Cybenko](#Cybenko89) for the large width case, or [Lu et al](#Lu17) for the large depth case), and so overfitting can be a problem. When training neural networks, it is important to regularise them to prevent overfitting. As written above, there are several forms of regularisation, but in this section we will look at three in particular: weight regularisation, dropout and early stopping.\n",
    "\n",
    "#### $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation\n",
    "Recall that for a linear model of the form\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_j w_j \\phi_j(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "a typical regularisation is to add a sum of squares penalty term to the loss term to discourage the weights $w_j$ from growing too large. In this case, the regularised loss takes the form\n",
    "\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, \\alpha) = L_0(\\mathbf{w}) + \\alpha_2 \\sum_i w_i^2,\n",
    "$$\n",
    "\n",
    "where $L_0$ is the unconstrained loss function, and $\\alpha_2$ is a regularisation hyperparameter. This is $\\mathcal{l}^2$ regularisation.\n",
    "\n",
    "This form of regularisation is often referred to as **weight decay**, although the two terms are technically not the same. Weight decay ([Hanson & Pratt](#Hanson88)) is defined as a modification to the update rule, rather than to the loss function itself:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{t+1} \\leftarrow (1 - \\lambda)\\theta_t - \\eta g_t,\n",
    "$$\n",
    "\n",
    "where $\\theta\\in\\mathbb{R}^p$ is the model parameters, $\\lambda$, $\\eta$ are hyperparameters, and $g_t$ is the $t$-th batch update. In the case of stochastic gradient descent, the update $g_t = \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m)$ and the two formulations are equivalent. However, this is not the case for all gradient-based optimisers commonly used in deep learning.\n",
    "\n",
    "An alternative weight regularisation is $\\mathcal{l}^1$ regularisation, in which the sum of absolute values of the weights are added to the loss term:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, \\alpha) = L_0(\\mathbf{w}) + \\alpha_1 \\sum_i |w_i|.\n",
    "$$\n",
    "\n",
    "This form of regularisation encourages sparsity in the weights. Both $\\mathcal{l}^1$ and $\\mathcal{l}^2$ regularisation discourage the weights from growing too large, which restricts the capacity of the network.\n",
    "\n",
    "It is also possible to add a weighted combination of both $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping\n",
    "\n",
    "You might have found in the last week that it is difficult to set a good number of epochs to train for ahead of time. In the simple MNIST example training is quick so it is not a problem to experiment, but in many cases training could take hours or days (or even longer!) and so this is not an option. \n",
    "\n",
    "Recall that deep learning models are usually vastly overparameterised, and have the capacity to drastically overfit. A simple but effective method is to simply stop the training before the model starts to overfit. The picture is similar to the balance between capacity and generalisation:\n",
    "\n",
    "<center><img src=\"figures/early_stopping.png\" alt=\"Early stopping\" style=\"width: 500px;\"/></center>\n",
    "<center>Prediction error vs number of training epochs</center>\n",
    "<br>\n",
    "\n",
    "With early stopping, the aim is to stop the training when the validation error is at a minimum. This means that the model needs to be regularly evaluated on a held-out validation set (that is not used for training), and the optimisation routine is terminated when the validation error starts to rise. Validation is normally performed once per epoch in the training run.\n",
    "\n",
    "In practice, the validation error measurements will be noisy, and so it is not a reliable measure to simply detect when the validation error increases and immediately stop the training. What is usually done is to periodically save model checkpoints (say once per epoch), and set a **patience** threshold, to specify a maximum number of validation runs that are allowed where the validation error does not improve upon the best score so far. If this patience threshold is reached, the training is terminated.\n",
    "\n",
    "The early stopping algorithm is outlined in pseudocode below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping inputs: `val_metric`, `max_patience`\n",
    "\n",
    "-------\n",
    ">```\n",
    ">best_valid_loss = np.inf\n",
    ">patience = 0\n",
    ">\n",
    ">for epoch in range(max_epochs):\n",
    ">    epoch_train_loss = train_model(train_data, train_loss)\n",
    ">    epoch_valid_loss = validate_model(valid_data, val_metric)\n",
    ">    if epoch_valid_loss < best_valid_loss:\n",
    ">        best_valid_loss = epoch_valid_loss\n",
    ">        save_model(epoch)\n",
    ">        patience = 0\n",
    ">    else:\n",
    ">        patience += 1\n",
    ">    \n",
    ">    if patience >= max_patience:\n",
    ">        break  # terminate training\n",
    ">```\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to validate the model using a measure that is different to the loss function used for training the model. Therefore `val_metric` is also an input to the early stopping algorithm above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, all of the regularisation techniques mentioned here (and more) can be used together in deep learning models (and they often are)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"keras_regularisation\"></a>\n",
    "## Keras regularisers, Dropout layers and callbacks\n",
    "\n",
    "In this section we will build on what we have covered already with the `Sequential` API, and include weight regularisers, `Dropout` layers, and introduce callback objects - these are very useful objects for dynamically performing operations during the training run. An example is the `EarlyStopping` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use the diabetes dataset from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset description\n",
    "\n",
    "print(diabetes_dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input and target data\n",
    "\n",
    "print(diabetes_dataset.keys())\n",
    "data = diabetes_dataset[\"data\"]\n",
    "targets = diabetes_dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the target data (this will make clearer training curves)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "targets = (targets - targets.mean()) / targets.std()\n",
    "data, targets = data.astype(np.float32), targets.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data into training and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(data, targets, test_size=0.2) \n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into training, validation and test TF Dataset objects\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_targets))\n",
    "tf_val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_targets))\n",
    "\n",
    "tf_train_dataset = tf_train_dataset.shuffle(353)\n",
    "\n",
    "tf_train_dataset = tf_train_dataset.batch(128)\n",
    "tf_val_dataset = tf_val_dataset.batch(89)\n",
    "\n",
    "tf_train_dataset = tf_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "tf_val_dataset = tf_val_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into training, validation and test torch DataLoader objects\n",
    "\n",
    "import torch\n",
    "\n",
    "pt_train_dataset = torch.utils.data.TensorDataset(torch.Tensor(train_data), torch.Tensor(train_targets))\n",
    "pt_val_dataset = torch.utils.data.TensorDataset(torch.Tensor(val_data), torch.Tensor(val_targets))\n",
    "\n",
    "pt_train_dataloader = torch.utils.data.DataLoader(pt_train_dataset, shuffle=True, batch_size=128)\n",
    "pt_val_dataloader = torch.utils.data.DataLoader(pt_val_dataset, shuffle=False, batch_size=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the MLP model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(train_data.shape[-1],)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "# history = model.fit(tf_train_dataset, epochs=100, validation_data=tf_val_dataset, verbose=False)\n",
    "history = model.fit(pt_train_dataloader, epochs=100, validation_data=pt_val_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularise the model\n",
    "\n",
    "Both $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation can easily be included using the `kernel_regularizer` and `bias_regularizer` keyword arguments in the `Dense` layer.\n",
    "\n",
    "Dropout can also be easily included as an additional layer of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the model using l2 regularisation and dropout\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "l2_coeff = 1e-5\n",
    "rate = 0.5\n",
    "\n",
    "def get_regularised_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(train_data.shape[-1],)),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "model = get_regularised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "# history = model.fit(tf_train_dataset, epochs=100, validation_data=tf_val_dataset, verbose=False)\n",
    "history = model.fit(pt_train_dataloader, epochs=100, validation_data=pt_val_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the $\\mathcal{l}^2$ regularisation and dropout have helped to reduce the overfitting of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "We can go one step further and introduce early stopping as well. We can do this using a Keras callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "\n",
    "model = get_regularised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EarlyStopping` callback is a built-in callback in the `keras.callbacks` module. You can see a complete list of built-in callbacks [here](https://keras.io/api/callbacks/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EarlyStopping callback\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_mae', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "# history = model.fit(tf_train_dataset, epochs=100, validation_data=tf_val_dataset, verbose=False, callbacks=[earlystopping])\n",
    "history = model.fit(pt_train_dataloader, epochs=100, validation_data=pt_val_dataloader, verbose=False, callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation metrics\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "ymax, ymin = plt.gca().get_ylim()\n",
    "plt.vlines(earlystopping.best_epoch, ymax=ymax, ymin=ymin, linestyle='--', color='r')\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['loss'])))\n",
    "plt.legend(['Training', 'Val loss', 'Val MAE', 'Best epoch'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom callbacks\n",
    "\n",
    "It is also possible (and often useful) to create custom callbacks to perform certain actions depending on the training progress. We will look at building a custom callback to save the model weights, dependent on the performance of a specified validation metric.\n",
    "\n",
    "Note that the `keras.callbacks` module has the built-in callback [`ModelCheckpoint`](https://keras.io/api/callbacks/model_checkpoint/), which automatically handles model saving for Keras models. The following callback is for instructive purposes only, which also shows how to manually save Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom callback for saving the model weights\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class CheckpointCallback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.best_val = keras.Variable(np.inf, trainable=False)\n",
    "                        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_mae = logs['val_mae']\n",
    "        if val_mae < self.best_val:\n",
    "            print(f\"Saving best model at epoch {epoch}\")\n",
    "            self.best_val.assign(val_mae)\n",
    "            self.model.save(self.filepath)\n",
    "\n",
    "ckpt_callback = CheckpointCallback('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `model.save()` is an alias for `keras.saving.save_model()`. See [here](https://keras.io/api/models/model_saving_apis/model_saving_and_loading/) for further information on saving and loading Keras models.\n",
    "\n",
    "When implementing your own custom callback, there are additional methods available for actions at different points during the training and validation loops. See [this guide](https://keras.io/guides/writing_your_own_callbacks/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "\n",
    "model = get_regularised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['mae']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstantiate the EarlyStopping callback\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_mae', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "# history = model.fit(tf_train_dataset, epochs=100, validation_data=tf_val_dataset, verbose=False, \n",
    "#                     callbacks=[earlystopping, ckpt_callback])\n",
    "history = model.fit(pt_train_dataloader, epochs=100, validation_data=pt_val_dataloader, verbose=False, \n",
    "                    callbacks=[earlystopping, ckpt_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the saved model file\n",
    "\n",
    "!ls model.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation metrics\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "ymax, ymin = plt.gca().get_ylim()\n",
    "plt.vlines(earlystopping.best_epoch, ymax=ymax, ymin=ymin, linestyle='--', color='r')\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['loss'])))\n",
    "plt.legend(['Training', 'Val loss', 'Val MAE', 'Saved model'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise the model\n",
    "\n",
    "model = get_regularised_model()\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['mae']) \n",
    "model.evaluate(pt_val_dataloader)  # or tf_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the best model weights\n",
    "\n",
    "model = keras.saving.load_model('model.keras')\n",
    "model.evaluate(tf_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "!rm -r model.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"weight_init\"></a>\n",
    "## Weight initialisation\n",
    "\n",
    "In practice, a good initialisation strategy for the weights of our network is often necessary to enable the network to train efficiently. With a bad initialisation, the network might take a long time to train, or might not be able to train at all. To understand why problems might arise, we need to look back to the backpropagation algorithm.\n",
    "\n",
    "We first recall the main equations from the backpropagation algorithm for an MLP network with $L$ hidden layers. The MLP is defined with the transformations:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(0)} &:= \\mathbf{x},\\\\\n",
    "\\mathbf{h}^{(k)} &= \\sigma\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, L,\\\\\n",
    "\\hat{\\mathbf{y}} &= \\sigma_{out}\\left( \\mathbf{w}^{(L)}\\mathbf{h}^{(L)} + b^{(L)} \\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$, $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$, $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{n_k}$, $\\hat{\\mathbf{y}}\\in Y$, $\\sigma, \\sigma_{out}:\\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions that are applied element-wise, $n_0 := D$, and $n_k$ is the number of units in the $k$-th hidden layer. \n",
    "\n",
    "Also recall that we define the **pre-activations**\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(k)} = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \n",
    "$$\n",
    "\n",
    "and **post-activations**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k)} = \\sigma(\\mathbf{a}^{(k)}).\n",
    "$$\n",
    "\n",
    "<center><img src=\"figures/forward.png\" alt=\"Forward pass\" style=\"width: 800px;\"/></center>\n",
    "<center>Pre-activations, post-activations, weights and biases in the forward pass</center>\n",
    "<br>\n",
    "\n",
    "Define the **error** $\\delta^{(k)}_p := \\frac{\\partial L_i}{\\partial a^{(k)}_p}$, where $L_i$ is the loss for example $i$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} &= \\delta^{(k+1)}_p h^{(k)}_q \\\\\n",
    "\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} &= \\delta^{(k+1)}_p. \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "We can summarise the backpropagation algorithm as the forward and backward pass.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "\\begin{eqnarray}\n",
    "&\\text{Forward pass:} \\\\\n",
    "&\\mathbf{a}^{(k)}  = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\\\\n",
    "           & \\mathbf{h}^{(k)}  = \\sigma(\\mathbf{a}^{(k)})  \\\\\n",
    "\\end{eqnarray}\n",
    "        </td>\n",
    "        <td>\n",
    "\\begin{eqnarray}\n",
    "            &\\text{Backward pass:} \\\\\n",
    "&\\mathbf{\\delta}^{(k)} = \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)} \\\\\n",
    "& \\sigma'(\\mathbf{a}^{(k)}) = \\text{diag}([\\sigma'(a_p^{(k)})]_{p=1}^{n_k})\n",
    "\\end{eqnarray}\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Xavier / Glorot initialisation\n",
    "\n",
    "We will look at a common initialisation strategy often referred to as Xavier or Glorot initialisation, after the lead author of the paper that first proposed it ([Glorot 2010](#Glorot10)). The idea behind this initialisation strategy is to choose a distribution from which we can sample the initial weight values, in such a way that it ensures a strong signal is able to propagate both forwards and backwards through the network in the backpropagation algorithm.\n",
    "\n",
    "At a high level, this initialisation strategy consists of choosing a univariate distribution with zero mean that each of the weight parameters can be independently sampled from. As we will see, we will need to control the variance of this weight distribution to ensure good signal flow. The biases do not play an important role, and we initialise all bias terms to zero.\n",
    "\n",
    "In order to motivate things, we can see the effects of this initialisation scheme in the following figure, taken from the original paper.\n",
    "\n",
    "<center><img src=\"figures/activation_saturation.png\" alt=\"Saturation of activations\" style=\"width: 800px;\"/></center>\n",
    "<center>Activation values of an MLP with four hidden layers over a training run</center>\n",
    "<br>\n",
    "\n",
    "In the above figure, mean and standard deviation statistics of activation values were recorded over a training run of a multilayer perceptron with four hidden layers, each of which uses a sigmoid activation function. \n",
    "\n",
    "At the start of training, the activations in the last hidden layer quickly go to zero, which is one of the saturation values of the sigmoid function. This saturation of activation values lasts for a long time in the training, suggesting that the gradients that are being computed are very small, and things only start picking up again at around epoch 100. \n",
    "\n",
    "The hypothesis made in the paper is that the root of the problem is a bad initial set of parameters, which doesn't allow a strong training signal to be propagated to all the layers of the network. In particular, at the start of training there is a strong signal, or gradient for the parameters in the last hidden layer, but this gradient diminishes very quickly as you go backwards down the layers. \n",
    "\n",
    "The figures below show histograms of activation values (left), and gradient values (right). The top and bottom rows show histograms for these values without and with Xavier initialisation respectively. \n",
    "\n",
    "The top left figure shows that in the lower layers there is a good spread of activation values. However, as you go deeper into the network these activations start to shrink, because the sigmoid activations are starting to saturate. This can be interpreted as the forward signal being diminished as you go through the layers.\n",
    "\n",
    "The top right figure shows the reverse for the gradients. The deepest hidden layer has the largest spread of values, but by the time the error has been backpropagated to the first hidden layer, the signal is significantly smaller.\n",
    "\n",
    "On the bottom row you can see the effect of the Xavier initialisation is to produce a strong signal that propagates well forwards and backwards throughout the layers of the network. In particular, the spread of activation values is roughly the same in all layers.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figures/activation_histograms.png\" alt=\"Saturation of activations\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"figures/gradient_histograms.png\" alt=\"Saturation of activations\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>Histograms of activation and gradient values</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation of the Xavier initialisation scheme makes the following assumptions:\n",
    "\n",
    "* The pre-activation values are in the linear region of the activation function: $\\sigma(\\mathbf{a}^{(k)}) \\approx \\mathbf{a}^{(k)}$, $\\sigma'(\\mathbf{a}^{(k)})\\approx I$\n",
    "* The input features $x_j$ have zero mean and shared variances ($\\text{Var}[x_j] = \\text{Var}[x]$)\n",
    "\n",
    "The first of these is a strong assumption, and later work build on the findings in the paper to relax this. The second assumption is trivially satisfied by normalising the data inputs.\n",
    "\n",
    "We will fix a zero-mean univariate distribution for each layer, and within the layer we will independently sample each of the weight parameters from this distribution.\n",
    "\n",
    "**The aim of Xavier initialisation is to control the variance of activations within each layer, as well as the variance of the errors.** We want these variances to remain roughly constant throughout all of the layers of the network.\n",
    "\n",
    "If we first consider the variance of the activations, then from the forward pass equations and the linearity of the activation function, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_j^{(k)} & = \\sum_{i=1}^{n_{k-1}} w_{ji}^{(k-1)} a_i^{(k-1)} + b_j^{(k-1)},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$. With our assumptions on the distributions of the input features and weight parameters, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}[a_j^{(k)}] &= \\text{Var}[x] \\prod_{k'=0}^{k-1} n_{k'} \\text{Var}[W^{(k')}],\\label{variances_forward}\\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\text{Var}[x]$ is the (shared) variance of each of the input features $x_j$, and $\\text{Var}[W^{(k')}]$ is the variance of the (univariate) distribution over the weights in layer $k'$.\n",
    "\n",
    "For the variance of the errors, we have from the backward pass equations\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{\\delta}^{(k)} &= \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}[\\delta_j^{(k)}] &= \\text{Var}\\left[ \\frac{\\partial L_i}{\\partial \\mathbf{a}^{(L+1)} } \\right] \\prod_{k'=k}^{L} n_{k'+1} \\text{Var}[W^{(k')}],\\label{variances_backward}\\tag{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where now $\\text{Var}\\left[ \\frac{\\partial L_i}{\\partial \\mathbf{a}^{(L+1)} } \\right]$ is the variance of the partial derivative of the per-example loss with respect to an output pre-activation, which is assumed to be approximately equal for each output neuron (which is reasonable for a typical loss function, since each $a_j^{(k)}$ has the same variance from \\eqref{variances_forward}).\n",
    "\n",
    "Remember that we want is for the signal to propagate well through the network in both forward and backward passes, such that the variances of the activations and the variances of the errors remain roughly constant throughout the layers. That is, we want to have $\\text{Var}[\\mathbf{a}^{(k)}]\\approx \\text{Var}[\\mathbf{a}^{(k')}]$ and also $\\text{Var}[\\delta^{(k)}]\\approx \\text{Var}[\\delta^{(k')}]$.\n",
    "\n",
    "From \\eqref{variances_forward} and \\eqref{variances_backward} we can see that this translates into the conditions\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n_k \\text{Var}[W^{(k)}] &= 1 \\qquad\\forall k\\quad\\text{(forward pass condition)} \\\\\n",
    "n_{k+1} \\text{Var}[W^{(k)}] &= 1 \\qquad\\forall k \\quad\\text{(backward pass condition)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unfortunately, unless subsequent layer widths are the same, we can't satisfy both of these requirements simultaneously. Xavier initialisation makes a compromise between these conditions, and sets the variance of the weight distribution to be the harmonic mean of $1 / n_k$ and $1 / n_{k+1}$:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{Var}[W^{(k)}] = \\frac{2}{n_k + n_{k+1}},\\quad k=0, 1,\\ldots, L}\n",
    "$$\n",
    "\n",
    "An example distribution that is zero mean and has the correct variance would be the uniform distribution with the following limits:\n",
    "\n",
    "$$\n",
    "w_{ij}^{(k)} \\sim U\\left[ - \\frac{\\sqrt{6}}{\\sqrt{n_k + n_{k+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n_k + n_{k+1}}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal initialisation\n",
    "\n",
    "An alternative initialisation scheme is orthogonal initialisation, proposed in [Saxe 2014](#Saxe14). The intuition behind this scheme is quite straightforward: both the forward and backward passes involve multiplication by the weight matrices $\\mathbf{W}^{(k)}$ for each $k$:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "\\begin{eqnarray}\n",
    "&\\text{Forward pass:} \\\\\n",
    "&\\mathbf{a}^{(k)}  = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\\\\n",
    "           & \\mathbf{h}^{(k)}  = \\sigma(\\mathbf{a}^{(k)})  \\\\\n",
    "\\end{eqnarray}\n",
    "        </td>\n",
    "        <td>\n",
    "\\begin{eqnarray}\n",
    "            &\\text{Backward pass:} \\\\\n",
    "&\\mathbf{\\delta}^{(k)} = \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)} \\\\\n",
    "& \\sigma'(\\mathbf{a}^{(k)}) = \\text{diag}([\\sigma'(a_p^{(k)})]_{p=1}^{n_k})\n",
    "\\end{eqnarray}\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Rather than choosing a univariate distribution to independently sample weight values from, the authors in this paper suggested to initialise each weight matrix $\\mathbf{W}^{(k)}$ to be an (semi-)orthogonal matrix. This prevents the matrix multiplication from growing or shrinking the vector of activations or errors respectively, again encouraging a strong signal to be able to propagate to all layers in both forward and backward passes.\n",
    "\n",
    "Practically, this can be done by first randomly initialising a matrix $\\overline{\\mathbf{W}}$, and computing its singular value decomposition\n",
    "\n",
    "$$\n",
    "\\overline{\\mathbf{W}} = U\\Sigma V^T.\n",
    "$$\n",
    "\n",
    "The matrices U and V in the SVD are orthogonal matrices. With the correct settings for the dimensions of these matrices, the matrix V can then be used to initialise the weights in this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "### References\n",
    "\n",
    "<a class=\"anchor\" id=\"Chen19\"></a>\n",
    "* Chen, J. & Kyrillidis, A., (2019), \"Decaying Momentum Helps Neural Network Training\", arXiv preprint arXiv:1910.04952.\n",
    "<a class=\"anchor\" id=\"Cybenko89\"></a>\n",
    "* Cybenko, G. (1989) \"Approximations by superpositions of sigmoidal functions\", Mathematics of Control, Signals, and Systems, **2** (4), 303–314.\n",
    "<a class=\"anchor\" id=\"Duchi11\"></a>\n",
    "* Duchi, J., Hazan, E., & Singer, Y. (2011), \"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\", *Journal of Machine Learning Research*, **12**, 2121–2159.\n",
    "<a class=\"anchor\" id=\"Glorot10\"></a>\n",
    "* Glorot, X. & Bengio, Y. (2010), \"Understanding the difficulty of training deep feedforward neural networks\", Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS'10).\n",
    "<a class=\"anchor\" id=\"Hanson88\"></a>\n",
    "* Hanson, S. J. & Pratt, L. Y. (1988) \"Comparing biases for minimal network construction with back-propagation\", in *Proceedings of the 1st International Conference on Neural Information Processing Systems*,  177–185.\n",
    "<a class=\"anchor\" id=\"Hardt15\"></a>\n",
    "* Hardt, M., Recht, B., & Singer, Y. (2015), \"Train faster, generalize better: Stability of stochastic gradient descent\", in *Proceedings of the 33rd International Conference on International Conference on Machine Learning*, **48**, 1225-1234.\n",
    "<a class=\"anchor\" id=\"Keskar17\"></a>\n",
    "* Keskar, N. S. * Socher, R. (2017), \"Improving Generalization Performance by Switching from Adam to SGD\", arXiv preprint, abs/1712.07628.\n",
    "<a class=\"anchor\" id=\"Kingma15\"></a>\n",
    "* Kingma, D. P. & Ba, J. L. (2015), \"Adam: a Method for Stochastic Optimization\", International Conference on Learning Representations, 1–13.\n",
    "<a class=\"anchor\" id=\"Lu17\"></a>\n",
    "* Lu, Z., Pu, H., Wang, F. Hu, Z., & Wang, L. (2017) \"The Expressive Power of Neural Networks: A View from the Width\", Advances in Neural Information Processing Systems 30. Curran Associates, Inc., 6231–6239.\n",
    "<a class=\"anchor\" id=\"Nesterov83\"></a>\n",
    "* Nesterov, Y. (1983), \"A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)\", Doklady ANSSSR (translated as Soviet. Math. Docl.), **269**, 543–547.\n",
    "<a class=\"anchor\" id=\"Pennington14\"></a>\n",
    "* Pennington, J., Socher, R. & Manning, C. D. (2014), \"Glove: Global vectors for word representation\", in *Proceedings of Empirical Methods in Natural Language Processing (EMNLP)*.\n",
    "<a class=\"anchor\" id=\"Qian99\"></a>\n",
    "* Qian, N. (1999), \"On the momentum term in gradient descent learning algorithms\", Neural Networks: The Official Journal of the International Neural Network Society, **12** (1), 145–151.\n",
    "<a class=\"anchor\" id=\"Robbins51\"></a>\n",
    "* Robbins, H. and Monro, S. (1951), \"A stochastic approximation method\", *The annals of mathematical statistics*, 400–407.\n",
    "<a class=\"anchor\" id=\"Saxe14\"></a>\n",
    "* Saxe, A., McClelland, J. and Ganguli, S. (2014), \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\", 2nd International Conference on Learning Representations, (ICLR) 2014, Banff, AB, Canada, April 14-16, 2014.\n",
    "<a class=\"anchor\" id=\"Smith15\"></a>\n",
    "* Smith, L. N. (2015), \"Cyclical Learning Rates for Training Neural Networks\", arXiv preprint, abs/1506.01186."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

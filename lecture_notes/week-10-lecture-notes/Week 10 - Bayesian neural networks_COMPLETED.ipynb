{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 10: Bayesian neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. The need for uncertainty](#need_for_uncertainty)\n",
    "\n",
    "[3. Aleatoric uncertainty (\\*)](#aleatoric_uncertainty)\n",
    "\n",
    "[4. Bayesian neural networks](#bnns)\n",
    "\n",
    "[5. Bayes by Backprop](#bayes_by_backprop)\n",
    "\n",
    "[6. Custom DenseVariational layer (\\*)](#dense_variational)\n",
    "\n",
    "[7. MC Dropout](#mc_dropout)\n",
    "\n",
    "[8. Uncertainty estimation](#uncertainty_estimation)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In the last week of the module we studied the variational autoencoder (VAE) algorithm, and saw how it can be used for inference and learning in a latent variable generative model. The VAE algorithm optimises a lower bound on the marginal likelihood (or evidence), known as the ELBO. \n",
    "\n",
    "You also saw how the VAE can be implemented in Keras. One practical challenge of the VAE is the approximation of the true posterior distribution.\n",
    "\n",
    "In this week of the module, we will look at the formulation and implementation of Bayesian neural networks. This is a subfield of probabilistic deep learning that treats the parameters of the network as random variables, and aims to perform inference on them. Therefore, what we are ultimately interested in learning is the posterior distribution over the model parameters, given the data.\n",
    "\n",
    "A key challenge in the implementation of Bayesian neural networks that is similar to the VAE is the approximation of the true posterior distribution. We will again use variational inference to approximate the posterior, and in particular two approaches that define parameterised families of distributions for the approximate posterior. These are Bayes by Backprop ([Blundell et al 2015](#Blundell15)) and MC Dropout ([Gal & Ghahramani 2016](#Gal16)).\n",
    "\n",
    "You will see how these algorithms can be implemented using Keras, and use the approximate posterior distributions to compute the predictive distribution at test time, as well as learning how these distributions can be used to quantify the uncertainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"need_for_uncertainty\"></a>\n",
    "## The need for uncertainty\n",
    "\n",
    "One of the main advantages of taking a probabilistic approach to deep learning is that it allows us to account for sources of uncertainty in our modelling process. This is an important point for critical applications, where the negative consequences of inaccurate predictions can be serious, such as autonomous driving, or medical diagnoses. \n",
    "\n",
    "Unfortunately, deep learning models are not always accurate, especially when making predictions on new data points that are dissimilar to the data that they were trained on. Worse still, what we often see in this case is that these predictions are overconfident. For example, the neural net might give a very high probability to an incorrect prediction for a data example that is unfamiliar to the model.\n",
    "\n",
    "We would like our models to be able to assign levels of uncertainty to predictions. However, a first question is what are the main sources of uncertainty that we need to take into account? \n",
    "\n",
    "The two categories of uncertainty that we will look at are often referred to as aleatoric and epistemic uncertainty. Aleatoric uncertainty is uncertainty in the data itself. For example, this could come from measurement error, noise in the labels of a dataset. Or it could come down to inherent stochasticity in the data generating process itself. This kind of uncertainty is irreducible in the sense that it represents a randomness in the problem that's always going to be there regardless of the model you use, or the amount of data you collect. We simply have to account for it.\n",
    "\n",
    "For instance, think of a simple coin flipping experiment. Let's say we have a dataset of the outcomes of a large number of coin tosses. No model is going to be able to predict with certainty what the outcome of the next coin toss is going to be. All we can do is estimate the probability of the coin turning up heads.\n",
    "\n",
    "Aleatoric uncertainty comes in two flavours: homoscedastic and heteroscedastic. The distinction comes down to whether or not the noise is dependent on the input variable. If the data uncertainty is the same for all target variables, regardless of the input, then it is homoscedastic. If the data uncertainty varies according to the input variable, then it is heteroscedastic.\n",
    "\n",
    "The other category of uncertainty is epistemic uncertainty, which is model uncertainty. Suppose we design a deep learning model for a problem, and that model has the capacity to model the underlying data distribution accurately. If there isn't enough data available then the model won't be able to learn effectively. In this case, the problem isn't down to randomness in the data, but rather to the uncertainty about which parameter values accurately model the data. In the small data regime, there are potentially many values of the parameters that could explain the data. Epistemic uncertainty is something that will decrease as we gather more data, since the model gets more information about which parameters accurately explain the data.\n",
    "\n",
    "In the rest of this week, you'll learn methods for designing your deep learning models to account for both aleatoric and epistemic uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"aleatoric_uncertainty\"></a>\n",
    "## Aleatoric uncertainty\n",
    "\n",
    "In this section we will see how we can use probabilistic modelling to account for aleatoric uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with synthetic data $(x_i, y_i)_{i=1}^{1000}$, generated from the following equation:\n",
    "\n",
    "$$\n",
    "y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.linspace(-1, 1, 1000)\n",
    "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1) \n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(x_train, y_train, alpha=0.1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homoscedastic aleatoric uncertainty\n",
    "Recall that homoscedastic noise is where the variance does not depend on the input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that captures the aleatoric uncertainty with a fixed scale\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[1]),\n",
    "    Dense(8, activation='tanh'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will interpret the output of the model above as parameterising the mean of a Normal distribution with fixed standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NLL loss function\n",
    "\n",
    "y_std = 0.5\n",
    "pi = ops.array(np.pi)\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    mean_pred = y_pred\n",
    "    log_Z = 0.5 * ops.log(2 * pi)\n",
    "    log_prob = -0.5 * ops.square((y_true - mean_pred) / y_std) - ops.log(y_std) - log_Z\n",
    "    return -ops.sum(log_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model using the negative log-likelihood\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
    "model.fit(x_train, y_train, epochs=200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample model predictions\n",
    "\n",
    "y_mean = model(x_train)\n",
    "y_samples = y_mean + keras.random.normal(y_mean.shape) * y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and model samples\n",
    "\n",
    "y_mean_minus_2std = y_mean - 2 * y_std\n",
    "y_mean_plus_2std = y_mean + 2 * y_std\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.2, label='Data')\n",
    "ax1.scatter(x_train, ops.convert_to_numpy(y_samples), alpha=0.2, color='red', label='Model samples')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.2, label='Data')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean), color='red', alpha=0.8, label=r'Model $\\mu$')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean_plus_2std), color='green', alpha=0.8, label=r'Model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean_minus_2std), color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Variable for the learned standard deviation\n",
    "\n",
    "y_std = keras.Variable([0.], name='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that captures the aleatoric uncertainty with a learned scale\n",
    "\n",
    "from keras import Model\n",
    "\n",
    "class LearnedSDMLP(Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.y_log_std = keras.Variable([0.], name='log_std')\n",
    "        inputs = Input(shape=[1])\n",
    "        h = Dense(8, activation='tanh')(inputs)\n",
    "        outputs = Dense(1)(h)\n",
    "        super().__init__(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = LearnedSDMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the NLL loss function\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    log_Z = 0.5 * ops.log(2 * pi)\n",
    "    log_prob = -0.5 * ops.square((y_true - y_pred) / ops.exp(model.y_log_std)) - model.y_log_std - log_Z\n",
    "    return -ops.sum(log_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model using the negative log-likelihood\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
    "model.fit(x_train, y_train, epochs=200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and model samples\n",
    "\n",
    "y_mean = model(x_train)\n",
    "y_std = ops.exp(model.y_log_std)\n",
    "y_samples = y_mean + keras.random.normal(y_mean.shape) * y_std\n",
    "\n",
    "y_mean_minus_2std = y_mean - 2 * y_std\n",
    "y_mean_plus_2std = y_mean + 2 * y_std\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.2, label='Data')\n",
    "ax1.scatter(x_train, ops.convert_to_numpy(y_samples), alpha=0.2, color='red', label='Model samples')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.2, label='Data')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean), color='red', alpha=0.8, label=r'Model $\\mu$')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean_plus_2std), color='green', alpha=0.8, label=r'Model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean_minus_2std), color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heteroscedastic aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that captures the aleatoric uncertainty\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[1]),\n",
    "    Dense(8, activation='tanh'),\n",
    "    Dense(2)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the NLL loss function\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    log_Z = 0.5 * ops.log(2 * pi)\n",
    "    y_mean, y_log_std = ops.split(y_pred, 2, axis=-1)\n",
    "    log_prob = -0.5 * ops.square((y_true - y_mean) / ops.exp(y_log_std)) - y_log_std - log_Z\n",
    "    return -ops.sum(log_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model using the negative log-likelihood\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
    "model.fit(x_train, y_train, epochs=200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and model samples\n",
    "\n",
    "y = model(x_train)\n",
    "y_mean, y_log_std = ops.split(y, 2, axis=-1)\n",
    "y_std = ops.exp(y_log_std)\n",
    "y_samples = y_mean + keras.random.normal(y_mean.shape) * y_std\n",
    "\n",
    "y_mean_plus_2std = y_mean - 2 * y_std\n",
    "y_mean_minus_2std = y_mean + 2 * y_std\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.2, label='Data')\n",
    "ax1.scatter(x_train, ops.convert_to_numpy(y_samples), alpha=0.2, color='red', label='Model samples')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.2, label='Data')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean), color='red', alpha=0.8, label=r'Model $\\mu$')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean_plus_2std), color='green', alpha=0.8, label=r'Model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, ops.convert_to_numpy(y_mean_minus_2std), color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise. Create a synthetic dataset of the form $y = f(x) + \\epsilon$, so that the noise is independent of the input. Design and train a probabilistic model including a separate learned standard deviation Variable as above. How well does the learned scale parameter match the noise level you defined to generate the data?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"bnns\"></a>\n",
    "## Bayesian neural networks\n",
    "\n",
    "Up until this point in the course, we have taken the approach of training our models according to the principle of maximum likelihood. This means trying to optimise our deep learning models to find the set of parameters that maximise the likelihood of the data. However, deep learning models often have millions of parameters that we are trying to fit, and maybe only tens of thousands of data points. So it's reasonable to assume that there may be many settings of our parameters that lead to models that fit the data well.\n",
    "\n",
    "With Bayesian neural networks, what we're ultimately interested in doing is learning a distribution over the parameters of our model that reflects the probability that various settings of the parameters could explain the data. In other words, we treat the network parameters as random variables, and perform inference on them.\n",
    "\n",
    "By learning this distribution over the model parameters, you can view this as learning an ensemble of deep learning models. We can sample models by sampling from this distribution, each of which should give us a good fit to the data.\n",
    "\n",
    "By taking this approach, there are two main benefits that we're aiming for. One is improved accuracy - because we're learning a potentially infinite ensemble of networks, if we average our predictions over the ensemble, we might expect that this will improve our predictions. The second typical motivation that people have to look at Bayesian neural networks is uncertainty estimation, or giving the model the ability to quantify how confident it is about its predictions. \n",
    "\n",
    "In this week we will first look at popular methods for developing Bayesian neural networks, and then how we can  use these models to quantify different sources of uncertainty.\n",
    "\n",
    "There are several different approaches to developing Bayesian neural networks that have been proposed in the literature, including the Laplace approximation ([MacKay 1995](#MacKay92)), limiting behaviour in the large width limit ([Neal 1995](#Neal95)), Bayes by Backprop ([Blundell et al 2015](#Blundell15)), MC Dropout ([Gal & Ghahramani 2016](#Gal16)), stochastic weight averaging ([Izmailov et al 2018](#Izmailov18), [Maddox et al 2019](#Maddox19)) and even Deep Ensembles ([Lakshminarayanan et al 2017](#Lakshminarayanan15)) can be viewed as a Bayesian approximation ([Wilson & Izmailov 2020](#Wilson20)). However, the core aims of these approaches are the same: they seek to approximate the posterior distribution over the model parameters, given the data, and assuming a suitable prior distribution. This posterior distribution can then be used to compute the predictive distribution, which means averaging our predictions over networks, where networks are weighted according to the posterior distribution over the parameters.\n",
    "\n",
    "#### Supervised learning\n",
    "\n",
    "We will start by briefly reviewing the typical supervised learning setting, where we have a dataset $\\mathcal{D} = (x_n, y_n)_{n=1}^N$ of inputs $x_n$ and outputs $y_n$, and we want to model the data with a neural network that depends on parameters $\\theta$. We will assume that the network outputs a distribution $p(y \\mid x, \\theta)$ over predicted values $y$.\n",
    "\n",
    "The principle we often use to train models like this is maximum likelihood. If we assume our data is i.i.d., then we can write the data likelihood as a product of likelihoods over each data point:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D} \\mid \\theta) = \\prod_{n=1}^N p(y_n \\mid x_n, \\theta),\n",
    "$$\n",
    "\n",
    "and so the log-likelihood becomes the sum\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D} \\mid \\theta) = \\sum_{n=1}^N \\log p(y_n \\mid x_n, \\theta).\n",
    "$$\n",
    "\n",
    "This is just the log-probability that our model with parameters theta assigns to the dataset. And what we'd like to do is find the parameters that maximise this likelihood. Recall that since the logarithm is a strictly concave function, a given value theta will maximise the likelihood if and only if it also maximises the log-likelihood. So when we optimise the network, we search for the maximum likelihood estimate\n",
    "\n",
    "$$\n",
    "\\theta_{MLE} = \\underset{\\theta}{\\arg\\max} \\left[ \\log p(\\mathcal{D} \\mid \\theta) \\right].\n",
    "$$\n",
    "\n",
    "In a first step towards being Bayesian, we might also consider a prior distribution $p(\\theta)$, and consider trying to find parameters $\\theta$ that maximise the posterior distribution $p(\\theta \\mid \\mathcal{D})$. Remember that the posterior distribution can be calculated according to Bayes' rule\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D} \\mid \\theta)p(\\theta)}{p(\\mathcal{D})}.\\tag{1}\n",
    "$$\n",
    "\n",
    "On the right hand side, we have the likelihood $p(\\mathcal{D}\\mid\\theta)$ multiplied by the prior $p(\\theta)$, divided by the marginal likelihood (or model evidence) $p(\\mathcal{D})$.\n",
    "\n",
    "Maximising the posterior $p(\\theta\\mid\\mathcal{D})$ is the same as maximising the log-posterior $\\log p(\\theta\\mid\\mathcal{D})$, which we can write as a sum of log terms\n",
    "\n",
    "$$\n",
    "\\log p(\\theta\\mid\\mathcal{D}) = \\log p(\\mathcal{D} \\mid \\theta) + \\log p(\\theta) - \\log p(\\mathcal{D}).\n",
    "$$\n",
    "\n",
    "The log marginal likelihood $\\log p(\\mathcal{D})$ doesn't depend on the parameters $\\theta$, so maximising the posterior is equivalent to maximising the sum of the log likelihood and the log prior:\n",
    "$$\n",
    "\\theta_{MAP} = \\underset{\\theta}{\\arg\\max} \\left[ \\log p(\\mathcal{D} \\mid \\theta) + \\log p(\\theta) \\right].\n",
    "$$\n",
    "\n",
    "#### Predictive distribution\n",
    "\n",
    "In Bayesian neural networks, we are interested in computing an approximation to the true posterior distribution $p(\\theta \\mid \\mathcal{D}$. Computing the expression $(1)$ above is intractable, mainly due to the normalising constant $p(\\mathcal{D})$. Computing this term directly would involve evaluating the integral \n",
    "\n",
    "$$\n",
    "\\int_{\\theta} p(\\mathcal{D}\\mid\\theta)p(\\theta) d\\theta,\n",
    "$$\n",
    "\n",
    "which for all but the most trivial settings has no closed form solution, and is not possible to compute within a reasonable time.\n",
    "\n",
    "But in principle, if we had access to the posterior $p(\\theta\\mid\\mathcal{D})$, then this would give us some information about the level of uncertainty in our model. For example, if the posterior was very highly concentrated in the parameter space then this would indicate a low level of model uncertainty, whereas if the posterior is very spread out, this could be indicative of high model uncertainty.\n",
    "\n",
    "We could also use the posterior to average our predictions over all possible models that explain the data. This is what is represented by the predictive distribution\n",
    "\n",
    "$$\n",
    "p(y^* \\mid x^*, \\mathcal{D}) = \\int_{\\theta} p(y^* \\mid x^*, \\theta) p(\\theta \\mid \\mathcal{D}) d\\theta,\\tag{2}\n",
    "$$\n",
    "\n",
    "where we use the posterior $p(\\theta \\mid \\mathcal{D})$ to weight the predictions of our model $p(y^* \\mid x^*, \\theta)$. For a given test input $x^*$, we produce a distribution over target values $y^*$, by computing a Bayesian model average, marginalising out the parameters $\\theta$. It's worth emphasising the important difference here to what we normally do when we use deep learning models at test time: instead of using a single set of parameters $\\theta$ to produce our predictions, we use all settings of the parameters, weighted by their posterior probabilities.\n",
    "\n",
    "Since deep learning models are typically under-specified, there will likely be many parameter settings that explain the data, and the posterior is likely to be spread out. All these competing parameter settings correspond to a potentially very diverse range of model hypotheses for the data. That means that there is likely to be a significant difference between the predictions of a single model (with a single set of parameters) and the Bayesian model average over an ensemble of many different but high performing models.\n",
    "\n",
    "#### Posterior approximation\n",
    "\n",
    "We can't directly compute the posterior distribution $p(\\theta\\mid\\mathcal{D})$, so we approximate it with a distribution $q_\\phi(\\theta)$, which itself has parameters $\\phi$. The distribution $q$ is the variational distribution, and our objective is to try to optimise its parameters to match it with the true posterior.\n",
    "\n",
    "Once we've computed our approximate posterior $q$, then we can use this to approximate the predictive distribution:\n",
    "\n",
    "$$\n",
    "p(y^* \\mid x^*, \\mathcal{D}) \\approx \\int_{\\theta} p(y^* \\mid x^*, \\theta) q_\\phi(\\theta) d\\theta.\\tag{3}\n",
    "$$\n",
    "\n",
    "Unfortunately, we can't tractably compute this integral either, but we will be able to sample from the distribution $q$, so in practice we'll resort to approximating the predictive distribution with Monte Carlo samples:\n",
    "\n",
    "$$\n",
    "p(y^* \\mid x^*, \\mathcal{D}) \\approx \\frac{1}{K} \\sum_{k=1}^K p(y^* \\mid x^*, \\theta_k),\\qquad \\theta_k\\sim q_\\phi(\\theta).\\tag{4}\n",
    "$$\n",
    "\n",
    "#### Evidence Lower bound (ELBO)\n",
    "\n",
    "We have now described the setting of the problem, and what we would like to be able to do. The question now is how we go about optimising the parameters $\\phi$ in order to match the true posterior $p(\\theta\\mid\\mathcal{D})$ as closely as possible.\n",
    "\n",
    "We will try to minimise the discrepancy between the variational approximation and the true posterior, as measured by the Kullback-Leibler divergence between $q$ and $p$. We have that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(q_\\phi(\\theta) \\mid\\mid p(\\theta\\mid\\mathcal{D})) &= \\mathbb{E}_{q_\\phi(\\theta)} [\\log q_\\phi(\\theta) - \\log p(\\theta \\mid\\mathcal{D}) ] \\\\\n",
    "&=  \\mathbb{E}_{q_\\phi(\\theta)} [\\log q_\\phi(\\theta) - \\log p(\\mathcal{D} \\mid\\theta) - \\log p(\\theta) + \\log p(\\mathcal{D}) ] \\\\\n",
    "&= \\log p(\\mathcal{D}) + \\mathbb{E}_{q_\\phi(\\theta)} [\\log q_\\phi(\\theta) -  \\log p(\\mathcal{D} \\mid\\theta) - \\log p(\\theta) ] \\\\\n",
    "&= \\log p(\\mathcal{D}) - \\left\\{ \\mathbb{E}_{q_\\phi(\\theta)} [\\log p(\\mathcal{D}\\mid\\theta) ] - D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ) \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term in the curly braces in the last expression is the ELBO. This is a lower bound on the marginal likelihood, since $D_{KL}(q_\\phi(\\theta) \\mid\\mid p(\\theta\\mid\\mathcal{D})) \\ge 0$ implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "ELBO &:= \\mathbb{E}_{q_\\phi(\\theta)} [\\log p(\\mathcal{D}\\mid\\theta) ] - D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ) \\tag{5} \\\\\n",
    " &\\le \\log p(\\mathcal{D}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Furthermore, because the model evidence is independent of the distribution $q$, we can also see that minimising the KL divergence between the variational distribution $q$ and the true posterior $p$ is equivalent to maximising the ELBO.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, the ELBO expression in the context of Bayesian neural networks is $(5)$, which you can think of as a sum of an expected log likelihood under the approximate posterior $q$, and the negative KL-divergence between the approximate posterior $q_\\phi(\\theta)$ and the prior distribution $p(\\theta)$.\n",
    "\n",
    "This ELBO expression is a lower bound on the model evidence, and minimising the KL divergence between the variational posterior and the true posterior is equivalent to maximising the ELBO. This optimisation is with respect to the choice of variational posterior $q$, which in turn depends on the variational parameters $\\phi$. In practice, what we often actually do is minimise the negative ELBO.\n",
    "\n",
    "This derivation of the ELBO objective is quite similar to what we've seen with the variational autoencoder ([Kingma & Welling 2014](#Kingma14), [Rezende & Mohamed 2015](#Rezende15)), but there are differences. One important difference is that one of the terms of our ELBO expression depends on the data, and the other term doesn't.\n",
    "\n",
    "The first term $\\mathbb{E}_{q_\\phi(\\theta)} [\\log p(\\mathcal{D}\\mid\\theta) ]$ is a data-fitting term. This term captures how well our approximate posterior explains the data $\\mathcal{D}$. \n",
    "\n",
    "You can think of the second term $- D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) )$ as a regularisation term, which measures how far the variational posterior diverges from the prior distribution over the model parameters. This second term doesn't depend on the data at all. \n",
    "\n",
    "We'll see later on how this property affects the practical optimisation of the ELBO objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a class=\"anchor\" id=\"bayes_by_backprop\"></a>\n",
    "## Bayes by Backprop\n",
    "\n",
    "Bayes by Backprop ([Blundell et al 2015](#Blundell15)) is a popular approach in Bayesian neural networks, which assumes a particular form of the parameterised family of distributions for the variational posterior.\n",
    "\n",
    "Recall that our maximisation objective is the ELBO:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\mathcal{D}) := \\mathbb{E}_{q_\\phi(\\theta)} [\\log p(\\mathcal{D}\\mid\\theta) ] - D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) )\\tag{6} \n",
    "$$\n",
    "\n",
    "Maximising the ELBO is equivalent to minimising the Kullback-Leibler divergence between the approximate and the true posterior. In the context of Bayesian neural networks, the ELBO objective consists of two terms; the first is the expected log likelihood with respect to the approximate posterior. Maximising this term encourages our approximate posterior to fit the data. The second term is a KL-divergence term, which you can think of as a regularisation term. Minimising this term keeps the approximate posterior close to the prior distribution over the model parameters.\n",
    "\n",
    "An important property of the ELBO to notice is that only the first term depends on the dataset $\\mathcal{D}$.\n",
    "\n",
    "Bayes by backprop refers to a particular choice of the variational family of distributions $q_\\phi$. Suppose we are considering a multilayer perceptron model for a supervised learning problem. Our model consists of a number of hidden layers, and weights and biases that parameterise the transformations from layer to layer.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(0)} &:= \\mathbf{x}, \\\\\n",
    "\\mathbf{h}^{(k)} &= \\sigma\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, L,\\\\\n",
    "\\hat{y} &= \\sigma_{out}\\left( \\mathbf{w}^{(L)}\\mathbf{h}^{(L)} + b^{(L)} \\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$, $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$, $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{n_k}$, and we have set $n_0 := D$, and $n_k$ is the number of units in the $k$-th hidden layer.\n",
    "\n",
    "<center><img src=\"figures/mlp.png\" style=\"width: 700px;\"/></center>\n",
    "<center>A multilayer perceptron model</center>\n",
    "<br>\n",
    "\n",
    "The collection of model parameters $\\theta$ is given by the set of weights and biases:\n",
    "\n",
    "$$\n",
    "\\theta = \\{w^{(k)}_{ij}, b_i^{(k)}\\}, \\qquad k=0,\\ldots,L,\\quad i=1,\\ldots,n_{k+1},\\quad j=1,\\ldots,n_k,\n",
    "$$\n",
    "where $w^{(k)}_{ij} := (\\mathbf{W}^{(k)})_{ij}$ and  $b_i^{(k)} := (\\mathbf{b}^{(k)})_i$.\n",
    "\n",
    "Bayes by backprop makes the choice to use a mean field approximation for the variational posterior. This means that we define the approximate posterior as a fully factorised distribution, so it can be written as the product of individual distributions over each individual weight and bias.\n",
    "\n",
    "$$\n",
    "q_\\phi(\\theta) = \\prod_{k=0}^L \\left( \\prod_{i,j} q_\\phi(w_{ij}^{(k)}) \\prod_i q_\\phi(b_i^{(k)}) \\right),\n",
    "$$\n",
    "\n",
    "where we are abusing the notation by using the same $q_\\phi$ for each of the distributions in these products. In principle these are different distributions for each weight and bias.\n",
    "\n",
    "In particular, Bayes by backprop chooses to use normal distributions for each one of the distributions in the above expression. So we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\phi(w_{ij}^{(k)}) = N(w_{ij}^{(k)}; M_{ij}^{(k)}, V_{ij}^{(k)}) \\\\\n",
    "q_\\phi(b_{i}^{(k)}) = N(b_{i}^{(k)}; m_{i}^{(k)}, v_{i}^{(k)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then our variational parameters $\\phi$ are the set of mean and variance parameters above:\n",
    "\n",
    "$$\n",
    "\\phi = \\{M^{(k)}_{ij}, V_{ij}^{(k)}, m^{(k)}_{i}, v_{i}^{(k)}\\}, \\qquad k=0,\\ldots,L,\\quad i=1,\\ldots,n_{k+1},\\quad j=1,\\ldots,n_k.\n",
    "$$\n",
    "\n",
    "These variational parameters are what we want to optimise in order to maximise the ELBO objective $(6)$. We would like to optimise our objective function using stochastic gradient descent, or some other gradient-based optimiser. However, our objective function is defined with an expectation over a distribution that depends on the parameters that we want to optimise.\n",
    "\n",
    "We can use the same reparameterisation trick that we used in the variational autoencoder again here.\n",
    "\n",
    "To recap how that works, note that our variational posterior $q_\\phi(\\theta)$ can be reparameterised such that sampling from the variational posterior is equivalent to first sampling an auxiliary noise variable $\\epsilon$, and then transforming that noise variable with the deterministic transformation $g_\\phi$:\n",
    "\n",
    "$$\n",
    "\\theta \\sim q_\\phi(\\theta) \\qquad \\Longleftrightarrow \\qquad \\epsilon \\sim p(\\epsilon),\\quad \\theta = g_\\phi(\\epsilon).\n",
    "$$\n",
    "\n",
    "In particular, we can define the noise distribution $p(\\epsilon) = N(\\mathbf{0}, \\mathbf{I}_P)$ as a diagonal Gaussian with zero mean, and identity covariance matrix. Here $P = |\\theta |$ is the total number of weights and biases in our model. We can then just shift and scale our noise sample with mean $\\mu$ and standard deviation $\\sigma$:\n",
    "\n",
    "$$\n",
    "g_\\phi(\\epsilon) = \\mu + \\sigma \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are given by\n",
    "\n",
    "$$\n",
    "\\mu = \\left[\n",
    "\\begin{array}{c}\n",
    "\\vdots \\\\\n",
    "M_{ij}^{(k)} \\\\\n",
    "\\vdots \\\\\n",
    "m_{ij}^{(k)} \\\\\n",
    "\\vdots\n",
    "\\end{array}\n",
    "\\right],\\qquad\n",
    "\\sigma = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "\\ddots & 0 & \\cdots & 0 & \\cdots \\\\\n",
    "0 & \\sqrt{V_{ij}^{(k)}} & \\cdots & 0 & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sqrt{v_i^{(k)}} & 0 \\\\\n",
    "0 & 0 & \\cdots & 0 & \\ddots\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "With this reparameterisation trick, we can rewrite our objective function to use the expectation over the noise distribution instead of the variational posterior. \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\mathcal{D}) := \\mathbb{E}_{p(\\epsilon)} [\\log p(\\mathcal{D}\\mid g_\\phi(\\epsilon)) ] - D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ), \\tag{7} \n",
    "$$\n",
    "\n",
    "and now we can take the gradient with respect to $\\phi$ inside the expectation.\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi\\mathcal{L}(\\phi, \\mathcal{D}) = \\mathbb{E}_{p(\\epsilon)} [\\nabla_\\phi \\log p(\\mathcal{D}\\mid g_\\phi(\\epsilon)) ] - \\nabla_\\phi D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ).\n",
    "$$\n",
    "\n",
    "Note that if the prior $p(\\theta)$ is also chosen to be a Gaussian distribution, then the KL divergence term can be computed analytically. In this case, we can then take gradients of this term with respect to $\\phi$ directly.\n",
    "\n",
    "In practice, we will use an objective function that approximates the ELBO using Monte Carlo samples:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}) := \\left[\\frac{1}{K} \\sum_{j=1}^K \\log p(\\mathcal{D}\\mid g_\\phi(\\epsilon^{(j)})) \\right] - D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ),\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon).\\tag{8}\n",
    "$$\n",
    "\n",
    "In the above expression we're using $K$ Monte Carlo samples, but in practice people tend to use $K=1$. This is our objective in the case that the KL divergence between the approximate posterior and the prior can be analytically computed.\n",
    "\n",
    "Of course this might not be the case, and then we would need to approximate the KL divergence term as well. To do that, we can use the fact that\n",
    "\n",
    "$$\n",
    "D_{KL}(q_\\phi(\\theta) \\mid\\mid p(\\theta)) = \\mathbb{E}_{p(\\epsilon)} [\\log q_\\phi(g_\\phi(\\epsilon)) - \\log p(g_\\phi(\\epsilon)) ].\n",
    "$$\n",
    "\n",
    "We can use the same reparameterisation trick again here, and our Monte Carlo objective now takes the following form.\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\phi, \\mathcal{D}) := \\frac{1}{K} \\left[ \\sum_{j=1}^K \\log p(\\mathcal{D}\\mid g_\\phi(\\epsilon^{(j)})) -\\log q_\\phi(\\theta^{(j)}) + \\log p(\\theta^{(j)}) \\right],\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon).\\tag{9}\n",
    "$$\n",
    "\n",
    "One more thing to consider with these objective functions is that the first term is an evaluation of the full dataset likelihood. In practice this isn't feasible to compute except for very small datasets, and we'll need to use minibatches in our training run as usual.\n",
    "\n",
    "However, we need to be a bit careful with how to compute our objective when using minibatches. Remember that only the first term in our objective depends on the dataset, and the KL divergence is independent of the data. This is true for both forms of our objective $(8)$ and $(9)$.\n",
    "\n",
    "So if we only compute the data likelihood over a minibatch, then we need to scale up the minibatch likelihood so that it's an unbiased estimator of the full data likelihood.\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\phi, \\mathcal{D}_m) := \\frac{1}{K} \\sum_{j=1}^K  \\left[ \\frac{N}{M} \\log p(\\mathcal{D}_m\\mid g_\\phi(\\epsilon^{(j)})) -\\log q_\\phi(\\theta^{(j)}) + \\log p(\\theta^{(j)}) \\right],\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}_m) := \\left[\\frac{1}{K} \\cdot \\frac{N}{M} \\sum_{j=1}^K \\log p(\\mathcal{D}_m\\mid g_\\phi(\\epsilon^{(j)})) \\right] - D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ),\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)\n",
    "$$\n",
    "\n",
    "In the above, $\\mathcal{D}_m$ indicates a minibatch of data, $|\\mathcal{D}_m| = M$, and $N$ is the size of the full dataset. Again in practice, because we usually compute average data log-likelihoods over a minibatch, it's more convenient to work with the following scaled version of the objective.\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\phi, \\mathcal{D}_m) := \\frac{1}{K} \\sum_{j=1}^K  \\left[ \\frac{1}{M} \\log p(\\mathcal{D}_m\\mid g_\\phi(\\epsilon^{(j)})) - \\frac{1}{N} \\left( \\log q_\\phi(\\theta^{(j)}) + \\log p(\\theta^{(j)}) \\right) \\right],\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)\\tag{10}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}_m) := \\left[\\frac{1}{K} \\cdot \\frac{1}{M} \\sum_{j=1}^K \\log p(\\mathcal{D}_m\\mid g_\\phi(\\epsilon^{(j)})) \\right] - \\frac{1}{N} D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ),\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)\\tag{11}\n",
    "$$\n",
    "\n",
    "The first term in both $(10)$ and $(11)$ is now an average log-likelihood over the minibatch (as we normally compute), and the KL-divergence terms have a weighting of $1/N$. These final two expressions are the objectives that are typically implemented for the Bayes by Backprop algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dense_variational\"></a>\n",
    "## Custom DenseVariational layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will see how the Bayes by Backprop algorithm can be implemented with use of the `DenseVariational` probabilistic layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to work with synthetic data $(x_i, y_i)_{i=1}^{1000}$, generated from the following equation:\n",
    "\n",
    "$$\n",
    "y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.linspace(-1, 1, 1000)\n",
    "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the Bayes by Backprop objective (with or without the exact KL-divergence term):\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\phi, \\mathcal{D}_m) := \\frac{1}{K} \\sum_{j=1}^K  \\left[ \\frac{1}{M} \\log p(\\mathcal{D}_m\\mid g_\\phi(\\epsilon^{(j)})) - \\frac{1}{N} \\left( \\log q_\\phi(\\theta^{(j)}) + \\log p(\\theta^{(j)}) \\right) \\right],\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}_m) := \\left[\\frac{1}{K} \\cdot \\frac{1}{M} \\sum_{j=1}^K \\log p(\\mathcal{D}_m\\mid g_\\phi(\\epsilon^{(j)})) \\right] - \\frac{1}{N} D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ),\\qquad \\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom layer\n",
    "\n",
    "from keras.layers import Layer, Activation\n",
    "\n",
    "class DenseVariational(Layer):\n",
    "\n",
    "    def __init__(self, units, activation=None, kl_weight=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        in_units = inputs_shape[-1]\n",
    "        self.kernel_mean = self.add_weight(\n",
    "            name='kernel_mean',\n",
    "            shape=(in_units, self.units),\n",
    "            initializer='glorot_uniform'\n",
    "        )\n",
    "        self.kernel_logstd = self.add_weight(\n",
    "            name='kernel_logstd',\n",
    "            shape=(in_units, self.units),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        self.bias_mean = self.add_weight(\n",
    "            name='bias_mean',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        self.bias_logstd = self.add_weight(\n",
    "            name='bias_logstd',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        self.activation_fn = Activation(self.activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        kernel = self.kernel_mean + (keras.random.normal(self.kernel_mean.shape) * ops.exp(self.kernel_logstd))\n",
    "        bias = self.bias_mean + (keras.random.normal(self.bias_mean.shape) * ops.exp(self.bias_logstd))\n",
    "        outputs = self.activation_fn(inputs @ kernel + bias)\n",
    "\n",
    "        kernel_kl_loss = 0.5 * (ops.square(self.kernel_mean) + ops.exp(2 * self.kernel_logstd) - 1.0 - (2 * self.kernel_logstd))\n",
    "        bias_kl_loss = 0.5 * (ops.square(self.bias_mean) + ops.exp(2 * self.bias_logstd) - 1.0 - (2 * self.bias_logstd))\n",
    "        kl_loss = ops.sum(kernel_kl_loss) + ops.sum(bias_kl_loss)\n",
    "        if self.kl_weight is not None:\n",
    "            kl_loss = kl_loss * self.kl_weight\n",
    "        self.add_loss(kl_loss)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that captures the aleatoric and epistemic uncertainty\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Input\n",
    "\n",
    "N = x_train.shape[0]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[1]),\n",
    "    DenseVariational(8, activation='tanh', kl_weight=1/N),\n",
    "    DenseVariational(2, kl_weight=1/N)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NLL loss function\n",
    "\n",
    "pi = ops.array(np.pi)\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    log_Z = 0.5 * ops.log(2 * pi)\n",
    "    y_mean, y_log_std = ops.split(y_pred, 2, axis=-1)\n",
    "    log_prob = -0.5 * ops.square((y_true - y_mean) / ops.exp(y_log_std)) - y_log_std - log_Z\n",
    "    return -ops.sum(log_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
    "history = model.fit(x_train, y_train, epochs=300, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and an ensemble of model predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_train, y_train, marker='.', alpha=0.2, label='Data')\n",
    "\n",
    "ensemble_size = 5\n",
    "\n",
    "for i in range(ensemble_size):\n",
    "    y_pred = model(x_train)\n",
    "    y_mean, y_log_std = ops.split(y_pred, 2, axis=-1)\n",
    "    y_std = ops.exp(y_log_std)\n",
    "    y_mean = ops.convert_to_numpy(y_mean)\n",
    "    y_std = ops.convert_to_numpy(y_std)\n",
    "    y_mean_plus_2std = y_mean - 2 * y_std\n",
    "    y_mean_minus_2std = y_mean + 2 * y_std\n",
    "    if i == 0:\n",
    "        plt.plot(x_train, y_mean, color='red', alpha=0.8, label=r'Model $\\mu$')\n",
    "        plt.plot(x_train, y_mean_plus_2std, color='green', alpha=0.8, label=r'Model $\\mu \\pm 2 \\sigma$')\n",
    "        plt.plot(x_train, y_mean_minus_2std, color='green', alpha=0.8)\n",
    "    else:\n",
    "        plt.plot(x_train, y_mean, color='red', alpha=0.8)\n",
    "        plt.plot(x_train, y_mean_plus_2std, color='green', alpha=0.8)\n",
    "        plt.plot(x_train, y_mean_minus_2std, color='green', alpha=0.8)        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mc_dropout\"></a>\n",
    "## MC Dropout\n",
    "\n",
    "In this section, we will go through an approach to Bayesian neural network modelling known as MC Dropout ([Gal & Ghahramani 2016](#Gal16)). We have seen Dropout already earlier in the course, and it is very commonly used in neural network training. As we'll see, there is a nice Bayesian interpretation for training regularised neural networks with dropout, which motivates the use of dropout at test time as well as in training.\n",
    "\n",
    "To recap the problem setting: we assume a prior distribution $p(\\theta)$ over the model weights, and given the dataset $\\mathcal{D}$, we would like to approximate the true posterior given the data $p(\\theta\\mid\\mathcal{D})$, with a variational distribution $q_\\phi(\\theta)$.\n",
    "\n",
    "We choose to minimise the Kullback-Leibler divergence $D_{KL}(q_\\phi(\\theta) \\mid\\mid p(\\theta\\mid\\mathcal{D}))$, which turns out to be equivalent to maximising the ELBO objective\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\mathcal{D}) = \\mathbb{E}_{q_\\phi(\\theta)} [ \\log p(\\mathcal{D}\\mid\\theta)] - D_{KL}(q_\\phi(\\theta)\\mid\\mid p(\\theta))\\tag{12}\n",
    "$$\n",
    "\n",
    "We can view the ELBO objective as the sum of two terms, the first of which tries to maximise the expected log-likelihood of the dataset under our variational distribution $q_\\phi$, and the second term is the negative KL-divergence between $q_\\phi(\\theta)$ and the prior $p(\\theta)$, which regularises $q_\\phi$ so that it doesn't diverge too far from the prior. Recall that only the first of these two terms depends on the dataset $\\mathcal{D}$.\n",
    "\n",
    "We will again assume a diagonal Gaussian distribution for the prior, with zero mean and identity covariance matrix.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "Dropout is usually introduced as a training technique, where weight connections in the network are randomly dropped out according to some fixed probability. \n",
    "\n",
    "<center><img src=\"figures/dropout.png\" alt=\"Dropout\" style=\"width: 700px;\"/></center>\n",
    "<center>Neural network with dropout</center>\n",
    "<br>\n",
    "\n",
    "This has a regularisation effect on the network, but perhaps more importantly encourages robustness of features learned by the network. \n",
    "\n",
    "Let's consider that we're modelling our data with a multilayer perceptron with $L$ hidden layers and denote $\\mathbf{W}^{(k)}$ to be the weight matrix that define the transformation from hidden layer $k$ to hidden layer $k+1$, and similarly for the bias terms $\\mathbf{b}^{(k)}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k+1)} = \\sigma\\left( \\mathbf{W}^{(k)}\\mathbf{h}^{(k)} + \\mathbf{b}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "Also define $n_k$ to be the number of neurons in layer $k$ where the input layer is the zero-th layer. Then the matrix $\\mathbf{W}^{(k)}$ has shape $n_{k+1}\\times n_k$, and $\\mathbf{b}^{(k)}$ is a vector of length $n_{k+1}$.\n",
    "\n",
    "There are two equivalent ways of thinking about dropout: either as randomly dropping out neurons within a hidden layer, or by randomly dropping out the weight connections between hidden layers. We're going to be thinking of the latter, so the dropout technique can be summarised as follows: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{(k)} &\\leftarrow \\mathbf{W}^{(k)} \\cdot\\text{diag} ([\\mathbf{z}_{k, j}]_{j=1}^{n_{k-1}})\\\\\n",
    "\\mathbf{z}_{k, j} &\\sim \\text{Bernoulli}(p_k), \\qquad k=0,\\ldots, L,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "On each pass through the network, we multiply the weight matrix $\\mathbf{W}^{(k)}$ by a diagonal matrix $\\text{diag} ([\\mathbf{z}_{k, j}]_{j=1}^{n_{k-1}})$, where the diagonal elements are each independently sampled from a Bernoulli distribution with probability $p_k$. This has the effect of randomly dropping out columns of the matrix $\\mathbf{W}^{(k)}$, independently with probability $1 - p_k$. In this notation, $p_k$ is the keep rate, whereas the dropout rate is $1 - p_k$.\n",
    "\n",
    "Note that we are introducing dropout into all of the weight connections of the network, including the first set of weights that maps the inputs $\\mathbf{x}$ to the first hidden layer. Usually you wouldn't use dropout on the input layer, which is what you see in the diagram above, but this really depends on the nature of the data. If dropout is not being used on $\\mathbf{W}^{(0)}$, then this is equivalent to setting $p_0=1$. \n",
    "\n",
    "The insight here is that dropout can be viewed as inducing a distribution over the model weights. And since the columns of each weight matrix $\\mathbf{W}^{(k)}$ are dropped out independently, this is a highly multimodal distribution. \n",
    "\n",
    "#### Variational distribution\n",
    "\n",
    "We will denote the parameters of the model \n",
    "\n",
    "$$\n",
    "\\theta = \\left\\{\\mathbf{W}^{(k)}_j, \\mathbf{b}^{(k)} \\right\\},\n",
    "$$ \n",
    "\n",
    "where $\\mathbf{W}^{(k)}_j \\in\\mathbb{R}^{n_{k+1}}$ is the $j$-th column of $\\mathbf{W}^{(k)}$, and $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ is the bias vector for the $k$-th hidden layer.\n",
    "\n",
    "In MC Dropout, we will use the dropout distribution as the variational distribution $q_\\phi$. In this case, we could write our variational distribution $q_\\phi$ as follows:\n",
    "\n",
    "$$\n",
    "q_\\phi(\\theta) = \\prod_{k=0}^L \\delta(\\mathbf{b}^{(k)} = \\mathbf{m}^{(k)}) \\prod_{j=1}^{n_k} \\left ( p_k \\delta(\\mathbf{W}_j^{(k)} = \\mathbf{M}_j^{(k)}) + (1-p_k) \\delta(\\mathbf{W}_j^{(k)} = \\mathbf{0})\\right)\\tag{13}\n",
    "$$\n",
    "\n",
    "We are not applying dropout to the bias terms, so we can write this as a delta distribution, where the bias vector $\\mathbf{b}^{(k)}$ is fixed to the value $\\mathbf{m}^{(k)}$. For the weight matrix $\\mathbf{W}^{(k)}$, we can write the dropout distribution as a weighted sum of delta distributions, weighted by the keep rate $p_k$. With probability $p_k$, the $j$-th column of the weight matrix $\\mathbf{W}^{(k)}$ will equal the vector value $\\mathbf{M}_j^{(k)}$, and with probability $1 - p_k$, the $j$-th column of $\\mathbf{W}^{(k)}$ will be zeros.\n",
    "\n",
    "So now $\\mathbf{M}_j^{(k)}$ and $\\mathbf{m}^{(k)}$ ($k=0,\\ldots,L$, $j=1,\\ldots,n_k$) are our variational parameters, that we'd like to optimise:\n",
    "\n",
    "$$\n",
    "\\phi = \\left\\{ \\mathbf{M}_j^{(k)}, \\mathbf{m}^{(k)} \\right\\}_{j, k}.\n",
    "$$\n",
    "\n",
    "We would like to optimise our ELBO objective $(12)$ with a gradient-based optimiser as usual, but we have the same problem that the objective contains an expectation over the distribution $q_\\phi$, which depends on the variational parameters $\\phi$. We can use the same reparameterisation trick again here that we used before.\n",
    "\n",
    "We introduce the auxiliary noise variable $\\epsilon$ as the collection of noise variables\n",
    "\n",
    "$$\n",
    "\\epsilon = \\left\\{ \\epsilon_j^{(k)} \\right\\}_{j, k},\\qquad \\epsilon^{(k)}_j \\sim \\text{Bernoulli}(p_k),\\tag{14}\n",
    "$$\n",
    "\n",
    "where again $k=0,\\ldots,L$ indexes the layers and $j=1,\\ldots,n_k$ indexes the neurons in each layer. We can also write the transformation \n",
    "\n",
    "$$\n",
    "g_\\phi(\\epsilon) = \\left\\{ g_{\\phi,j}^{(k)}(\\epsilon) \\right\\}_{j, k},\\qquad g_{\\phi,j}^{(k)}(\\epsilon) = \\left( \\epsilon_j^{(k)}\\mathbf{M}^{(k)}_j, \\mathbf{m}^{(k)} \\right).\\tag{15}\n",
    "$$\n",
    "\n",
    "Then sampling $\\theta$ from the variational distribution $q_\\phi$ is equivalent to first sampling a noise variable $\\epsilon$, and then transforming the noise variable with the transformation $g_\\phi$:\n",
    "\n",
    "$$\n",
    "\\theta \\sim q_\\phi(\\theta) \\quad \\Longleftrightarrow \\quad \\epsilon\\sim p(\\epsilon),\\quad \\theta = g_\\phi(\\epsilon).\\tag{16}\n",
    "$$\n",
    "\n",
    "With this reparameterisation, we can rewrite our objective function $(12)$ in the following form:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\mathcal{D}) = \\mathbb{E}_{p(\\epsilon)} [ \\log p(\\mathcal{D}\\mid g_\\phi(\\epsilon))] - D_{KL}(q_\\phi(\\theta)\\mid\\mid p(\\theta)),\\tag{17}\n",
    "$$\n",
    "\n",
    "so that the expectation is now being taken with respect to the noise distribution $p(\\epsilon)$, which means that we can take the gradient with respect to $\\phi$ inside the expectation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi\\mathcal{L}(\\phi, \\mathcal{D}) = \\mathbb{E}_{p(\\epsilon)} [\\nabla_\\phi \\log p(\\mathcal{D}\\mid g_\\phi(\\epsilon)) ] - \\nabla_\\phi D_{KL} (q_\\phi(\\theta) \\mid\\mid p(\\theta) ).\n",
    "$$\n",
    "\n",
    "Remember that the probabilities $p_k$ are fixed and are not part of our variational parameters, so it's not a problem that the noise distribution depends on these. \n",
    "\n",
    "#### KL-divergence term\n",
    "\n",
    "With the above approach, we can approximate and optimise the first term of the ELBO objective, but there is a problem with the KL-divergence term. The problem is that the KL divergence is not actually not defined, which is due to the fact that our variational distribution is defined in terms of delta measures. The technical issue is that the variational distribution $q_\\phi(\\theta)$ is not absolutely continuous with respect to the prior distribution $p(\\theta)$. Absolute continuity of measures is defined as follows.\n",
    "\n",
    "**Definition.** _A measure $\\mu$ is absolutely continuous with respect to the measure $\\lambda$ if for for every set $A$ with $\\lambda(A) = 0$, we have $\\mu(A)=0$._\n",
    "\n",
    "In our case, the above condition is not satisfied for the distributions $q_\\phi(\\theta)$ and $p(\\theta)$. For example, we can define the (single) set of parameter values\n",
    "\n",
    "$$\n",
    "A = \\left\\{ \\left\\{ \\mathbf{M}_j^{(k)} \\right\\}, \\left\\{ \\mathbf{m}^{(k)} \\right\\} \\right\\}_{j, k},\\qquad k=0,\\ldots,L, \\quad j=1,\\ldots,n_k\n",
    "$$\n",
    "\n",
    "and we have $\\int_A q_\\phi(\\theta) d\\theta = (p_k)^{Ln_k}$. However, $A$ is a single point in the continuous parameter space $\\theta$, and the prior $p(\\theta)$ is a Gaussian distribution. Therefore we have $\\int_A p(\\theta) d\\theta = 0$, and the condition for absolute continuity is violated.\n",
    "\n",
    "Absolute continuity of measures is a technical condition that we need in order for the KL divergence to be defined. In our case, this condition is broken, so the KL-divergence term in our objective $(17)$ is undefined. Another way to think about this is that the KL divergence term blows up with this choice of prior and variational posterior.\n",
    "\n",
    "#### Gaussian mixture variational posterior\n",
    "\n",
    "The solution the authors of the original paper took was to replace the dropout distribution with something similar; namely the following mixture of Gaussian distributions:\n",
    "\n",
    "$$\n",
    "q_\\phi(\\theta) = \\prod_{k=0}^L N(\\mathbf{b}^{(k)}; \\mathbf{m}^{(k)}, \\sigma^2\\mathbf{I}_{n_{k+1}}) \\prod_{j=1}^{n_k} \\left ( p_k N(\\mathbf{W}_j^{(k)}; \\mathbf{M}_j^{(k)}, \\sigma^2\\mathbf{I}_{n_{k+1}}) + (1-p_k) N(\\mathbf{W}_j^{(k)}; \\mathbf{0}, \\sigma^2\\mathbf{I}_{n_{k+1}})\\right)\\tag{18}\n",
    "$$\n",
    "\n",
    "where the weight matrix columns $\\mathbf{W}_j^{(k)}$ are distributed according to a mixture of two Gaussians with means equal to $\\mathbf{M}^{(k)}_j$ and zero, and with a diagonal covariance matrix with a small variance $\\sigma^2$. These Gaussians are weighted by the probabilities $p_k$. The bias vectors are distributed according to a (single) Gaussian distribution with mean located at $\\mathbf{m}^{(k)}$, and with the same diagonal covariance matrix with variance $\\sigma^2$.\n",
    "\n",
    "This variational posterior is in a sense fattening out the dropout delta distributions that we had before (see $(13)$). If we fix $\\sigma^2$ to some small value, then this is a reasonable approximation of the dropout distribution $(13)$. \n",
    "\n",
    "We can still use the reparameterisation trick in a similar way with this variational posterior. This time, we define the collection of auxiliary noise variables (cf. $(14)$)\n",
    "\n",
    "$$\n",
    "\\epsilon := \\left\\{\\hat{\\epsilon}_j^{(k)}, \\tilde{\\epsilon}_{W,j}^{(k)}, \\tilde{\\epsilon}_{b,j}^{(k)} \\right\\}_{j,k} \\qquad \\hat{\\epsilon}_j^{(k)} \\sim\\text{Bernoulli}(p_k),\\quad  \\tilde{\\epsilon}_{W,j}^{(k)} \\sim N(\\mathbf{0},\\sigma^2\\mathbf{I}_{n_{k+1}}),\\quad \\tilde{\\epsilon}_{b,j}^{(k)} \\sim N(\\mathbf{0},\\sigma^2\\mathbf{I}_{n_{k+1}}),\\tag{19}\n",
    "$$\n",
    "\n",
    "and then we can define our transformation $g_\\phi(\\epsilon)$ as follows (cf. $(15)$):\n",
    "\n",
    "$$\n",
    "g_\\phi(\\epsilon) = \\left\\{ g_{\\phi,j}^{(k)}(\\epsilon) \\right\\}_{j,k},\\qquad g_{\\phi,j}^{(k)}(\\epsilon) := \\left( \\hat{\\epsilon}_j^{(k)} \\mathbf{M}_j^{(k)} + \\tilde{\\epsilon}_{W,j}^{(k)}, \\mathbf{m}^{(k)} + \\tilde{\\epsilon}_{b,j}^{(k)} \\right)\\tag{20}\n",
    "$$\n",
    "\n",
    "With this reparameterisation, sampling $\\theta$ from $q_\\phi$ is equivalent to first sampling $\\epsilon$ from the noise distribution (this time defined as the collection of Bernoulli and Gaussian distributions in $(19)$), and then transforming $\\epsilon$ with $g_\\phi$.\n",
    "\n",
    "$$\n",
    "\\theta \\sim q_\\phi(\\theta) \\quad \\Longleftrightarrow \\quad \\epsilon\\sim p(\\epsilon),\\quad \\theta = g_\\phi(\\epsilon)\n",
    "$$\n",
    "\n",
    "In practice we would approximate the expectation in $(17)$ with Monte Carlo samples, so the objective becomes\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}) := \\left[ \\frac{1}{K} \\sum_{j=1}^K \\log p(\\mathcal{D} \\mid \\theta^{(j)}) \\right] - D_{KL}(q_\\phi(\\theta) \\mid\\mid p(\\theta))\\tag{21}\n",
    "$$\n",
    "\n",
    "In practice, most implementations use $K=1$. The $\\theta^{(j)}$ in the above is given by the transformation $g_\\phi(\\epsilon^{(j)})$, where $\\epsilon^{(j)} \\sim p(\\epsilon)$.\n",
    "\n",
    "Note that this corresponds to how we usually think of using dropout. We sample weights from the approximate dropout distribution, then run the forward and backward pass of the network using these weight values, compute the gradients and update the parameters.\n",
    "\n",
    "With this variational distribution the KL-divergence is now defined: the mixture of Gaussians variational posterior $q_\\phi(\\theta)$ is now absolutely continuous with respect to the prior $p(\\theta)$. But we can't compute the KL-divergence term analytically, as there's no closed form expression for the KL-divergence between a mixture of Gaussians and a single Gaussian, so we have to approximate this term.\n",
    "\n",
    "The following proposition is reproduced from the original paper ([Gal & Ghahramani 2016](#Gal16)). \n",
    "\n",
    "**Proposition.** _Fix $K, L\\in\\mathbb{N}$, a probabilitiy vector $\\mathbf{p} = (p_1,\\ldots, p_L)$, and $\\Sigma_i\\in\\mathbb{R}^{K\\times K}$ positive-definite for $i=1,\\ldots,L$, with the elements of each $\\Sigma_i$ not dependent on $K$. Let\n",
    "$$\n",
    "q(\\mathbf{x}) = \\sum_{i=1}^L p_i N(\\mathbf{x}; \\mathbf{\\mu}_i, \\Sigma_i)\n",
    "$$\n",
    "be a mixture of Gaussians with $L$ components and $\\mathbf{\\mu}_i\\in\\mathbb{R}^K$ normally distributed, and let $p(\\mathbf{x}) = N(0,\\mathbf{I}_K)$._\n",
    "\n",
    "_The KL divergence between $q(\\mathbf{x})$ and $p(\\mathbf{x})$ can be approximated as:\n",
    "$$\n",
    "D_{KL}(q(\\mathbf{x}) \\mid\\mid p(\\mathbf{x})) \\approx \\sum_{i=1}^L \\frac{p_i}{2} (\\mu_i^T \\mu_i + \\text{tr}(\\Sigma_i) - K(1 + \\log 2\\pi) - \\log |\\Sigma_i|)\n",
    "$$\n",
    "plus a constant for large enough $K$._\n",
    "\n",
    "The statement of the proposition says that if we have a distribution $q$ which is a mixture of $L$ Gaussians, each weighted with probabilities $p_i$, and with means $\\mu_i$ and covariance matrix $\\Sigma_i$, and if the distribution $p$ is a zero mean Gaussian with identity covariance matrix, then the KL divergence can be approximated with the above expression, up to some additive constant. This approximation is good for large $K$, where $K$ is the dimension of the Gaussian distributions above. \n",
    "\n",
    "In our case, $K$ corresponds to the widths of the hidden layers. So if our hidden layers are sufficiently wide, then we can replace the KL divergence term in our ELBO objective with expressions of this form. In this case, the KL divergence term in the ELBO objective can be approximated as\n",
    "\n",
    "$$\n",
    "D_{KL}(q_\\phi(\\theta) \\mid\\mid p(\\theta)) \\approx \\sum_{k=0}^L (n_k + 1)n_{k+1} (\\sigma^2 - \\log (\\sigma^2) - 1) + \\frac{p_k}{2}\\sum_{j=1}^{n_k} \\left|\\left| \\mathbf{M}_j^{(k)} \\right|\\right|^2_2 + \\frac{1}{2} \\left|\\left| \\mathbf{m}^{(k)} \\right|\\right|^2_2 + C.\\tag{22}\n",
    "$$\n",
    "\n",
    "The above is a sum over the layers of the network (recall $n_k$ and $n_{k+1}$ are referring to the widths of the layers), $\\sigma^2$ is the variance of the Gaussians in our variational posterior mixture distribution, the $p_k$ are the keep probabilities, and $\\mathbf{M}_j^{(k)}$ and $\\mathbf{m}^{(k)}$ are the variational parameters that are the means of the Gaussian distributions over the weights and biases of the network. $C$ is some constant.\n",
    "\n",
    "Bear in mind that we are considering $\\sigma^2$ to be fixed to some small value, so that our variational posterior is practically the same thing as the dropout distribution. Clearly though, with $\\sigma^2$ being small, the $\\log(\\sigma^2)$ term is going to be very large. And that means that this KL divergence will be large. However, it's also still a constant with respect to our variational parameters, so in terms of optimising the ELBO objective, we can simply leave this term out. \n",
    "\n",
    "#### MC Dropout objective\n",
    "\n",
    "Plugging $(22)$ into $(21)$, we obtain\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}) \\approx \\left[ \\frac{1}{K} \\sum_{j=1}^K \\log p(\\mathcal{D} \\mid \\theta^{(j)}) \\right] - \\sum_{k=0}^L \\left( \\frac{p_k}{2}\\sum_{j=1}^{n_k} \\left|\\left| \\mathbf{M}_j^{(k)} \\right|\\right|^2_2 + \\frac{1}{2} \\left|\\left| \\mathbf{m}^{(k)} \\right|\\right|^2_2 \\right) + \\tilde{C},\n",
    "$$\n",
    "\n",
    "In the above, all terms that don't depend on the variational parameters are absorbed into the constant $\\tilde{C}$. We have also combined the columns $\\mathbf{M}_j^{(k)}$ into a single matrix $\\mathbf{M}^{(k)}$ for notational brevity.\n",
    "\n",
    "As usual, we will use minibatches to approximate the first term, and that means that we need to scale up the first term so it's an unbiased estimator of the expected log-likelihood of the whole dataset:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\phi, \\mathcal{D}_m) \\approx \\left[ \\frac{1}{K} \\frac{N}{M} \\sum_{j=1}^K \\log p(\\mathcal{D}_m \\mid \\theta^{(j)}) \\right] - \\sum_{k=0}^L \\left( \\frac{p_k}{2}\\sum_{j=1}^{n_k} \\left|\\left| \\mathbf{M}_j^{(k)} \\right|\\right|^2_2 + \\frac{1}{2} \\left|\\left| \\mathbf{m}^{(k)} \\right|\\right|^2_2 \\right) + \\tilde{C},\n",
    "$$\n",
    "\n",
    "Here $N$ is the number of examples in the dataset, and $M$ is the size of the minibatch. We are more likely to work with a scaled version of this objective, so that the first term is an average log-likelihood over the minibatch $\\mathcal{D}_m$. Then the final MC-dropout maximisation objective is\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^{MC}(\\phi, \\mathcal{D}_m) :\\approx \\left[ \\frac{1}{K} \\frac{1}{M} \\sum_{j=1}^K \\log p(\\mathcal{D}_m \\mid \\theta^{(j)}) \\right] - \\sum_{k=0}^L \\left( \\frac{p_k}{2N}\\sum_{j=1}^{n_k} \\left|\\left| \\mathbf{M}_j^{(k)} \\right|\\right|^2_2 + \\frac{1}{2N} \\left|\\left| \\mathbf{m}^{(k)} \\right|\\right|^2_2 \\right),\\tag{23}\n",
    "$$\n",
    "\n",
    "where we have divided through by $N$ in order to have a per-example average for the log-likelihood term, and we have removed any constant terms that don't depend on the variational parameters since these are irrelevant for maximising the objective.\n",
    "\n",
    "In $(23)$, $\\theta^{(j)}$ is again defined as the transformation $g_\\phi(\\epsilon^{(j)})$ of a sample from the noise distribution $\\epsilon^{(j)}\\sim p(\\epsilon)$, where the noise distribution is the collection of Bernoulli and Gaussian distributions from $(19)$. \n",
    "\n",
    "Since $\\sigma^2$ is taken to be small and the variational posterior is close to the dropout distribution, in practice you would just sample directly from the dropout distribution $(13)$. Or in other words, you would just implement dropout as usual.\n",
    "\n",
    "Note that the objective function $(23)$ is a typical regularised loss function, where we are using dropout in our network. The first term is just an average log likelihood that we compute after stochastically dropping out the weights as per usual with dropout. And the second term is just $l^2$ regularisation on the weights and biases of the network. \n",
    "\n",
    "So what we have shown is that using dropout in an $l^2$-regularised network has an interpretation as a Bayesian neural network, where we are using the dropout distribution as the variational posterior. It also suggests that the $l^2$ regularisation coefficients should be given by $p_k/2N$ for the weights and $1/2N$ for the biases throughout the network.\n",
    "\n",
    "This insight of MC dropout also suggests that dropout doesn't have to just be for training. When we view it as a Bayesian approximation, we should also be using dropout at test time to give approximations of the predictive distribution. Remember that this is ultimately the goal here - even though we know that the dropout distribution won't be a good approximation of the true posterior in terms of KL-divergence, the hope is that it gives a good approximation of the predictive distribution, when it's used to marginalise out the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"uncertainty_estimation\"></a>\n",
    "## Uncertainty estimation\n",
    "\n",
    "One of the key motivations we gave for Bayesian neural networks was to quantify the uncertainty in the predictions. For this section, assume that we have computed an approximate posterior distribution $q_\\phi(\\theta)$, and we want to consider how we can use it to measure different sources of uncertainty.\n",
    "\n",
    "#### Predictive distribution\n",
    "\n",
    "We can use $q_\\phi(\\theta)$ to approximate the predictive distribution, by marginalising out the parameters of the model. \n",
    "\n",
    "$$\n",
    "p(y^* \\mid x^*, \\mathcal{D}) \\approx \\int_\\theta p(y^* \\mid x^*, \\theta) q_\\phi(\\theta) d\\theta\\tag{24}\n",
    "$$\n",
    "\n",
    "In practice, we'll have to estimate this approximation, by drawing $K$ samples from our variational distribution $q_\\phi$, and computing the Monte Carlo estimate\n",
    "\n",
    "$$\n",
    "p(y^* \\mid x^*, \\mathcal{D}) \\approx \\frac{1}{K} \\sum_{k=1}^K p(y^* \\mid x^*,\\theta_k),\\quad \\theta_k \\sim q_\\phi(\\theta)\n",
    "$$\n",
    "\n",
    "We hope that our model performance will improve by computing this predictive distribution, since instead of using a single point estimate for our parameters, we're averaging over many competing parameter settings, all of which provide good explanations for the data. An alternative way to think about it is that we are using an ensemble to make our predictions.\n",
    "\n",
    "However, that's not all we can do with our approximate posterior distribution. We expect that it should also contain important information about how certain we can be about our predictions. For example, if $q_\\phi$ is highly concentrated in parameter space, then all the members of the ensemble will agree with each other, and we can be more confident about the prediction. On the other hand, if $q_\\phi$ is spread out, then we might get wildly different predictions for a given input, meaning that perhaps we shouldn't be so confident in giving our final prediction.\n",
    "\n",
    "We would like to use the fact that we have the approximate posterior distribution $q_\\phi$ to quantify the uncertainty of our trained model.\n",
    "\n",
    "#### Sources of uncertainty\n",
    "\n",
    "We will first review the sources of uncertainty that we'd like to be able to capture. The first source of uncertainty is aleatoric uncertainty. This is also called data uncertainty.\n",
    "\n",
    "It's usually the case that datasets are noisy; we might have a dataset of examples and corresponding labels, but some examples might be mislabelled, or else contaminated with measurement noise. Or we might just not have collected enough information about the inputs in order to be able to distinguish the classes.\n",
    "\n",
    "<center><img src=\"figures/aleatoric_uncertainty.png\" alt=\"Aleatoric uncertainty\" style=\"width: 400px;\"/></center>\n",
    "<center>Aleatoric uncertainty in a binary classification example</center>\n",
    "<br>\n",
    "\n",
    "The above diagram shows an example of aleatoric uncertainty in a binary classification problem, where we'd like to learn a decision boundary, for example using logistic regression. There is clearly an overlap between the classes, so our model is not going to be able to completely discriminate between these two classes. There will always be some uncertainty in the predictions, simply due to the nature of the data. In this sense, this form of uncertainty is irreducible. The uncertainty comes from the the noise that is inherent in the data, and there's nothing we can do about it. We simply have to capture this uncertainty with our model.\n",
    "\n",
    "Another important source of uncertainty is epistemic uncertainty. This is often referred to as model uncertainty. This is uncertainty about the right choice of model amongst all possible models in our hypothesis space. Or you could think of it as uncertainty about the right set of model parameters. There may be many parameter settings that explain the data well, so we are uncertain about which one we should choose.\n",
    "\n",
    "The typical cause of epistemic uncertainty is insufficient data. If our dataset is small, or has poor coverage over the data domain, then there could be many possible models within our model class that fit the data. We can think of this case as the case where the posterior distribution over model parameters is very spread out. Given the data, there are many varied choices of parameters that explain the data.\n",
    "\n",
    "<center><img src=\"figures/epistemic_uncertainty.png\" alt=\"Epistemic uncertainty\" style=\"width: 400px;\"/></center>\n",
    "<center>Epistemic uncertainty in a binary classification example</center>\n",
    "<br>\n",
    "\n",
    "The diagram above illustrates this situation. Our dataset is small and doesn't fill up the input space as much as we'd like, and this leads to there being many choices of parameter settings that fit the data. These different parameter settings give very different decision boundaries. So we could obtain quite different predictions from these different models if we consider a test input that is far away from the training data.\n",
    "\n",
    "In contrast to aleatoric uncertainty, this type of uncertainty is reducible. If we collect more training data, then the number of possible models that could reasonably explain the data will narrow down. In the limit of infinite data, epistemic uncertainty will vanish, as the true posterior distribution concentrates around a single point in parameter space.\n",
    "\n",
    "In fact, the situation is slightly more complicated surrounding these sources of uncertainty, and we are making a few technical assumptions. For example, we are making an assumption of model identifiability; that is, that different parameter settings always give you different predictive functions. We are also assuming that the model class is sufficiently rich to capture the true data distribution. If that wasn't the case, then what we might identify as aleatoric uncertainty might actually be reducible if we used a higher capacity class of models. In what follows, we will assume that our model class is well defined, with a sufficiently high capacity to model the data distribution.\n",
    "\n",
    "#### Entropy, conditional entropy and mutual information\n",
    "\n",
    "We would like to measure the uncertainty of our predictions using our approximate posterior $q_\\phi$, and the predictive distribution $(24)$.\n",
    "\n",
    "We will use some ideas from information theory, which will provide us with some important and useful quantities to measure uncertainty. Here we review the main concepts that we will need. For a more complete reference, see [MacKay 2003](#MacKay03).\n",
    "\n",
    "The first important quantity that we'll use is the entropy of a random variable. Entropy can be thought of as a measure of the uncertainty in the random variable. If we consider a random variable $Y$ that can assume one of $n$ discrete states, then the entropy of $Y$ is given by\n",
    "\n",
    "$$\n",
    "H(Y) = \\mathbb{E}_p[-\\log p(y)] = -\\sum_{i=1}^n p(y_i) \\log p(y_i).\\tag{25}\n",
    "$$\n",
    "\n",
    "In the above expression we are summing over the possible values of the random variable $Y$, and computing the expected value of $-\\log p(y)$. The quantity $-\\log p(y)$ is known as the _information content_ of the event $y$. \n",
    "\n",
    "This concept of entropy was introduced by [Claude Shannon in 1948](#Shannon48) in the context of communicating messages about events, where the events are sampled from a probability distribution. The idea is that the information contained in an event $y$ is dependent on how likely $y$ is to occur. For example, if $p(y)=1$, then the information content is zero, because $y$ was guaranteed to occur.\n",
    "\n",
    "On the other hand, if an event $y$ has a very low probability, then we're very unlikely to see it, so the message about the event $y$ in this case is very informative. In terms of communicating these events, we could think of encoding each possible event as a unique string of bits, and sending a stream of bit strings to a receiver, who can then decode that stream of bits to see what events have occurred.\n",
    "\n",
    "The information content of an event tells you how long the encoding should be in an optimal coding scheme. If we measure the information content using a base 2 logarithm, $-\\log_2 p(y)$, then the encoding length is measured in bits, or sequences of zeros and ones. However, we will use the natural logarithm, in which case the units are 'natural units', or nats. \n",
    "\n",
    "So the entropy is the average information content, or coding length, of the random variable $Y$. \n",
    "\n",
    "If any event has probability zero, then we define its contribution to the sum $(25)$ to be zero. This makes sense, since \n",
    "\n",
    "$$\n",
    "\\lim_{p(y)\\rightarrow 0} p(y)\\log p(y) = 0.\n",
    "$$\n",
    "\n",
    "A simple example to demonstrate entropy would be a Bernoulli random variable with probability $p$, $Y\\sim\\text{Bernoulli}(p)$. In this case, the entropy of $Y$ is the binary entropy:\n",
    "\n",
    "$$\n",
    "H(Y) = -p\\log p - (1-p) \\log (1-p)\n",
    "$$\n",
    "\n",
    "<center><img src=\"figures/binary_entropy.png\" alt=\"Binary entropy\" style=\"width: 400px;\"/></center>\n",
    "<center>The binary entropy function</center>\n",
    "<br>\n",
    " \n",
    "The figure above shows the binary entropy as a function of the probability $p$. We see that if the probability of the Bernoulli random variable is either zero or one, then the outcome is always the same, and there is no information gained in observing the outcomes. In both of these cases, the entropy is zero. So the information content of every outcome is zero, and we have no uncertainty in the outcome of the random variable.\n",
    "\n",
    "Also, the binary entropy is maximised at $p=1/2$, and this is the setting where we are most uncertain about the outcome. The value of the entropy $H(Y)$ at $p=1/2$ is equal to $\\log 2$ nats. If we were measuring the entropy in bits, then this would be precisely 1 bit. This makes sense, since we'd need exactly 1 bit to encode the information about the outcome of a fair coin flip. If the coin is biased, then it would be more efficient to use a shorter encoding for the more likely event.\n",
    "\n",
    "In general, for a discrete random variable that can take one of $n$ states, the distribution that will maximise entropy, or maximise the uncertainty, is the uniform distribution. If all of the probability mass goes on one of the events, then the entropy, or uncertainty, will be zero.\n",
    "\n",
    "The above is assuming a discrete distribution. For continuous random variables we define the differential entropy\n",
    "\n",
    "$$\n",
    "H(Y) = -\\int p(y) \\log p(y) dy.\\tag{26}\n",
    "$$\n",
    "\n",
    "There are two more related quantities from information theory that will be useful. The first is the conditional entropy of a random variable $Y$, given the random variable $X$, which for discrete random variables $Y$ and $X$, is defined as\n",
    "\n",
    "$$\n",
    "H(Y \\mid X) = - \\sum_{i,j} p(y_i, x_j) \\log p(y_i \\mid x_j).\\tag{27}\n",
    "$$\n",
    "\n",
    "The definition of conditional entropy is the expected information content of $Y$ given that $X$ is known, where the expectation is taken over the joint distribution between $Y$ and $X$. Note that we can also write the conditional entropy in the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y \\mid X) &= - \\sum_{j} p(x_j) \\sum_i p(y_i \\mid x_j) \\log p(y_i \\mid x_j) \\\\\n",
    "&= \\mathbb{E}_{x\\sim p(x)} \\left[ H(Y \\mid x) \\right] \\tag{28}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the term $H(Y\\mid x)$ is not the conditional entropy of $Y$ given $X$. It is the entropy of $Y$, given a _fixed value for $X$_. \n",
    "\n",
    "In the case where $X$ and $Y$ are independent, then the conditional entropy reduces to the entropy of the random variable $Y$. Knowing the value of $X$ wouldn't tell you anything about $Y$, so the average information needed to describe $Y$ would be the same. However, if $X$ and $Y$ are not independent, then knowing $X$ gives you some information about $Y$. \n",
    "\n",
    "In the other extreme case, if the value of $X$ completely determined the value of $Y$, then the conditional entropy would be zero. There would be no uncertainty about the outcome of $Y$ if $X$ was known.\n",
    "\n",
    "In general, if $X$ and $Y$ are dependent, then knowing the value of $X$ will reduce the uncertainty in predicting the value of $Y$, and so intuitively we can see that $H(Y\\mid X) \\le H(Y)$. By learning the value of $X$, we gain information about the value of $Y$, and the average code length needed to encode $Y$ will decrease.\n",
    "\n",
    "The amount of information gained is what is measured by the third quantity of interest, which is mutual information ([Kreer 1957](#Kreer57)). The mutual information $I(X;Y)$ between random variables $Y$ and $X$ can be expressed as the difference between the entropy of $Y$ and the conditional entropy of $Y$ given $X$.\n",
    "\n",
    "$$\n",
    "I(X; Y) = H(Y) - H(Y\\mid X)\\tag{29}\n",
    "$$\n",
    "\n",
    "The mutual information is symmetric in $X$ and $Y$, so we also have $I(X;Y) = I(Y;X)$. Note that from $(28)$ we can write the mutual information as\n",
    "\n",
    "$$\n",
    "I(X; Y) = H(Y) - \\mathbb{E}_{x\\sim p(x)} \\left[ H(Y \\mid x) \\right]\\tag{30}\n",
    "$$\n",
    "\n",
    "#### Quantifying uncertainty\n",
    "\n",
    "Recall the predictive distribution $(24)$ which is computed by marginalising out the model parameters $\\theta$, using the variational posterior distribution $q_\\phi(\\theta)$. Of course this is actually an approximation to the true predictive distribution, since $q_\\phi$ is an approximation of the true posterior.\n",
    "\n",
    "We could measure uncertainty by computing the entropy of the predictive distribution $H(p(Y \\mid x,\\mathcal{D}))$. Note that we are shifting the notation for the entropy, just to make clear which distribution over the target variable $Y$ we're referring to. So $H(p(Y \\mid x,\\mathcal{D}))$ is the entropy of the predictive distribution, where the input to the model $x$ is fixed, as well as the dataset $\\mathcal{D}$. Again this is not a conditional entropy, as in this expression we are only considering $Y$ to be a random variable.\n",
    "\n",
    "Practically, we can compute this by drawing Monte Carlo samples from our variational posterior:\n",
    "\n",
    "$$\n",
    "H(p(Y \\mid x,\\mathcal{D})) \\approx H\\left(\\frac{1}{K} \\sum_{k=1}^K p(y \\mid x, \\theta^{(k)}) \\right),\\qquad \\theta^{(k)}\\sim q_\\phi(\\theta)\\tag{31}\n",
    "$$\n",
    "\n",
    "This entropy calculation measures the uncertainty of the predictive distribution, but it doesn't distinguish between epistemic and aleatoric uncertainty. \n",
    "\n",
    "However, we could capture a measure of epistemic uncertainty by using the mutual information between the model parameters $\\theta$ and the target variable $Y$. From $(30)$, this is given by\n",
    "\n",
    "$$\n",
    "I(\\theta: Y \\mid \\mathcal{D}, x) = H(p(Y \\mid x, \\mathcal{D})) - \\mathbb{E}_{q_\\phi(\\theta)} H(p(Y \\mid x, \\theta))\\tag{32}\n",
    "$$\n",
    "\n",
    "Recall that the mutual information is measuring the expected amount of information we would gain about the random variable $\\theta$ if we observed the random variable $Y$. Or in other words, it quantifies how much we would learn about the parameters $\\theta$ by observing the label for this test input $x$. If we wouldn't learn anything, then that means that the model parameters must be well determined, at least for the input $x$, and we can conclude that the model uncertainty is very low.\n",
    "\n",
    "On the other hand, if knowing the label for this new input $x$ would tell us a lot about the parameters, then we can conclude that we must be very uncertain about the parameter value, at least for the input $x$. \n",
    "\n",
    "The information gained from knowing $Y$ would reduce the entropy of the random variable $\\theta$, and we'd be far less uncertain about it. This scenario would indicate a high model uncertainty.\n",
    "\n",
    "Note again that the term $H(p(Y \\mid x, \\theta))$ inside here is the entropy of $Y$, given the fixed input $x$ and a fixed set of parameter values $\\theta$, and the conditional entropy is the expectation of this quantity with respect to the distribution $q_\\phi(\\theta)$. In practice, we can compute this term by drawing Monte Carlo samples from $q_\\phi$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q_\\phi(\\theta)} H(p(Y \\mid x, \\theta)) \\approx \\frac{1}{K} \\sum_{k=1}^K H\\left(p(Y \\mid x, \\theta^{(k)}) \\right),\\qquad \\theta^{(k)} \\sim q_\\phi(\\theta)\\tag{33}\n",
    "$$\n",
    "\n",
    "This means for each sample $\\theta^{(k)}$, making a pass through the network to get the output distribution $p(Y \\mid x, \\theta^{(k)})$, computing the entropy of that distribution, and then averaging.\n",
    "\n",
    "Note that this first term in $(32)$ is the entropy of the predictive distribution $(31)$. We refer to this as the predictive entropy.\n",
    "\n",
    "The second term $\\mathbb{E}_{q_\\phi(\\theta)} H(p(Y \\mid x, \\theta))$ is referred to as the expected entropy, and this is a measure of the aleatoric uncertainty in our prediction. This makes intuitive sense, because what we're computing here is the average uncertainty in the predictions coming from our ensemble of models. If all the models in our ensemble give a high entropy to the output distribution, then we can conclude that the uncertainty is due to noise in the data, because all of the models that give a good explanation of the data are uncertain about the prediction for $x$. Even if we collected more data and narrowed down the possibilities for the correct parameter, it still wouldn't make any difference to this measure of uncertainty. So this is our irreducible source of uncertainty, which is data uncertainty.\n",
    "\n",
    "So finally, we can write down our decomposition of overall uncertainty as follows:\n",
    "\n",
    "$$\n",
    "\\underbrace{H(p(Y \\mid x, \\mathcal{D}))}_{\\text{Predictive entropy}} = \\underbrace{I(\\theta; Y \\mid \\mathcal{D}, x)}_{\\text{Epistemic uncertainty}} + \\underbrace{\\mathbb{E}_{q_\\phi(\\theta)} H(p(Y \\mid x, \\theta))}_{\\text{Aleatoric uncertainty}}.\n",
    "$$\n",
    "\n",
    "The predictive entropy is the sum of uncertainty measures coming from two sources. The first, mutual information, is measuring the epistemic uncertainty, and the second, the expected entropy, is measuring the aleatoric uncertainty.\n",
    "\n",
    "In practice, we estimate the predictive entropy and the expected entropy using the Monte Carlo estimates $(31)$ and $(33)$, and then subtract them to get an estimate of the mutual information $(32)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Blundell15\"></a>\n",
    "* Blundell, C., Cornebise, J., Kavukcuoglu, K. & Wierstra, D. (2015), \"Weight uncertainty in neural networks\", in *Proceedings of the 32nd International Conference on Machine Learning (ICML)*, Lille, France, 6-11 July 2015.\n",
    "<a class=\"anchor\" id=\"Izmailov18\"></a>\n",
    "* Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D. & Wilson, A. (2018), \"Averaging Weights Leads to Wider Optima and Better Generalizatio\", in *AUAI Press*, 876-885.\n",
    "<a class=\"anchor\" id=\"Gal16\"></a>\n",
    "* Gal, Y. & Ghahramani, Z. (2016), \"Dropout as a Bayesian approximation: representing model uncertainty in deep learning\", in *Proceedings of the 33rd International Conference on Machine Learning (ICML)*, New York, New York, 20-22 June 2016.\n",
    "<a class=\"anchor\" id=\"Lakshminarayanan15\"></a>\n",
    "* Lakshminarayanan, B., Pritzel, A. & Blundell, C. (2017), \"Simple and scalable predictive uncertainty estimation using deep ensembles\", in *Advances in Neural Information Processing Systems*, 6402-6413.\n",
    "<a class=\"anchor\" id=\"Kingma14\"></a>\n",
    "* Kingma, D. P. & Welling, M. (2014), \"Auto-Encoding Variational Bayes\", in *Proceedings of the 2nd International Conference on Learning Representations (ICLR)*, Banff, AB, Canada, April 14-16, 2014.\n",
    "<a class=\"anchor\" id=\"Kreer57\"></a>\n",
    "* Kreer, J.G. (1957), \"A question of terminology\", _IRE Transactions on Information Theory_, **3** (3), 208.\n",
    "<a class=\"anchor\" id=\"MacKay92\"></a>\n",
    "* MacKay, D.J.C. (1992), \"Bayesian methods for adaptive models\", PhD thesis, California Institute of Technology, 1992.\n",
    "<a class=\"anchor\" id=\"MacKay03\"></a>\n",
    "* MacKay, D.J.C. (2003), \"Information Theory, Inference, and Learning Algorithms\", Cambridge University Press, 2003.\n",
    "<a class=\"anchor\" id=\"Maddox19\"></a>\n",
    "* Maddox, W.J., Izmailov, P., Garipov, T., Vetrov, D.P. & Wilson, A.G. (2019), \"A Simple Baseline for Bayesian Uncertainty in Deep Learning\", in *Advances in Neural Information Processing Systems*, **32** Vancouver, Canada, 2019.\n",
    "<a class=\"anchor\" id=\"Neal95\"></a>\n",
    "* Neal, R.M. (1995), \"Bayesian learning for neural networks\", PhD thesis, University of Toronto, 1995.\n",
    "<a class=\"anchor\" id=\"Rezende15\"></a>\n",
    "* Rezende, D. & Mohamed, S. (2015), \"Variational Inference with Normalizing Flows\", in *Proceedings of Machine Learning Research*, **37**, 1530-1538.\n",
    "<a class=\"anchor\" id=\"Shannon48\"></a>\n",
    "* Shannon, C.E. (1948), \"A mathematical theory of communication\", _The Bell System Technical Journal_ **27**, 379-423.\n",
    "<a class=\"anchor\" id=\"Wilson20\"></a>\n",
    "* Wilson, A.G. & Izmailov, P. (2020), \"Bayesian Deep Learning and a Probabilistic Perspective of Generalization\", in *Advances in Neural Information Processing Systems*, **33**, 4697-4708."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

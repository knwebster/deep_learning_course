{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 5: Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Convolutional neural networks](#convnets)\n",
    "\n",
    "[3. CNNs and feature maps (\\*)](#cnnsfeaturemaps)\n",
    "\n",
    "[4. Padding and strides](#paddingstrides)\n",
    "\n",
    "[5. Transposed convolutions](#transposed_convolutions)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "The last week of the module focused on optimisation and regularisation of deep learning models. You learned about several techniques that are essential for successfully training large deep learning models in practice, including optimiser algorithms, weight regularisation, dropout and early stopping. \n",
    "\n",
    "You saw how callbacks can be used in Keras to perform actions depending on how the training progresses, such as early stopping or model saving. You also learned how to implement custom callbacks in a model.\n",
    "\n",
    "This week of the module focuses on another important model architecture. The multilayer perceptron is a general deep learning model architecture, which makes use of several fully connected (dense) layers. However it is often not the optimal choice, and in this week we will see how to build convolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"convnets\"></a>\n",
    "## Convolutional neural networks\n",
    "\n",
    "A convolutional neural network (CNN) is a type of neural network with a special structure. It can be seen as a special case of the multilayer perceptron architecture that builds certain assumptions into the design of the model, in particular using **local connectivity** and **equivariance**.\n",
    "\n",
    "An important motivating application for CNNs is **computer vision**, as the architectural design of these networks mimics the visual system, where neurons respond to stimulus in a restricted region of the visual field ([Hubel 1959](#Hubel59)). This concept led initially to the development of the neocognitron ([Fukushima 1980](#Fukushima80)), and later to the modern convolutional neural network trained by backpropagation ([LeCun et al 1989](#LeCun89)). \n",
    "\n",
    "In this exposition, we will focus on CNNs for image processing, using 2-D convolutions. However, convolutional neural networks have also been very successful when applied to time series data, using 1-D convolutions. They can also be applied to 3-D image processing tasks, or video analysis, with 3-D convolutions.\n",
    "\n",
    "#### The convolution operation\n",
    "The convolution operation for two (Lebesgue integrable) functions $h$ and $k$ is defined as\n",
    "\n",
    "$$\n",
    "(h * k)(t) = \\int_{-\\infty}^\\infty h(\\tau)k(t - \\tau)d\\tau.\n",
    "$$\n",
    "\n",
    "It can be described as the weighted average of the function $h$ according to the weighting function (or **kernel**) $k$ at each point in time $t$. As $t$ changes, the weighting function emphasises different parts of the input function $h$.\n",
    "\n",
    "In practice (and in the context of CNNs), we need to discretise the data, and work instead with discrete convolutions:\n",
    "\n",
    "$$\n",
    "(h * k)(t) = \\sum_{\\tau=-\\infty}^\\infty h(\\tau)k(t - \\tau).\n",
    "$$\n",
    "\n",
    "In this case, we can assume that both $h$ and $k$ (and the convolution $(h * k)$) take integer arguments. In addition, when the kernel function $k$ has finite support, we can write the above as a finite summation.\n",
    "\n",
    "In convolutional neural networks with image inputs, a 2-D discrete convolution is used:\n",
    "\n",
    "$$\n",
    "(\\mathbf{h} * \\mathbf{k})(i, j) = \\sum_{m, n} h(m, n) k(i - m, j - n).\n",
    "$$\n",
    "\n",
    "In the above, we will consider $h(i, j)$ and $k(i, j)$ to denote the $(i, j)$-th elements of the matrices $\\mathbf{h}\\in\\mathbb{R}^{n_h\\times n_w}$ and $\\mathbf{k}\\in\\mathbb{R}^{k_h\\times k_w}$ respectively, where $n_h$ and $n_w$ are the image height and width in pixels, and $k_h$ and $k_w$ are the kernel height and width in pixels. We will use bold lower case for the matrices $\\mathbf{h}$ and $\\mathbf{k}$ for consistency of notation with previous sections.\n",
    "\n",
    "In practice, many libraries implement the **cross-correlation** operation, which is the same as above but changing the orientation of the arguments:\n",
    "\n",
    "$$\n",
    "(\\mathbf{h} * \\mathbf{k})(i, j) = \\sum_{m, n} h(i + m, j + n) k(m, n). \\tag{1}\n",
    "$$\n",
    "\n",
    "We will refer to the above operation in CNNs as a **convolution**.\n",
    "\n",
    "As an example, consider a grayscale image $\\mathbf{x} =: \\mathbf{h}^{(0)}\\in\\mathbb{R}^{7\\times 7}$ with height and width $n_h=n_w=7$, as illustrated in the following figure.\n",
    "\n",
    "<center><img src=\"figures/1_channel_no_padding_crop.png\" alt=\"Input grayscale image\" style=\"width: 550px;\"/></center>\n",
    "<center>Pixels in a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7}$</center>\n",
    "<br>\n",
    "\n",
    "Suppose we define a kernel matrix $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$. The operation $(1)$ can be visualised in the following animation, as sweeping the convolutional kernel $\\mathbf{k}$ across the input image. At each step, the weights of the kernel matrix $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$ are multiplied pointwise by the values of the pixels outlined in red and summed together to produce the pre-activation of the neuron in the next hidden layer.\n",
    "\n",
    "<center><img src=\"figures/1_channel_no_padding.gif\" alt=\"2D convolution on a single channel input\" style=\"width: 600px;\"/></center>\n",
    "<center>2-D convolution with kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7}$</center>\n",
    "<br>\n",
    "\n",
    "In convolutional neural networks, the convolutional layers usually consist of the operation $(1)$ plus a bias term, followed by a pointwise activation function:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k)} = \\sigma\\left((\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)}) + b^{(k-1)}\\right). \\tag{2}\n",
    "$$\n",
    "\n",
    "In the above, the superscript $(k)$ indexes the layer as before, and the bias $b^{(k-1)}\\in\\mathbb{R}$ is added pointwise to the output of the convolution operation $(\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)})\\in\\mathbb{R}^{(n_h-k_h+1)\\times (n_w-k_w+1)}$.\n",
    "\n",
    "Different values of the kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4}$ will pick out different features in the input $\\mathbf{h}^{(k-1)}$. The output of $(2)$ is sometimes referred to as a **feature map**, and the kernel $\\mathbf{k}$ is also referred to as a **filter**.\n",
    "\n",
    "Note that the operation described above introduces a translational **equivariance** property in convolutional layers. That is, if the input image is translated, then the activations in the next hidden layer are also translated accordingly. Another way to view this is that the convolutional kernel searches for the same features across the input image.\n",
    "\n",
    "#### Multi-channel inputs and outputs\n",
    "The operations $(1)$ and $(2)$ can easily be extended to inputs with multiple channels. Consider, for example, an input RGB image, which has three channel values per pixel.\n",
    "\n",
    "<center><img src=\"figures/3_channel_no_padding_crop.png\" alt=\"Input RGB image\" style=\"width: 550px;\"/></center>\n",
    "<center>Pixels in an RGB input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3}$</center>\n",
    "<br>\n",
    "\n",
    "The input is now a rank-3 tensor $\\mathbf{x} = \\mathbf{h}^{(0)}\\in\\mathbb{R}^{7\\times 7\\times 3}$, and correspondingly we require a rank-3 kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{k_h\\times k_w\\times 3}$. For illustration we will again choose $k_h=3$, $k_w=4$. The operation $(1)$ now becomes\n",
    "\n",
    "$$\n",
    "(\\mathbf{h} * \\mathbf{k})(i, j) = \\sum_{m, n, p} h(i + m, j + n, p) k(m, n, p).\n",
    "$$\n",
    "\n",
    "This is visualised in the following animation, where the kernel $\\mathbf{k}$ again sweeps over the input image, this time multiplying a $3\\times 4\\times 3$ block of input pixels (outlined in red) elementwise by the values of the kernel tensor $\\mathbf{k}$, and summing the results to produce the output neuron pre-activation.\n",
    "\n",
    "<center><img src=\"figures/3_channel_no_padding.gif\" alt=\"2D convolution on a 3-channel (RGB) input\" style=\"width: 550px;\"/></center>\n",
    "<center>2-D convolution with kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 3}$ operating on an RGB input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3}$</center>\n",
    "<br>\n",
    "\n",
    "In convolutional layers, many filters are stacked on top of each other, so as to produce a multichannel output. In practice, we implement this with a rank-4 kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{k_h, k_w, c_{in}, c_{out}}$, where $c_{in}$ are the number of channels in the input, and $c_{out}$ are the number of channels in the output:\n",
    "\n",
    "$$\n",
    "(\\mathbf{h} * \\mathbf{k})(i, j, q) = \\sum_{m, n, p} h(i + m, j + n, p) k(m, n, p, q).\n",
    "$$\n",
    "\n",
    "This is visualised in the following animation, for an input $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3}$ and kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 3\\times 2}$.\n",
    "\n",
    "<center><img src=\"figures/3_2_channel_no_padding.gif\" alt=\"2D convolution on an 3-channel (RGB) input with 2 filters\" style=\"width: 550px;\"/></center>\n",
    "<center>2-D convolution with kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 3}$ operating on an RGB input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 3\\times 2}$ with 2 filters</center>\n",
    "<br>\n",
    "\n",
    "The convolutional layer operation $(2)$ now becomes the following\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k)} = \\sigma\\left((\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)}) + \\mathbf{b}^{(k-1)}\\right),\\tag{3}\n",
    "$$\n",
    "\n",
    "where again the superscript $(k)$ indexes the layer, and the bias $\\mathbf{b}^{(k-1)}\\in\\mathbb{R}^{c_{out}}$ is added pixel-wise to the output of the convolution operation $(\\mathbf{h}^{(k-1)} * \\mathbf{k}^{(k-1)})\\in\\mathbb{R}^{(n_h-k_h+1)\\times (n_w-k_w+1)\\times c_{out}}$.\n",
    "\n",
    "#### Pooling layers\n",
    "In many CNN models, convolutional layers are alternated with pooling layers. Pooling layers downsample the spatial dimensions of a layer by computing a summary statistic of (often non-overlapping) regions of the input layer's post-activations.\n",
    "\n",
    "A typical pooling layer type is the **max pooling** layer ([Zhou & Chellappa 1988](#Zhou88)) which takes the maximum activation in a region as the single neuron activation for that region. For example, we could divide the input layer into $2\\times2$ squares and take the maximum value for each square. This results in halving the spatial dimensions of the following layer.\n",
    "\n",
    "<center><img src=\"figures/max_pooling.png\" alt=\"Max pooling\" style=\"width: 550px;\"/></center>\n",
    "<center>Max pooling using $2\\times2$ pooling windows</center>\n",
    "<br>\n",
    "\n",
    "Pooling operations are usually performed separately for each input channel, so that the spatial dimensions are downsampled, but the channel dimensions stay the same. Other common pooling operations include average pooling, or computing the $\\mathcal{l}^2$ norm of the pixel values within each input region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cnnsfeaturemaps\"></a>\n",
    "## CNNs and feature maps\n",
    "\n",
    "In this section we will use the `Conv2D` and `MaxPool2D` layer to implement the convolution and pooling operations described above, and see how these easily fits into our existing model-building workflow.\n",
    "\n",
    "We will also see the effect of different kernel tensor choices on the output feature maps, and look at more complex feature maps from a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Conv2D` and `MaxPool2D` classes are imported from the `keras.layers` module just as the `Input`, `Flatten` and `Dense` layers we have already worked with. Note that there are also 1-D and 3-D variants of these layers available, which both work in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy model with Conv2D and MaxPool2D layers\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPool2D\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(32, 32, 3)),\n",
    "    Conv2D(8, (3, 5), activation='relu'),\n",
    "    MaxPool2D((2, 2)),\n",
    "    Conv2D(16, 3, activation='relu'),\n",
    "    MaxPool2D(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the layer variables' shapes\n",
    "\n",
    "print(model.layers[0].kernel.shape)\n",
    "print(model.layers[0].bias.shape)\n",
    "\n",
    "print(model.layers[2].kernel.shape)\n",
    "print(model.layers[2].bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge detection filters\n",
    "The kernels (or filters) in CNNs are typically learned with backpropagation. However, simple low-level features such as edge detection kernels can also be designed by hand. In this section we will see the output of such low-level kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model with a Conv2D layer\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None, None, 1)),\n",
    "    Conv2D(1, (3, 3), activation=None, use_bias=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shape dimension of `None` indicates that the model can take flexible input sizes in this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's weights\n",
    "\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image as grayscale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "image = keras.utils.load_img(Path(\"./figures/oscar.png\"), color_mode='grayscale')  # Loads into PIL format\n",
    "image = keras.utils.img_to_array(image, dtype='int32')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple and intuitive edge detection kernel is the [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple edge detection filters\n",
    "\n",
    "sobel_x = ops.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype='float32')\n",
    "sobel_y = ops.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype='float32')\n",
    "\n",
    "print(sobel_x)\n",
    "print(sobel_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model kernel\n",
    "\n",
    "def assign_filter(arr):\n",
    "    model.weights[0].assign(arr[:, :, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the feature maps\n",
    "\n",
    "assign_filter(sobel_x)\n",
    "gx = model(image[None, ...])[0]\n",
    "\n",
    "assign_filter(sobel_y)\n",
    "gy = model(image[None, ...])[0]\n",
    "\n",
    "g = ops.sqrt(ops.square(gx) + ops.square(gy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the image and feature map\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "fig.add_subplot(121)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "fig.add_subplot(122)\n",
    "plt.imshow(ops.convert_to_numpy(g), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract learned features from a pre-trained model\n",
    "In this section we will load a CNN model that has been pre-trained on the [ImageNet](http://www.image-net.org) dataset, which is a large scale image classification dataset which to date has over 20,000 categories and over 14 million images. Large deep learning models trained on this dataset tend to learn general, useful representations of image features that can be used for a range of image processing tasks.\n",
    "\n",
    "Below we will load the VGG-19 model ([Simonyan & Zisserman 2015](#Simonyan15)), which is available to load as a pre-trained model in the [`keras.applications`](https://keras.io/api/applications/) module. This might take a minute or two to download the first time you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG-19 model\n",
    "\n",
    "vgg = keras.applications.VGG19(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualise the features extracted by this model at different levels of hierarchy for the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a colour image\n",
    "\n",
    "image = keras.utils.load_img(Path(\"./figures/hoover_dam.JPEG\"), color_mode='rgb')\n",
    "image = keras.utils.img_to_array(image, dtype='int32')\n",
    "plt.figure(figsize=(6, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [functional API](https://keras.io/guides/functional_api/) to create a multi-output model that outputs different hidden layer outputs within the model. The Keras functional API is a way to create models that are more flexible than the Sequential API. The functional API can handle models that are defined as more complicated DAGS than just a linear sequence of layers. It can also handle shared layers and multiple inputs or outputs.\n",
    "\n",
    "Defining models using the functional API typically involves defining one or more input Tensors `inputs` (usually using the `Input` layer). The graph computations are carried by defining additional Tensors as the result of operations on previously defined Tensors (these operations are often themselves defined by layer objects). After all computations have been carried out, the graph outputs are defined as one or more output Tensors `outputs`. These `inputs` and `outputs` Tensors are then used to define a model as `model = Model(inputs=inputs, outputs=outputs)`.\n",
    "\n",
    "It is also worth knowing that any Keras `Model` (including models created using the `Sequential` API) have `inputs` and `outputs` attributes. These attributes store the input and output Tensors of the model. Below, we use the `inputs` attribute of the pre-trained VGG-19 model and the output Tensors of selected layers to define a new `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-output model\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = vgg.inputs\n",
    "layer_names = ['block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4', 'block5_conv4']\n",
    "outputs = [vgg.get_layer(layer_name).output for layer_name in layer_names]\n",
    "vgg_features = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model inputs and outputs Tensors\n",
    "\n",
    "print(vgg_features.inputs)\n",
    "print(vgg_features.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hierarchical features for this image\n",
    "\n",
    "image_processed = keras.applications.vgg19.preprocess_input(image)\n",
    "features = vgg_features(image_processed[None, ...])\n",
    "features = [image] + [ops.convert_to_numpy(f) for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_rows, n_cols = 2, 3\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 14))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.2)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    feature_map = features[i]\n",
    "    num_channels = feature_map.shape[-1]\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    if i == 0:\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].set_title('Original image')\n",
    "    else:\n",
    "        random_feature = np.random.choice(num_channels)\n",
    "        axes[row, col].imshow(feature_map[0, ..., random_feature])\n",
    "        axes[row, col].set_title('{}, channel {} of {}'.format(layer_names[i-1], random_feature + 1, num_channels))\n",
    "        \n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise:* load one of your own images to view the features extracted by the VGG-19 network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"paddingstrides\"></a>\n",
    "## Padding and strides\n",
    "\n",
    "Padding and strides are additional properties of convolutional (and pooling) layers that can give some flexibility over the spatial dimensions of the output. \n",
    "\n",
    "In this section, we will define these properties and the effects they have on the input and output dimensions of a convolutional layer. For a more complete guide to convolutional arithmetic, see [Dumoulin & Visin 2016](#Dumoulin16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "Recall that in our earlier example, an input grayscale image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ convolved with a kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times 1\\times 1}$ produced an output of size $\\mathbf{h}\\in\\mathbb{R}^{5\\times 4\\times 1}$:\n",
    "\n",
    "<center><img src=\"figures/1_channel_no_padding.gif\" alt=\"2D convolution on a single channel input\" style=\"width: 600px;\"/></center>\n",
    "<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ produces an output $\\mathbf{h}\\in\\mathbb{R}^{5\\times 4\\times 1}$</center>\n",
    "<br>\n",
    "\n",
    "In general, for a spatial dimension of size $i$ and a kernel of width $k$, the output size $o$ is given by\n",
    "\n",
    "$$\n",
    "o = i - k + 1 \\tag{4}\n",
    "$$\n",
    "\n",
    "In many model architectures, it is desirable to keep the spatial dimensions the same in the output of a convolutional layer. This can be achieved by padding the input layer with zeros.\n",
    "\n",
    "In the case of our kernel tensor $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times1\\times1}$, if we add 2 zeros in the first dimension and 3 zeros in the second dimension (distributed on either side of the image), this will result in an output $\\mathbf{h}\\in\\mathbb{R}^{7\\times 7\\times 1}$ that has the same spatial dimensions as the input $\\mathbf{x}$. This type of padding is known as \"SAME\" padding.\n",
    "\n",
    "<center><img src=\"figures/1_channel_with_padding.gif\" alt=\"2D convolution on a single channel input with 'SAME' padding\" style=\"width: 600px;\"/></center>\n",
    "<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 4\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ with \"SAME\" padding</center>\n",
    "<br>\n",
    "\n",
    "Now, if $p$ zeros are added to our input size $i$ with kernel width $k$, then the output size $o$ is given by\n",
    "\n",
    "$$\n",
    "o = i + p - k + 1 \\tag{5}\n",
    "$$\n",
    "\n",
    "If $p = k-1$ (\"SAME\" padding) then the $o = i$. If $p = 0$ (\"VALID\" padding) then we recover $(4)$ and $o = i-k+1$. There is also \"FULL\" padding where $p = 2(k-1)$, so that $o = i + k - 1$, although this is less common.\n",
    "\n",
    "In Keras, we can easily apply zero padding to convolutional layers with the `padding` keyword argument, which can be set to `\"VALID\"` (the default) or `\"SAME\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN with 'VALID' and 'SAME' padding\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPool2D\n",
    "\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "h = Conv2D(16, (3, 5), padding=\"VALID\", activation='relu')(inputs)\n",
    "h = MaxPool2D(2)(h)\n",
    "outputs = Conv2D(16, 3, padding=\"SAME\", activation='relu')(h)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the input and output shapes of each of the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strides\n",
    "Convolutions may also use a stride $s$, which is the distance between consecutive positions of the kernel. So far, all of our examples have used $s=1$, however it is easy to see that using $s>1$ leads to a downsampling of the input. The following animation shows our input $\\mathbf{x}\\in\\mathbb{R}^{7\\times7\\times 1}$, this time with a kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$, \"SAME\" padding, and a stride $s=2$ in both spatial dimensions:\n",
    "\n",
    "<center><img src=\"figures/1_channel_stride_2.gif\" alt=\"2D convolution on a single channel input with stride 2\" style=\"width: 600px;\"/></center>\n",
    "<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 7\\times 1}$ with \"SAME\" padding and stride of $(2, 2)$</center>\n",
    "<br>\n",
    "\n",
    "In the above example, the stride and kernel size exactly divides the input and padding size, however this does not need to be the case, such as in the following example where $\\mathbf{x}\\in\\mathbb{R}^{7\\times6\\times 1}$:\n",
    "\n",
    "<center><img src=\"figures/1_channel_7x6_stride_2.gif\" alt=\"2D convolution on a single channel input with stride 2\" style=\"width: 600px;\"/></center>\n",
    "<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 6\\times 1}$ with \"SAME\" padding and stride of $(2, 2)$</center>\n",
    "<br>\n",
    "\n",
    "In this example the input $\\mathbf{x}\\in\\mathbb{R}^{7\\times6\\times 1}$ has been downsampled to an output $\\mathbf{h}\\in\\mathbb{R}^{4\\times 3\\times 1}$. Note that if the input image size was instead $7\\times 5\\times 1$ then the output size would again be $4\\times 3\\times 1$.\n",
    "\n",
    "<center><img src=\"figures/1_channel_7x5_stride_2.gif\" alt=\"2D convolution on a single channel input with stride 2\" style=\"width: 600px;\"/></center>\n",
    "<center>A kernel $\\mathbf{k}\\in\\mathbb{R}^{3\\times 3\\times1\\times1}$ operating on a grayscale input image $\\mathbf{x}\\in\\mathbb{R}^{7\\times 5\\times 1}$ with \"SAME\" padding and stride of $(2, 2)$</center>\n",
    "<br>\n",
    "\n",
    "In all cases, for a spatial dimension of size $i$ with padding $p$ and a kernel of width $k$ with stride $s$, the output size $o$ is given by\n",
    "\n",
    "$$\n",
    "o = \\left\\lfloor \\frac{i + p - k}{s} \\right\\rfloor + 1 \\tag{6}\n",
    "$$\n",
    "\n",
    "In Keras, strides can be set in both convolutional and pooling layers using the `strides` keyword argument. \n",
    "\n",
    "Just as with the `kernel_size` argument for `Conv2D` layers (and the `pool_size` argument for `MaxPool2D` layers), either a tuple of integers can be passed in, or a single integer - which indicates the stride should be the same in all spatial dimensions. \n",
    "\n",
    "In `MaxPool2D` layers, the default stride is set to be equal to the `pool_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN using strides in the Conv2D and MaxPool2D layers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
    "\n",
    "inputs = Input(shape=(128, 128, 3))\n",
    "h = Conv2D(16, (5, 5), padding=\"VALID\", strides=(2, 3), activation='relu')(inputs)\n",
    "h = MaxPool2D(2)(h)\n",
    "h = Conv2D(16, 7, padding=\"SAME\", activation='relu')(h)\n",
    "h = MaxPool2D((2, 4), strides=(3, 2))(h)\n",
    "h = Conv2D(16, 3, padding=\"SAME\", strides=2, activation='relu')(h)\n",
    "h = Flatten()(h)\n",
    "h = Dense(16, activation='relu')(h)\n",
    "outputs = Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the input and output shapes of each of each layers, and compare with the formula $(6)$ for the output size given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transposed_convolutions\"></a>\n",
    "## Transposed convolutions\n",
    "\n",
    "Transposed convolutions are a type of convolutional layer that are not as common as the convolutional and pooling layers introduced earlier, but are important for certain applications. They can be seen as a kind of inverse of regular convolutional layers. The main problem they are trying to solve is to give a consistent way of upsampling an input, rather than downsampling it. \n",
    "\n",
    "We've seen that in convolutional layers and in pooling layers, depending on the layer settings, they can reduce the spatial dimensions of the input. In certain deep learning architectures, we would instead like to increase the spatial dimensions of an input. An example of this is when we are using encoder and decoder networks, which we will come to later on in the course. \n",
    "\n",
    "Transposed convolutions give us a way of doing this, while still preserving the main structural properties of convolutional layers. They are the analogue to transposing the weight matrix in fully connected layers; they essentially swap the forward and backward passes of a convolution. Here we will briefly cover the core properties of the transposed convolution. For a more detailed treatment, see [Dumoulin & Visin 2016](#Dumoulin16).\n",
    "\n",
    "Every regular convolutional layer has an associated transposed convolution that reverses the dimensions of input and output, whilst preserving the connectivity pattern between the layers. This transposed convolution can always be interpreted as a regular convolution, with particular kernel size, padding and stride parameters.\n",
    "\n",
    "In the case of a convolution with stride $s=1$, kernel size $k$, padding $p$ and input size $i$, recall the output size is $o = i + p - k +1$. There is an associated transposed convolution with kernel size $k' = k$, stride $s'=s=1$, and padding $p'=2(k-1) - p$ . Its output size is given by\n",
    "\n",
    "$$\n",
    "o' = i' + (k-1) - p,\n",
    "$$\n",
    "\n",
    "so with $i' = o$, we have $o' = i$, and the transposed convolution reverses the input and output dimensions.\n",
    "\n",
    "The following animations show an example of a $3\\times 3$ convolution applied to an input $\\mathbf{x}\\in\\mathbb{R}^{5\\times 5}$, with a stride of $1$ and no padding. The associated transposed convolution has an input $\\mathbf{y}\\in\\mathbb{R}^{3\\times 3}$, stride of one and padding of two zeros on either side of the input (\"FULL\" padding).\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figures/3x3conv.gif\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "<td> <img src=\"figures/3x3convt.gif\" alt=\"Drawing\" style=\"width: 448px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>A regular convolution (left) and its associated transposed convolution (right). The input and output dimensions are reversed, but the connectivity pattern between neurons remains the same</center>\n",
    "<br>\n",
    "\n",
    "In Keras, the transposed convolution is available as the `Conv2DTranspose` layer (also 1D and 3D variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transposed convolution layer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv2DTranspose\n",
    "\n",
    "cnn = Sequential([\n",
    "    Input(shape=(3, 3, 1)),\n",
    "    Conv2DTranspose(1, (3, 3), padding=\"VALID\")\n",
    "])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a regular convolution with stride $s>1$, we can think of the associated transposed convolution as having a stride $s'<1$ (another name for transposed convolutions is **fractionally strided convolutions**). In this case, the situation is more complicated. This is because for $s>1$, different sized inputs can lead to the same size output. \n",
    "\n",
    "Consider first the case where the regular convolution is such that $s$ divides $(i + p -k)$. Recall that the output size is then $o = \\left(\\frac{i + p -k}{s}\\right) + 1$. Then, the input to the associated transposed convolution adds $s-1$ zeros between its input units. It has kernel size $k'=k$, $s'=1$, and padding $p'=2(k-1) - p$. The output size is given by\n",
    "\n",
    "$$\n",
    "o' = s(i' - 1) + k - p.\n",
    "$$\n",
    "\n",
    "The following shows an example. An input $\\mathbf{x}\\in\\mathbb{R}^{3\\times 3}$ is processed by a convolution with a $3\\times 3$ kernel, stride $s=2$ and padding $p=2$ (\"SAME\" padding). Its output size is $2\\times 2$. The associated transposed convolution takes an input $\\mathbf{y}\\in\\mathbb{R}^{2\\times 2}$, inserts one zero in between the input units and padding $p'=2$.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figures/3x3convs2.gif\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "<td> <img src=\"figures/3x3convs2t.gif\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>A regular convolution with stride $s=2$ (left) and its associated transposed convolution (right). Note that $s \\mid (i + p - k)$. The transposed convolution inserts one zero in between each input unit</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transposed convolution layer with fractional stride\n",
    "\n",
    "cnn = Sequential([\n",
    "    Input(shape=(2, 2, 1)),\n",
    "    Conv2DTranspose(1, (3, 3), strides=2, padding=\"SAME\", output_padding=0)\n",
    "])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case where $s$ does not divide $(i + p - k)$ is accounted for by the parameter $a = (i + p - k)\\mod s$. The transposed convolution again adds $s-1$ zeros between input units, but also adds additional padding of $a$ zeros, which then increases the dimension of its output. It has kernel size $k'=k$, $s'=1$, padding $p'=2(k-1) - p + a$, and output size\n",
    "\n",
    "$$\n",
    "o' = s(i' - 1) + a + k - p\n",
    "$$\n",
    "\n",
    "The following shows hows the same output size as the previous example can be produced by the same convolution operation, for a different input size. The input is now $\\mathbf{x}\\in\\mathbb{R}^{4\\times 4}$, with a $3\\times 3$ kernel, stride $s=2$ and padding $p=2$ as before. The associated transposed convolution again takes an input $\\mathbf{y}\\in\\mathbb{R}^{2\\times 2}$, but this time produces an output with spatial dimensions $4\\times 4$.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figures/3x3convs2b.gif\" alt=\"Drawing\" style=\"width: 367px;\"/> </td>\n",
    "<td> <img src=\"figures/3x3convs2bt.gif\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>A regular convolution with stride $s=2$ where $s\\nmid (i + p - k)$ (left) and its associated transposed convolution (right). An additional $a = (i + p + k)\\text{ mod } s = 1$ zero is added to the input of the transposed convolution</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transposed convolution layer with fractional stride and additional output padding\n",
    "\n",
    "cnn = Sequential([\n",
    "    Input(shape=(2, 2, 1)),\n",
    "    Conv2DTranspose(1, (3, 3), strides=2, padding=\"SAME\", output_padding=1)\n",
    "])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Dumoulin16\"></a>\n",
    "* Dumoulin, V. & Visin, F. (2016), \"A guide to convolution arithmetic for deep learning\", arXiv preprint, abs/1603.07285.\n",
    "<a class=\"anchor\" id=\"Fukushima80\"></a>\n",
    "* Fukushima, K. (1980), \"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\", *Biological Cybernetics*, **3**6 (4), 193–202.\n",
    "<a class=\"anchor\" id=\"Hubel59\"></a>\n",
    "* Hubel, D. H. & Wiesel, T. N. (1959), \"Receptive fields of single neurones in the cat's striate cortex\", *Journal of Physiology* **148** (3), 574–91.\n",
    "<a class=\"anchor\" id=\"LeCun89\"></a>\n",
    "* LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989) \"Backpropagation Applied to Handwritten Zip Code Recognition\", AT&T Bell Laboratories.\n",
    "<a class=\"anchor\" id=\"Rumelhart86b\"></a>\n",
    "* Rumelhart, D. E., Hinton, G., and Williams, R. (1986b), \"Learning representations by back-propagating errors\", Nature, **323**, 533-536.\n",
    "<a class=\"anchor\" id=\"Simonyan15\"></a>\n",
    "* Simonyan, K. & Zisserman, A. (2015), \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", in *3rd International Conference on Learning Representations, (ICLR) 2015*, San Diego, CA, USA.\n",
    "<a class=\"anchor\" id=\"Vinyals19\"></a>\n",
    "* Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T. L., Gulcehre, C., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., & Silver, D.(2019) \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\", *Nature*, **575** (7782), 350-354.\n",
    "<a class=\"anchor\" id=\"Zhou88\"></a>\n",
    "* Zhou, Y. & Chellappa, R. (1988), \"Computation of optical flow using a neural network\", in *IEEE International Conference on Neural Networks*, IEEE, 71-78."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 9: Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Autoencoders](#autoencoders)\n",
    "\n",
    "[3. CNN autoencoder example (\\*)](#cnn_autoencoder_example)\n",
    "\n",
    "[4. The variational autoencoder](#vae)\n",
    "\n",
    "[5. VAE implementation (\\*)](#vae_implementation)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In the last week of the module we saw how normalising flow probabilistic deep learning models can be used to model data distributions. In particular, you learned about the NICE, RealNVP and Glow models, and saw how these flows can be built and trained using custom layers in Keras. \n",
    "\n",
    "In this week of the module, we will look at another important deep learning algorithm: the variational autoencoder, or VAE. The VAE is an algorithm for inference and learning in a latent variable generative model. It has been successfully applied in a variety of application domains, such as neuroimaging ([Benou et al 2016](#Benou16)), drug discovery ([Jin et al 2018](#Jin18)), anomaly detection ([Xu et al 2018](#Xu18)), image generation ([Vahdat & Kautz 2020](#Vahdat20)) and music generation ([Dhariwal et al 2020](#Dhariwal20)).\n",
    "\n",
    "In its simplest form, the VAE is an unsupervised learning algorithm, and like normalising flows, the generative model can be used to create new examples similar to the dataset. However, unlike normalising flows, the generative model is not invertible, and so it's not as straightforward to train the model using maximum likelihood.\n",
    "\n",
    "The VAE uses the principle of variational inference to approximate the posterior distribution, by defining a parameterised family of distributions conditioned on a data example, and then maximising a lower bound on the marginal likelihood. This is the evidence lower bound, or ELBO.\n",
    "\n",
    "In this week, we'll build up the pieces we need to implement a variational autoencoder with Keras, starting with looking at building regular autoencoder architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"autoencoders\"></a>\n",
    "## Autoencoders\n",
    "\n",
    "In this section, we'll look at how to implement a standard autoencoder architecture. \n",
    "\n",
    "An autoencoder can be viewed as a compression algorithm, similar to a VAE, although it's not a probabilistic model, and it is not a model of the underlying data distribution.\n",
    "\n",
    "The aim of an autoencoder is to learn an efficient data encoding. The network is normally trained in an unsupervised manner, and the task of the network is to reproduce its input as its output. \n",
    "\n",
    "<center><img src=\"figures/autoencoder.png\" alt=\"Autoencoder network\" style=\"width: 750px;\"/></center>\n",
    "<center>An autoencoder network architecture, with bottleneck latent variable $\\mathbf{z}$</center>\n",
    "<br>\n",
    "\n",
    "The autoencoder has a bottleneck architecture as in the above figure, and can be broken into two parts: the **encoder** network and the **decoder** network. In the middle of the bottleneck is the latent variable $\\mathbf{z}$, which captures the encoding of the data. The dimensionality of $\\mathbf{z}$ is typically much lower than the data $\\mathbf{x}$, and so the network is trained to perform nonlinear dimension reduction ([Kramer 1991](#Kramer91)). The job of the encoder is to learn an efficient representation of the data in a much lower dimensional encoding space, whilst the decoder is required to decompress the latent code to reconstruct the data input $\\mathbf{x}$.\n",
    "\n",
    "For an autoencoder network $f_{\\mathbf{\\theta}}$, the model is trained to minimise the loss\n",
    "\n",
    "$$\n",
    "L(\\theta; \\mathcal{D}_{train}) = \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{x_i\\in\\mathcal{D}_{train}}l(x_i, f_\\theta(x_i)),\n",
    "$$\n",
    "\n",
    "where $l: \\mathbb{R}^D\\times \\mathbb{R}^D\\mapsto\\mathbb{R}$ is a suitable loss function, such as mean squared error. In practice, the model is trained using minibatches of data as usual.\n",
    "\n",
    "There are several variants of the autoencoder model, one notable example being the **denoising autoencoder** ([Vincent & Larochelle 2010](#Vincent10)). In this model, the input $\\mathbf{x}$ is corrupted with noise to produce the input $\\tilde{\\mathbf{x}}$, and the model is trained to minimise the loss\n",
    "\n",
    "$$\n",
    "L(\\theta; \\mathcal{D}_{train}) = \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{x_i\\in\\mathcal{D}_{train}}l(x_i, f_\\theta(\\tilde{x}_i)).\n",
    "$$\n",
    "\n",
    "In other words, the model is tasked to clean the corrupted input by encoding it into a suitable representation. Intuitively, this is motivated by the idea that good representations should be robust to the corruption of the input $\\mathbf{x}$, and that to denoise the input successfully, the model needs to extract features that capture useful structure in the distribution of the input, and ignore features in the data that are unimportant.\n",
    "\n",
    "The noise is typically injected stochastically during the training run, according to a prescribed distribution $q(\\tilde{\\mathbf{x}} \\mid \\mathbf{x})$, so that the noise is different on each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cnn_autoencoder_example\"></a>\n",
    "## CNN autoencoder example\n",
    "\n",
    "In this section, we will implement a CNN autoencoder for the Fashion-MNIST dataset, and examine the learned encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fashion-MNIST dataset can be loaded with the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train = (x_train / 255.).astype(np.float32)\n",
    "x_test = (x_test / 255.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the class names\n",
    "\n",
    "class_names = np.array(['T-shirt/top', 'Trouser/pants', 'Pullover shirt', 'Dress',\n",
    "                        'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag','Ankle boot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "inx = np.random.choice(x_train.shape[0], n_rows*n_cols, replace=False)\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "for n, (image, label) in enumerate(zip(x_train[inx], y_train[inx])):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='binary')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "    axes[row, col].text(10., -2.5, f'{class_names[label]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the CNN autoencoder model\n",
    "We define the encoder and decoder networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN encoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
    "\n",
    "encoded_dim = 2\n",
    "\n",
    "cnn_encoder = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Conv2D(16, 5, activation='relu'),\n",
    "    MaxPool2D(2),\n",
    "    Conv2D(8, 5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(encoded_dim)\n",
    "])\n",
    "cnn_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute encodings before training\n",
    "\n",
    "inx = np.random.choice(x_test.shape[0], 1000, replace=False)\n",
    "untrained_encodings = ops.convert_to_numpy(cnn_encoder(x_test[inx]))\n",
    "untrained_encoding_labels = y_test[inx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot untrained encodings\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "cmap = plt.get_cmap('jet', 10)\n",
    "for i, class_label in enumerate(class_names):\n",
    "    inx = np.where(untrained_encoding_labels == i)[0]\n",
    "    plt.scatter(untrained_encodings[inx, 0], untrained_encodings[inx, 1],\n",
    "                color=cmap(i), label=class_label, alpha=0.7)\n",
    "plt.xlabel('$z_1$', fontsize=16) \n",
    "plt.ylabel('$z_2$', fontsize=16)\n",
    "plt.title('Encodings of example images before training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN decoder\n",
    "\n",
    "from keras.layers import Reshape, UpSampling2D, Conv2DTranspose\n",
    "\n",
    "cnn_decoder = Sequential([\n",
    "    Input(shape=(encoded_dim,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Reshape((8, 8, 8)),\n",
    "    Conv2DTranspose(16, 5, activation='relu'),\n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2DTranspose(1, 5, activation='sigmoid')\n",
    "])\n",
    "cnn_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "cnn_autoencoder = Model(inputs=cnn_encoder.inputs, outputs=cnn_decoder(cnn_encoder.outputs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train and test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for train and test sets\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, x_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the datasets\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "\n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "cnn_autoencoder.compile(loss='binary_crossentropy')\n",
    "cnn_autoencoder.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute encodings after training\n",
    "\n",
    "inx = np.random.choice(x_test.shape[0], 1000, replace=False)\n",
    "trained_encodings = ops.convert_to_numpy(cnn_encoder(x_test[inx]))\n",
    "trained_encoding_labels = y_test[inx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot untrained and trained encodings\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "cmap = plt.get_cmap('jet', 10)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, class_label in enumerate(class_names):\n",
    "    inx = np.where(untrained_encoding_labels == i)[0]\n",
    "    plt.scatter(untrained_encodings[inx, 0], untrained_encodings[inx, 1],\n",
    "                color=cmap(i), label=class_label, alpha=0.7)\n",
    "plt.xlabel('$z_1$', fontsize=16) \n",
    "plt.ylabel('$z_2$', fontsize=16)\n",
    "plt.title('Encodings of example images before training')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, class_label in enumerate(class_names):\n",
    "    inx = np.where(trained_encoding_labels == i)[0]\n",
    "    plt.scatter(trained_encodings[inx, 0], trained_encodings[inx, 1],\n",
    "                color=cmap(i), label=class_label, alpha=0.7)\n",
    "plt.xlabel('$z_1$', fontsize=16) \n",
    "plt.ylabel('$z_2$', fontsize=16)\n",
    "plt.title('Encodings of example images after training')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the autoencoder's reconstructions\n",
    "\n",
    "inx = np.random.choice(x_test.shape[0], 5, replace=False)\n",
    "reconstructed_example_images = ops.convert_to_numpy(cnn_autoencoder(x_test[inx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the autoencoder's reconstructions\n",
    "\n",
    "f, axs = plt.subplots(2, 5, figsize=(15, 4))\n",
    "for j in range(5):\n",
    "    axs[0, j].imshow(x_test[inx][j], cmap='binary')\n",
    "    axs[1, j].imshow(reconstructed_example_images[j].squeeze(), cmap='binary')\n",
    "    axs[0, j].axis('off')\n",
    "    axs[1, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Redesign the CNN autoencoder above using strides $\\ge2$ for the encoder, and design the decoder to be the reverse architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"vae\"></a>\n",
    "## The variational autoencoder\n",
    "\n",
    "We will now review the variational autoencoder (VAE) algorithm, its derivation from applying the principle of variational inference to a prescribed generative model, and its connection to standard autoencoders. The VAE was developed independently by [Kingma & Welling 2014](#Kingma14) and [Rezende et al 2014](#Rezende14) at about the same time. For a general reference on variational inference, see [Blei et al 2017](#Blei17).\n",
    "\n",
    "First, we describe the generative model behind the variational autoencoder. This is a **latent variable generative model**, where we introduce a latent (unobserved) random variable that is intended to capture hidden causes or explanations of the data. \n",
    "\n",
    "Furthermore, it is a **prescribed model** in the sense that we prescribe a noise model for the observations. Given a latent variable ${z}\\in\\mathbb{R}^l$, this determines a distribution over possible observations $p_\\theta({x} \\mid {z})$, with $x\\in\\mathbb{R}^D$. This class of generative model is also called a **likelihood-based model**, since the observations have an associated likelihood function. \n",
    "\n",
    "This is in contrast to an **implicit model**, where there is no likelihood function on the observations, and instead a realisation of the latent variable ${z}$ implicitly defines the observation ${x}$ (note that this is the case with normalising flows, although there we have the additional special structure that the generative model is invertible, and so the observation likelihood can still be explicitly computed). This is illustrated in the following figure.\n",
    "\n",
    "<center><img src=\"figures/generative_models.png\" alt=\"Generative models\" style=\"width: 400px;\"/></center>\n",
    "<center>Latent variable directed graphical models; (a) a prescribed generative model that defines a likelihood for each observation, and (b) an implicit generative model. The VAE is based on the prescribed model</center>\n",
    "<br>\n",
    "\n",
    "The generative model under consideration can be written as $p_\\theta({z})p_\\theta({x} \\mid {z})$, where the conditional distribution $p_\\theta({x} \\mid {z})$ is defined by a neural network. The **marginal likelihood** (or **model evidence**) of an individual datapoint $x\\in\\mathbb{R}^D$ is given by\n",
    "\n",
    "$$\n",
    "p_\\theta({x}) = \\int p_\\theta({z})p_\\theta({x} \\mid {z}) d{z},\\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters. \n",
    "\n",
    "Note that under the usual i.i.d. assumption of our dataset $\\mathbf{x} = (x_i)_{i=1}^N$,  the full data log-likelihood is given by\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{x}) = \\sum_{i=1}^N \\log p_\\theta(x_i).\n",
    "$$\n",
    "\n",
    "In the following we will continue to consider the likelihood of a single datapoint $x_i$, and drop the subscript $i$ for notational convenience.\n",
    "\n",
    "Now, we would like to choose the parameters $\\theta$ that maximise the marginal likelihood. Unfortunately, the integral above is intractable in general (as is the true posterior $p({z} \\mid {x})$), so we need to approximate it.\n",
    "\n",
    "The approximation that we will use is the **evidence lower bound** (ELBO), or **variational free energy**, which is a lower bound on the true marginal log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta({x}) &\\ge \\mathbb{E}_{q_\\phi({z}\\mid{x})} \\left[ \\log p_\\theta({x}\\mid{z})\\right] - D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right)\\tag{2}\\\\\n",
    "&=: \\mathcal{L}(\\theta, \\phi; {x}), \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $q_\\phi({z} \\mid {x})$ is a parameterised distribution of our choosing, and $D_{KL}$ denotes the Kullback-Leibler divergence, given by\n",
    "\n",
    "$$\n",
    "D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right) = \\int q_\\phi({z}\\mid{x})\\log\\left(\\frac{q_\\phi({z}\\mid{x})}{p_\\theta({z})}\\right)d{z}.\n",
    "$$\n",
    "\n",
    "The two terms in $(2)$ are often interpreted as a reconstruction loss term and a regularisation term:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) = \\underbrace{\\mathbb{E}_{q_\\phi({z}\\mid{x})} \\left[ \\log p_\\theta({x}\\mid{z})\\right]}_{\\text{reconstruction loss}} - \\underbrace{D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right)}_{\\text{regulariser}}.\n",
    "$$\n",
    "\n",
    "This decomposition shows the connection to autoencoders: if $q_\\phi({z}\\mid{x})$ is a parameterised neural network, then we can view this as the encoder and $p_\\theta({x}\\mid{z})$ as the decoder. Then the reconstruction loss is the probabilistic version of the autoencoder reconstruction loss (where we could consider $q_\\phi({z}\\mid{x})$ as a delta distribution). The second term regularises the encoder, and ensures it doesn't stray too far from the prior distribution $p_\\theta({z})$.\n",
    "\n",
    "The ELBO is also sometimes written as $\\mathcal{L}(\\theta, \\phi; {x}) = \\mathbb{E}_{q_\\phi({z}\\mid {x})}\\left[ -\\log q_\\phi({z}\\mid{x}) + \\log p_\\theta({x}, {z}) \\right]$.\n",
    "\n",
    "#### Derivation of the ELBO\n",
    "We will derive the evidence lower bound in two different ways. The first is a simple derivation using Jensen's inequality, and the second will help to shed some light on the optimal choice for the distribution $q_\\phi({z} \\mid {x})$.\n",
    "\n",
    "_Derivation 1_. The marginal log-likelihood is given by (cf. $(1)$)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta({x}) &= \\log\\int p_\\theta({x}\\mid{z})p_\\theta({z})d{z}\\\\\n",
    "&= \\log \\int p_\\theta({x}\\mid{z})\\frac{p_\\theta({z})}{q_\\phi({z}\\mid{x})}q_\\phi({z}\\mid{x})d{z}\\\\\n",
    "&\\ge \\int\\log\\left(p_\\theta({x}\\mid {z})\\frac{p_\\theta({z})}{q_\\phi({z}\\mid {x})}\\right)q_\\phi({z}\\mid {x})d{z}\\\\\n",
    "&= \\int q_\\phi({z}\\mid {x}) \\log p_\\theta({x}\\mid {z})d{z} - \\int q_\\phi({z}\\mid {x}) \\log \\left(\\frac{q_\\phi({z}\\mid {x})}{p_\\theta({z})}\\right)d{z}\\\\\n",
    "&= \\mathcal{L}(\\theta, \\phi; {x}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the third line in the above uses Jensen's inequality.\n",
    "\n",
    "_Derivation 2._ Let $q_\\phi({z}\\mid {x})$ be a parameterised family of distributions that we use to approximate the true posterior $p_\\theta({z}\\mid {x})$. We define the objective function that we wish to minimise as the KL-divergence $D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta({z}\\mid {x}))$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta( {z}\\mid {x})) &= \\int q_\\phi({z}\\mid {x}) \\log\\left(\\frac{q_\\phi({z}\\mid {x})}{p_\\theta({z}\\mid {x})}\\right) d{z}\\\\\n",
    "&= \\int q_\\phi({z}\\mid {x}) \\log\\left( \\frac{q_\\phi({z}\\mid {x}) p_\\theta({x})}{p_\\theta({x}\\mid{z})p_\\theta({z})}\\right) d{z}\\\\\n",
    "&= \\int q_\\phi({z}\\mid {x}) \\log p_\\theta({x}) d{z} - \\int q_\\phi({z}\\mid{x}) \\log p_\\theta({x}\\mid{z})d{z}\\\\\n",
    "& \\quad + \\int q_\\phi({z}\\mid {x})\\log\\left( \\frac{q_\\phi({z}\\mid {x})}{p_\\theta({z})}  \\right)d{z}\\\\\n",
    "&= \\log p_\\theta({x}) - \\mathcal{L}(\\theta, \\phi; {x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the KL-divergence is always non-negative, the above shows that $\\mathcal{L}(\\theta, \\phi; {x})$ is indeed a lower bound on the marginal log-likelihood $\\log p_\\theta({x})$. Furthermore, it shows that the gap in the bound is given by $D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta({z}\\mid {x}))$. \n",
    "\n",
    "We see that to maximise the ELBO, the distribution $q_\\phi({z}\\mid {x})$ should approximate the true posterior $p_\\theta({z}\\mid {x}))$. And the better the approximation, the tighter the bound.\n",
    "\n",
    "The following figure illustrates that the variational autoencoder adds the variational approximation $q_\\phi({z}\\mid {x})$ to the intractable true posterior $p_\\theta({z}\\mid {x}))$. \n",
    "\n",
    "<center><img src=\"figures/generative_variational.png\" alt=\"The generative model and variational approximation to the true posterior\" style=\"width: 250px;\"/></center>\n",
    "<center>The prescribed generative model underlying the variational autoencoder, and the variational approximation $q_\\phi({z}\\mid {x})$ with variational parameters $\\phi$ depicted with dashed lines</center>\n",
    "<br>\n",
    "\n",
    "Note that the generative model parameters $\\theta$ and the variational parameters $\\phi$ are **global variables**, whereas the latent random variable ${z}$ is a **local variable**. The variational parameters $\\phi$ are shared across all data points, and are not specific to individual data points, in contrast to traditional mean-field variational inference. This strategy is known as **amortized inference** ([Hoffman et al 2013](#Hoffman13)).\n",
    "\n",
    "#### The reparameterization trick\n",
    "We have now defined our ELBO objective function that we wish to maximise, which is a lower bound on the marginal log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) = \n",
    " \\mathbb{E}_{q_\\phi({z}\\mid {x})} \\left[ \\log p_\\theta({x}\\mid {z})\\right] - D_{KL}\\left( q_\\phi({z}\\mid {x}) || p_\\theta({z}) \\right)\n",
    "$$\n",
    "\n",
    "Note that we are able to evaluate the densities $q_\\phi({z}\\mid{x})$, $p_\\theta({x}\\mid{z})$, $p_\\theta({z})$ as well as sample from the approximating distribution $q_\\phi({z}\\mid {x})$, so the ELBO can be evaluated using Monte Carlo samples $\\{z^{(j)}\\}_{j=1}^L$, with $z^{(j)}$ sampled from $q_\\phi({z}\\mid{x})$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) \\approx \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x\\mid z^{(j)}) + \\log p_\\theta(z^{(j)}) − \\log q_\\phi(z^{(j)}\\mid x)\n",
    "$$\n",
    "\n",
    "The question remains how to optimise the ELBO with respect to the parameters $\\theta$ and $\\phi$. Note that taking gradients with respect to $\\phi$ is not straightforward, as the $z^{(j)}$ are samples. \n",
    "\n",
    "A typical **score-function estimator** ([Glynn 1990](#Glynn90), [Kleijnen & Rubinstein 1996](#Kleijnen96)) for the general type of problem of taking a gradient of an expectation of some function $f(z)$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}\\left[ f(z) \\right] &= \\mathbb{E}_{q_\\phi(z)} \\left[ f(z) \\nabla_\\phi \\log q_\\phi(z) \\right]\\\\\n",
    "&\\approx \\frac{1}{L} \\sum_{j=1}^L \\left[ f(z^{(j)}) \\nabla_\\phi \\log q_\\phi(z^{(j)}) \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This estimator is also used in reinforcement learning for policy gradients, where it is often referred to as the REINFORCE algorithm ([Williams 1992](#Williams92)). However, this estimator typically has high variance ([Blei et al 2012](#Blei12)), and in our case we can do better, in particular since our function $f(z)$ $\\left(=\\log p_\\theta(x, z^{(j)})  − \\log q_\\phi(z^{(j)}\\mid x)\\right)$ is differentiable.\n",
    "\n",
    "The 'reparameterization trick' simply reparameterises the random latent variable $z\\sim q_\\phi(z\\mid x)$ using a differentiable transformation $g_\\phi(\\epsilon, x)$ of an auxiliary noise variable $\\epsilon\\sim p(\\epsilon)$:\n",
    "\n",
    "$$\n",
    "z \\sim q_\\phi(z\\mid x)\\\\\n",
    "z = g_\\phi(\\epsilon, x),\\quad \\epsilon\\sim p(\\epsilon)\n",
    "$$\n",
    "\n",
    "An example is if $q_\\phi(z\\mid x)$ a multivariate Gaussian distribution $N(\\mu_\\phi(x), \\Sigma_\\phi(x))$. Then we could reparameterise the distribution as \n",
    "\n",
    "$$\n",
    "p(\\epsilon)=N(\\mathbf{0},\\mathbf{I}),\\quad g_\\phi(\\epsilon,x)=\\mu_\\phi(x)+L_\\phi(x)\\epsilon,\\quad \\text{where }L_\\phi(x)L_\\phi(x)^T =\\Sigma_\\phi(x).\n",
    "$$\n",
    "\n",
    "The encoder network would then output the distribution parameters $\\mu_\\phi(x)$ and $L_\\phi(x)$, which are both fully differentiable with respect to $\\phi$. Note that the noise variable distribution does not depend on any parameters. \n",
    "\n",
    "If the transformation $g_\\phi(\\cdot, x):\\mathbb{R}^d\\mapsto\\mathbb{R}^l$ is invertible, this is nothing more than a change of variables, so the change of variables formula applies to give\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\phi(z\\mid x) &= \\left|\\, \\det J_{g_\\phi}(\\epsilon)\\,\\right|^{-1}\\cdot p(\\epsilon)\\\\\n",
    "&= \\left|\\, \\frac{d\\epsilon}{dz}\\,\\right|\\cdot p(\\epsilon)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This leads to the **pathwise estimator** ([Devroye 1996](#Devroye96)), which for a general function $f(z)$ and reparameterization $g_\\phi(\\epsilon)$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}\\left[ f(z) \\right] &= \\nabla_\\phi \\int {q_\\phi(z)} f(z)dz\\\\\n",
    "&= \\nabla_\\phi \\int \\left|\\, \\frac{d\\epsilon}{dz}\\,\\right|\\cdot p(\\epsilon) f(g_\\phi(\\epsilon)) \\left|\\, \\frac{dz}{d\\epsilon}\\,\\right| d\\epsilon\\\\\n",
    "&= \\nabla_\\phi \\mathbb{E}_{p(\\epsilon)} \\left[ f(g_\\phi(\\epsilon)) \\right]\\\\\n",
    "&=  \\mathbb{E}_{p(\\epsilon)} \\left[ \\nabla_\\phi f(g_\\phi(\\epsilon)) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In our case, the reparameterization $g_\\phi(\\epsilon, x)$ and noise variable $\\epsilon\\sim p(\\epsilon)$ leads to the **Stochastic Gradient Variational Bayes** (SGVB) estimator $\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) \\approx \\mathcal{L}(\\theta, \\phi; {x})$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x,z^{(j)})− \\log q_\\phi(z^{(j)}|x)\\tag{3}\\\\\n",
    "\\text{ where }z^{(j)} = g_\\phi(\\epsilon^{(j)}, x)\\quad\\text{and}\\quad\\epsilon^{(j)}\\sim p(\\epsilon)\n",
    "$$\n",
    "\n",
    "We can now use this estimator to approximate the ELBO objective, and take its gradients with respect to the parameters $(\\theta, \\phi)$ on minibatches of data to optimise them:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{x\\sim p_{data}}\\left[ \\log p_\\theta(x)\\right] &\\approx \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{i\\in\\mathcal{D}_{train}} \\log p_\\theta(x_i)\\\\\n",
    "&\\ge \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{i\\in\\mathcal{D}_{train}} \\mathcal{L}(\\theta, \\phi; {x_i})\\\\\n",
    "&\\approx \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{i\\in\\mathcal{D}_{train}} \\hat{\\mathcal{L}}^A(\\theta,\\phi;x)\\\\\n",
    "&\\approx \\frac{1}{M} \\sum_{i\\in\\mathcal{D}_{m}}  \\hat{\\mathcal{L}}^A(\\theta,\\phi;x),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where as usual $\\mathcal{D}_m$ is a randomly sampled minibatch of training data points, and $M=|\\mathcal{D}_m|$. \n",
    "\n",
    "Note that we wish to maximise the above objective, so in practice we will take the negative of the quantity above to minimise.\n",
    "\n",
    "Finally, depending on the choice of prior $p_\\theta(z)$ and variational posterior $q_\\phi(z\\mid x)$, it may be possible to analytically evaluate the KL-divergence term $D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z))$. This is true, for example, in the case where both distributions are Gaussian. In this case, there is no need to approximate this term in the ELBO with Monte Carlo samples, and we can instead use the alternate version of the SGVB estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x\\mid z^{(j)}) − D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z)),\\tag{4}\n",
    "$$\n",
    "\n",
    "where as before $z^{(j)} = g_\\phi(\\epsilon^{(j)}, x)$ and $\\epsilon^{(j)}\\sim p(\\epsilon)$.\n",
    "\n",
    "It is worth noting that it is common in practice to take a single Monte Carlo sample ($L=1$) in the SGVB estimator $(3)$ or $(4)$, particularly for larger minibatch sizes.\n",
    "\n",
    "We summarise the Auto-Encoding Variational Bayes (variational autoencoder) algorithm as follows. The algorithm inputs are the encoder and decoder networks $q_\\phi(z \\mid x)$, $p_\\theta(x \\mid z)$, prior distribution $p_\\theta(z)$, minibatch size $M$ and number of Monte Carlo samples $L$.\n",
    "\n",
    "> Initialise $\\phi$, $\\theta$ randomly<br>\n",
    "> **while** not converged:<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; sample minibatch $\\mathcal{D}_m$ of $M$ data examples<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; sample $M\\times L$ noise variables $\\epsilon^{(i, j)}$ for each $x_i\\in\\mathcal{D}_m$ and $j=1,\\ldots,L$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; compute gradient $\\frac{1}{M}\\sum_{x_i\\in\\mathcal{D}_m}\\nabla_{\\phi, \\theta} \\hat{\\mathcal{L}}(\\theta,\\phi;x_i)$, where $\\hat{\\mathcal{L}}$ is $\\hat{\\mathcal{L}}^A$ or $\\hat{\\mathcal{L}}^B$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; update parameters by applying gradient with a NN optimiser (e.g. SGD, Adam)\n",
    "\n",
    "<center><img src=\"figures/vae.png\" alt=\"VAE sketch\" style=\"width: 900px;\"/></center>\n",
    "<center>The variational encoder. The encoder/inference network defines the latent variable distribution via the reparameterization trick, the decoder/generative network reconstructs the original input by defining a likelihood $p_\\theta(x\\mid z)$. The variational posterior $q_\\phi(z \\mid x)$ is penalised for varying too much from the prior $p_\\theta(z)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"vae_implementation\"></a>\n",
    "## VAE implementation\n",
    "\n",
    "In this section we will develop a full implementation of the variational autoencoder. This implementation will involve model subclassing, which is a fully flexible way to build models in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Frey Face dataset\n",
    "We will use the [Frey Face](https://cs.nyu.edu/~roweis/data.html) dataset to demonstrate the VAE, as in the original paper by [Kingma & Welling](#Kingma14). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "faces_data = np.load('./data/frey_faces.npy')\n",
    "faces_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val = train_test_split(faces_data, test_size=0.1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 4, 10\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "inx = np.random.choice(x_train.shape[0], n_rows*n_cols, replace=False)\n",
    "fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "\n",
    "for n, image in enumerate(x_train[inx]):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into DataLoaders\n",
    "\n",
    "import torch\n",
    "\n",
    "class FreyFaceDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, images):\n",
    "        self.images = (images / 255.).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        return (image,)\n",
    "\n",
    "\n",
    "train_dataset = FreyFaceDataset(x_train)\n",
    "val_dataset = FreyFaceDataset(x_val)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=100)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative model\n",
    "Recall that the generative model $p_\\theta({z})p_\\theta({x} \\mid {z})$ is defined by the prior $p_\\theta({z})$ and decoder $p_\\theta({x} \\mid {z})$. We will choose a standard isotropic Gaussian distribution for the prior.\n",
    "\n",
    "For the decoder, we follow [Kingma & Welling](#Kingma14) and use a Gaussian likelihood, but constrain the mean to $[0, 1]$. \n",
    "\n",
    "It is worth mentioning that it is also common practice to use an independent Bernoulli likelihood per pixel in the decoder for similar image datasets ([Kingma & Welling](#Kingma14) uses this for MNIST), despite this being incorrect as the data is not binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape\n",
    "\n",
    "img_h, img_w = 28, 20\n",
    "latent_dim = 2\n",
    "\n",
    "inputs = Input(shape=(latent_dim,))\n",
    "h = Dense(200, activation='relu')(inputs)\n",
    "h = Dense(img_h * img_w * 2)(h)\n",
    "h = Reshape((img_h, img_w, 2))(h)\n",
    "h1, h2 = ops.unstack(h, axis=-1)\n",
    "x_mean = ops.sigmoid(h1)\n",
    "x_log_std = h2\n",
    "\n",
    "decoder = Model(inputs=inputs, outputs=[x_mean, x_log_std], name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference model\n",
    "We now define the encoder, or inference model $q_\\phi(z\\mid x)$. We will use a diagonal Gaussian for the approximate posterior, where the mean and diagonal covariance matrix are predicted by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "\n",
    "from keras.layers import Flatten\n",
    "\n",
    "inputs = Input(shape=(img_h, img_w))\n",
    "h = Flatten()(inputs)\n",
    "h = Dense(200, activation='relu')(h)\n",
    "h = Dense(2 * latent_dim)(h)\n",
    "z_mean, z_log_var = ops.split(h, 2, axis=-1)\n",
    "\n",
    "encoder = Model(inputs=inputs, outputs=[z_mean, z_log_var], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the encoder and decoder\n",
    "We now compile and fit the encoder and decoder networks. Since the prior and approximate posterior are both Gaussian, we will use the second form of the SGVB estimator (we will set $L=1$):\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x\\mid z^{(j)}) − D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z)),\n",
    "$$\n",
    "\n",
    "We have chosen the prior $p_\\theta(z)$ to be $N(\\mathbf{0}, \\mathbf{I})$, and the approximate posterior can be written as $N(\\mu_q, \\text{diag}(\\sigma_q))$. In this case, we can write the KL divergence as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z)) = \\frac{1}{2}\\left[ \\mu_q^T\\mu_q + \\sum_{i=1}^l (\\sigma_q)_i - l - \\log \\prod_{i=1}^l (\\sigma_q)_i \\right],\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $l$ is the dimension of the latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the VAE, we will use [model subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/#the-model-class) to override the in-built `train_step` method (see the [TensorFlow](https://keras.io/guides/custom_train_step_in_tensorflow/) and [PyTorch](https://keras.io/guides/custom_train_step_in_torch/) guides). This gives us control over what happens when we call the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VAE Model object\n",
    "\n",
    "from keras.metrics import Mean\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_metric = Mean(name='loss')\n",
    "        self.nll_metric = Mean(name='nll')\n",
    "        self.kl_metric = Mean(name='kl')\n",
    "        self.pi = ops.array(np.pi)\n",
    "\n",
    "    def _get_losses(self, data):\n",
    "        data = data[0]\n",
    "        z_mean, z_log_var = self.encoder(data)\n",
    "        kl_loss = 0.5 * (ops.square(z_mean) + ops.exp(z_log_var) - 1 - z_log_var)\n",
    "        kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "        \n",
    "        epsilon = keras.random.normal(ops.shape(z_mean))\n",
    "        z_std = ops.exp(0.5 * z_log_var)\n",
    "        z_sample = z_mean + (z_std * epsilon)\n",
    "        \n",
    "        x_mean, x_log_std = self.decoder(z_sample)  # (B, 28, 20)\n",
    "        log_Z = 0.5 * ops.log(2 * self.pi)\n",
    "        nll_loss = 0.5 * ops.square((data - x_mean) / ops.exp(x_log_std)) + x_log_std + log_Z\n",
    "        nll_loss = ops.mean(ops.sum(nll_loss, axis=[-1, -2]))\n",
    "\n",
    "        loss = kl_loss + nll_loss\n",
    "        return loss, kl_loss, nll_loss\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        epsilon = keras.random.normal(ops.shape(z_mean))\n",
    "        z_std = ops.exp(0.5 * z_log_var)\n",
    "        z_sample = z_mean + (z_std * epsilon)\n",
    "        return self.decoder(z_sample)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if keras.config.backend() == 'tensorflow':\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "            grads = tape.gradient(loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        else:\n",
    "            assert keras.config.backend() == 'torch'\n",
    "            self.zero_grad()\n",
    "            loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gradients = [v.value.grad for v in self.trainable_weights]    \n",
    "            with torch.no_grad():\n",
    "                self.optimizer.apply(gradients, self.trainable_weights)\n",
    "            \n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.nll_metric.update_state(nll_loss)\n",
    "        self.kl_metric.update_state(kl_loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.nll_metric.update_state(nll_loss)\n",
    "        self.kl_metric.update_state(kl_loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric, self.nll_metric, self.kl_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model\n",
    "\n",
    "vae = VAE(encoder, decoder, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the Model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "history = vae.fit(train_dataloader, validation_data=val_dataloader, epochs=200, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.plot(history.history['kl'], label='train')\n",
    "plt.plot(history.history['val_kl'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"KL loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.plot(history.history['nll'], label='train')\n",
    "plt.plot(history.history['val_nll'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"NLL loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on the validation set\n",
    "\n",
    "vae.evaluate(val_dataloader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View samples and reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the generative model\n",
    "\n",
    "samples = ops.convert_to_numpy(vae.decoder(keras.random.normal(shape=(40, 2)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the samples\n",
    "\n",
    "n_rows, n_cols = 4, 10\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "\n",
    "for n, image in enumerate(samples):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstructions from the validation dataset\n",
    "\n",
    "images = next(iter(val_dataloader))[0]\n",
    "reconstructions = ops.convert_to_numpy(vae(images)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some reconstructions from the test dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "outer = gridspec.GridSpec(1, 2, hspace=0.2)\n",
    "n_rows, n_cols = 4, 5\n",
    "fig.text(0.23, 0.9, \"Test dataset images\", fontsize=14)\n",
    "fig.text(0.66, 0.9, \"VAE reconstructions\", fontsize=14)\n",
    "for i in range(2):\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(n_rows, n_cols,\n",
    "                    subplot_spec=outer[i], wspace=0., hspace=0.)\n",
    "    display_images = [images, reconstructions][i]\n",
    "    for j in range(n_rows * n_cols):\n",
    "        row = j // n_cols\n",
    "        col = j % n_cols\n",
    "        ax = plt.Subplot(fig, inner[j])\n",
    "        ax.imshow(display_images[j], cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        fig.add_subplot(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Rewrite the loss function above so the KL divergence is approximated with Monte Carlo samples, so the SGVB estimator $\\hat{\\mathcal{L}}^A(\\theta,\\phi;x)$ is used instead. Also try modifying the posterior to be a full covariance Gaussian. Does this improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Benou16\"></a>\n",
    "* Benou, A., Veksler, R., Friedman, A. & Raviv, T.R. (2016), \"De-noising of contrast-enhanced MRI sequences by an ensemble of expert deep neural networks\", in *International Workshop on Deep Learning in Medical Image Analysis*, Athens, Greece, 21 October 2016.\n",
    "<a class=\"anchor\" id=\"Blei17\"></a>\n",
    "* Blei, D. M., Kucukelbir, A. & McAuliffe, J. D. (2017), \"Variational Inference: A Review for Statisticians\", *Journal of the American Statistical Association*, **112** (518), 859-877.\n",
    "<a class=\"anchor\" id=\"Blei12\"></a>\n",
    "* Blei, D. M., Jordan, M. I. & Paisley, J. W. (2012), \"Variational bayesian inference with stochastic search\", in *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 1367–1374.\n",
    "<a class=\"anchor\" id=\"Devroye96\"></a>\n",
    "* Devroye, L. (1996), \"Random Variate Generation in One Line of Code\", in *Proceedings of the 28th Conference on Winter Simulation*, Coronado, California, USA, 265-272.\n",
    "<a class=\"anchor\" id=\"Dhariwal20\"></a>\n",
    "* Dhariwal, P., Heewoo, J., Payne, C., Kim, J. W., Radford, A. & Sutskever, I. (2020), \"Jukebox: A Generative Model for Music\", arXiv preprint, abs/2005.00341.\n",
    "<a class=\"anchor\" id=\"Glynn90\"></a>\n",
    "* Glynn, P. W. (1990), \"Likelihood Ratio Gradient Estimation for Stochastic Systems\", *Communications of the ACM*, **33** (10), 75-84.\n",
    "<a class=\"anchor\" id=\"Hoffman13\"></a>\n",
    "* Hoffman, M. D., Blei, D. M., Wang, C, & Paisley, J. (2013), \"Stochastic Variational Inference\", *Journal of Machine Learning Research*, **14** (1), 1532-4435.\n",
    "<a class=\"anchor\" id=\"Jin18\"></a>\n",
    "* Jin, W., Barzilay, R. & Jaakkola, T. (2018), \"Junction Tree Variational Autoencoder for Molecular Graph Generation\", in *Proceedings of Machine Learning Research*, **80**, 2323-2332.\n",
    "<a class=\"anchor\" id=\"Kingma14\"></a>\n",
    "* Kingma, D. P. & Welling, M., \"Auto-Encoding Variational Bayes\" (2014), in *Proceedings of the 2nd International Conference on Learning Representations (ICLR)*, Banff, AB, Canada, April 14-16, 2014.\n",
    "<a class=\"anchor\" id=\"Kleijnen96\"></a>\n",
    "* Kleijnen, J. P. C. & Rubinstein, R. Y. (1996), \"Optimization and sensitivity analysis of computer simulation models by the score function method\", *European Journal of Operational Research*, **88**, 413-427 .\n",
    "<a class=\"anchor\" id=\"Kramer91\"></a>\n",
    "* Kramer, M. A. (1991), \"Nonlinear principal component analysis using autoassociative neural networks\", *AIChE Journal*, **37** (2), 233–243. \n",
    "<a class=\"anchor\" id=\"Rezende14\"></a>\n",
    "* Rezende, D. J., Mohamed, S. & Wierstra, D. (2014), \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\", in *Proceedings of the 31st International Conference on Machine Learning, PMLR*, **32** (2), 1278-1286.\n",
    "<a class=\"anchor\" id=\"Salakhutdinov08\"></a>\n",
    "* Salakhutdinov, R. and Murray, I. (2008), \"On the quantitative analysis of deep belief networks\", in *Proceedings of the 25th international conference on Machine learning*, 892-879.\n",
    "<a class=\"anchor\" id=\"Vahdat20\"></a>\n",
    "* Vahdat, A. and Kautz, J. (2020), \"NVAE: A Deep Hierarchical Variational Autoencoder\", in *Proceedings of the 34th International Conference on Neural Information Processing Systems*.\n",
    "<a class=\"anchor\" id=\"Vincent10\"></a>\n",
    "* Vincent, P. & Larochelle, H. (2010), \"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\", *Journal of Machine Learning Research*, **11**, 3371–3408.\n",
    "<a class=\"anchor\" id=\"Williams92\"></a>\n",
    "* Williams, R. J. (1992), \"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning\", *Machine Learning*, **8** (3-4), 229-256.\n",
    "<a class=\"anchor\" id=\"Xu18\"></a>\n",
    "* Xu, H., Chen, W., Zhao, N., Li, Z., Bu, J., Li, Z., Liu, Y., Zhao, Y., Pei, D., Feng, Y., Chen, J. J., Wanb, Z. & Qiao, H. (2018), \"Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications\", *Proceedings of the 2018 World Wide Web Conference*, Palais des congrès de Lyon, Lyon, France, 23-27 April 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1d503f-c52a-4dde-802b-7e9eeb05a8ed",
   "metadata": {},
   "source": [
    "# Custom training loop in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a953e-22b3-4615-9648-24779d57426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364f7b0-3cd7-463b-964f-b774938661af",
   "metadata": {},
   "source": [
    "One of the benefits of using the Keras API is that it handles the execution of the training loop behind the scenes, minimising the need for boiler plate code. Keras is also sufficiently flexible that it can take care of most custom training algorithms; all models and training pipelines in this module can be done in Keras.\n",
    "\n",
    "Nevertheless, for completeness, in this notebook we will see how a low-level training loop can be implemented directly in PyTorch using the automatic differentiation tools we have covered. This approach breaks down the training loop and can give you extra flexibility when you need it. In this notebook, we will implement everything in PyTorch in order to provide a complete example for how a training loop can be constructed in this framework. Throughout the rest of the course, we will always use Keras to construct models, optimizers and losses.\n",
    "\n",
    "We will demonstrate the implementation of the training loop using a classifier model on the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e849517-8d1c-488c-8489-2136b42ae062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"~/torchdata\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "val_dataset = datasets.FashionMNIST(\n",
    "    root=\"~/torchdata\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1755eea-409e-419c-98df-205e43911cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "\n",
    "classes = train_dataset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd64a3-abfb-4e64-85c3-b925b76e8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a few training data examples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "random_inx = np.random.choice(len(train_dataset), n_rows * n_cols, replace=False)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "\n",
    "for n, i in enumerate(random_inx):\n",
    "    image = torch.squeeze(train_dataset[i][0], dim=0)\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image)\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "    axes[row, col].text(10., -1.5, f'{classes[train_dataset[i][1]]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05705f5-1089-4fc7-922e-9e21f9bf6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "batch_size = 3\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5579a0-8b69-43db-953a-959f0c628ba1",
   "metadata": {},
   "source": [
    "Below we will build the model in PyTorch for demonstration purposes. In this course we will always use Keras for model building in order to be backend agnostic. The `torch.nn` module contains many important building blocks for neural network models. As you can see below, the PyTorch API is similar to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f73a7-df0b-4c8c-9fca-4132121c3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from torch.nn import Sequential, Flatten, Linear, ReLU\n",
    "\n",
    "fashion_mnist_model = Sequential(\n",
    "    Flatten(),\n",
    "    Linear(784, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126302e-202f-4cc8-a811-12c92017b69d",
   "metadata": {},
   "source": [
    "PyTorch model layers/submodules (referred to as 'children') can be extracted using the `children` or `named_children` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea374fe-9602-4e60-b986-a5a13082ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, submodule in fashion_mnist_model.named_children():\n",
    "    print(name, submodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a7829-5b27-48e6-b08d-d3824bc815ce",
   "metadata": {},
   "source": [
    "An alternative (and common) way to create the same model as above is to subclass `torch.nn.Module`. Layers are defined in the `__init__` method, and the forward pass is defined in `forward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f877a3-4557-4ace-b37f-3e2194a63194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "class FashionMNISTModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(784, 64)\n",
    "        self.linear2 = Linear(64, 64)\n",
    "        self.linear3 = Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        return self.linear3(x)\n",
    "\n",
    "fashion_mnist_model = FashionMNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a59f-4dc7-497a-b77c-6bf4aeef7d94",
   "metadata": {},
   "source": [
    "The code below would still work if the model above is built with the Keras API using the `torch` backend. Likewise for the loss and optimizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db5c35-7789-463e-9ca2-c7bec31de46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimiser\n",
    "\n",
    "rmsprop = torch.optim.RMSprop(fashion_mnist_model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a03e84-42c9-4038-b7e4-46c53ef3c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800cc8f-887a-41bf-9c7f-000dab8bff59",
   "metadata": {},
   "source": [
    "The training loop consists of an outer loop than iterates through the epochs, and an inner loop that iterates through the dataset. At each inner iteration, we extract a batch of examples from the dataset, get the model predictions, compute the loss and apply a gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b2d38-bd51-4e1b-b171-254f9398b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the custom training loop\n",
    "\n",
    "import time\n",
    "\n",
    "epochs = 5\n",
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    fashion_mnist_model.train()\n",
    "    losses = []\n",
    "    for images, labels in train_loader:\n",
    "        rmsprop.zero_grad()\n",
    "        logits = fashion_mnist_model(images)\n",
    "        batch_loss = loss_fn(logits, labels)\n",
    "        batch_loss.backward()\n",
    "        rmsprop.step()        \n",
    "        losses.append(batch_loss.item())\n",
    "\n",
    "    fashion_mnist_model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            logits = fashion_mnist_model(images)\n",
    "            batch_loss = loss_fn(logits, labels)\n",
    "            val_losses.append(batch_loss.numpy())\n",
    "    \n",
    "    print(f\"End of epoch {epoch}, training loss: {np.mean(losses):.4f}, validation loss: {np.mean(val_losses):.4f}\")\n",
    "print(f\"End of training, time: {time.perf_counter() - start:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d525e-7fec-4f3f-b9ba-7ce9b63602f4",
   "metadata": {},
   "source": [
    "Some explanations are required in the above code:\n",
    "\n",
    "* Before the training loop, we set `fashion_mnist_model.train()` and before the validation loop, we set `fashion_mnist_model.eval()`. These commands set the model into different training/evaluation modes, and is necessary for layers such as batch normalization or dropout, that behave differently at training time and at test time. To check the model mode, you can use the `.training` flag.\n",
    "* Before running the forward pass in training, we set `rmsprop.zero_grad()`. This sets all accumulated gradients (on the leaf Tensors) to zero. Otherwise, the gradients would accumulate every time we call `batch_loss.backward()` (which we might want to do, for example if we want to use gradients averaged over multiple minibatches).\n",
    "    * We could alternatively write `fashion_mnist_model.zero_grad()`, which would be equivalent since we defined the `rmsprop` optimizer using all of the parameters of the model.\n",
    "* `rmsprop.step()` is the line that applies the optimizer update once the gradients have been calculated with `batch_loss.backward()`.\n",
    "* When performing the validation steps, we run the model with the `torch.no_grad` context. This context tells PyTorch that we do not want to compute gradients of any operations carried out within that context. In other words, any Tensor defined within this context will have `requires_grad` set to `False`, and we will not be able to call `backward()` on any of these Tensors. This saves a lot of computational overhead, and is useful when we are just evaluating a model and don't want to take gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b53a6-478c-4460-8348-2615c8fa5c01",
   "metadata": {},
   "source": [
    "Watch out for the following regarding loss function signatures: while all NumPy/TensorFlow/JAX/Keras APIs (as well as Python unittest APIs) use the argument order convention `fn(y_true, y_pred)` (reference values first, predicted values second), PyTorch uses `fn(y_pred, y_true)` for its losses. So make sure to invert the order of logits and targets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

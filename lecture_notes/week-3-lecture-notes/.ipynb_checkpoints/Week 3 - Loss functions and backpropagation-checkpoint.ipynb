{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 3: Loss functions and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Loss functions](#loss_functions)\n",
    "\n",
    "[3. Error backpropagation](#backprop)\n",
    "\n",
    "[4. Automatic differentiation (\\*)](#autodiff)\n",
    "\n",
    "[5. Datasets (\\*)](#datasets)\n",
    "\n",
    "[6. Dropout](#dropout)\n",
    "\n",
    "[7. Batch normalisation (\\*)](#batchnorm)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In the last week of the module we took a first look at the prototypical deep learning architecture, which is the multilayer perceptron. You also trained your first deep learning models in Keras, using the Sequential API, and learned the core methods `compile`, `fit`, `evaluate` and `predict`. You saw how the low level Tensor objects are included in these models to encapsulate mutable parameters and computational operations.\n",
    "\n",
    "In this week of the course, we will look closer at the issue of training neural networks. We will begin by walking through some commonly used loss functions and the principle of maximum likelihood, and study the important backpropagation algorithm, with a focus on its application to MLP models. We will also look at two commonly used techniques used in deep learning models, which are dropout and batch normalization. \n",
    "\n",
    "We will also learn how to implement all of these techniques in TensorFlow and PyTorch, and see how gradients can easily be computed using automatic differentiation tools in both of these frameworks. One of the major advantages of using deep learning frameworks like TensorFlow or PyTorch is the ability to automatically compute gradients of any differentiable operation. For most use cases, Keras will handle the gradient computation and parameter updates behind the scenes, but it will be useful later to understand how automatic differentiation is performed at a lower level, particularly when implementing custom training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"loss_functions\"></a>\n",
    "## Loss functions\n",
    "\n",
    "The loss function is the performance measure which is used to train neural networks. In order to enable gradient-based training, this loss function should be differentiable with respect to the model parameters. \n",
    "\n",
    "Given a task and some data, we need a way of formulating a suitable loss function to test our model. The principle that's often used to do this is maximum likelihood.\n",
    "\n",
    "#### The likelihood function\n",
    "\n",
    "A probability density function (or probability mass function) $P(y\\mid\\theta)$ is usually viewed as a function of a sample $y$, with the parameter(s) $\\theta$ fixed. In contrast, the likelihood function $\\mathcal{L}(\\theta \\mid y)$ considers $y$ to be fixed and is viewed as a function of $\\theta$:\n",
    "\n",
    "$$\\mathcal{L}(\\theta\\mid y) := P(y\\mid\\theta).$$\n",
    "\n",
    "For independent and identically distributed (i.i.d.) samples $y_1,\\ldots,y_N$, the probability density/mass function (and likelihood function) decomposes as the product\n",
    "\n",
    "$$\\mathcal{L}(\\theta\\mid y_1,\\ldots,y_N) = P(y_1,\\ldots,y_N\\mid\\theta) = \\prod_{i=1}^N P(y_i\\mid\\theta).$$\n",
    "\n",
    "An example is the [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with parameter $\\theta$. This is the distribution of a random variable that takes value 1 with probability $\\theta$ and 0 with probability $1-\\theta$. Let $P(y\\mid\\theta)$ be the probability that the event $y$ occurs given parameter $\\theta$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta\\mid y) = P(y\\mid\\theta) &= \\begin{cases}\n",
    "1 - \\theta \\quad \\text{if} \\, y = 0 \\\\\n",
    "\\theta \\quad \\quad \\, \\, \\, \\text{if} \\, y = 1 \\\\\n",
    "\\end{cases} \\\\\n",
    "&= (1 - \\theta)^{1 - y} \\theta^y \\quad y \\in \\{0, 1\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we assume samples $y_1,\\ldots,y_N$ are i.i.d., we also have\n",
    "$$\n",
    "\\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N) = \\prod_{i=1}^N (1 - \\theta)^{1 - y_i} \\theta^{y_i}.\n",
    "$$\n",
    "\n",
    "Another example is the [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). This distribution has two parameters: a mean $\\mu$ and a standard deviation $\\sigma$, so then $\\theta = (\\mu, \\sigma)$. The probability density function is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta\\mid y) = P(y | \\theta) = P(y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\Big( - \\frac{1}{2 \\sigma^2} (y - \\mu)^2 \\Big).\n",
    "$$\n",
    "\n",
    "For a sequence of i.i.d. observations $y_1, \\ldots, y_N$, the likelihood is \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu, \\sigma\\mid y_1, \\ldots, y_N) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\Big( - \\frac{1}{2 \\sigma^2} (y_i - \\mu)^2 \\Big).\n",
    "$$\n",
    "\n",
    "#### Maximum likelihood estimation\n",
    "\n",
    "The likelihood function is commonly used in statistical inference when we are trying to fit a distribution to some data. Suppose we have observed data $y_1, \\ldots, y_N$, assumed to be from some distribution with unknown parameter $\\theta$, which we want to estimate. The likelihood is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N).\n",
    "$$\n",
    "\n",
    "The *maximum likelihood estimate* $\\theta_{\\text{MLE}}$ of the parameter $\\theta$ is then the value that maximises the likelihood $\\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N)$. \n",
    "\n",
    "Recall that, for i.i.d. observations, the likelihood becomes a product:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N) = \\prod_{i=1}^N P(y_i | \\theta).\n",
    "$$\n",
    "\n",
    "Furthermore, since the $\\log$ function is a strictly concave function, maximising the likelihood is equivalent to maximising the log-likelihood $\\log \\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N)$. This changes the product into a sum:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{\\text{MLE}} &= \\arg \\max_{\\theta} \\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N) \\\\\n",
    "&= \\arg \\max_{\\theta} \\log \\mathcal{L}(\\theta\\mid y_1, \\ldots, y_N) \\\\\n",
    "&= \\arg \\max_{\\theta} \\log \\prod_{i=1}^N \\mathcal{L}(\\theta\\mid y_i) \\\\\n",
    "&= \\arg \\max_{\\theta} \\sum_{i=1}^N \\log \\mathcal{L}(\\theta\\mid y_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Conventionally, we usually choose to instead equivalently *minimise* the *negative log-likelihood*:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{MLE}} = \\arg \\min_{\\theta} \\left( - \\sum_{i=1}^N \\log \\mathcal{L}(\\theta\\mid y_i) \\right).$$\n",
    "\n",
    "#### Training neural networks\n",
    "\n",
    "We can use the principle of maximum likelihood to formulate loss functions that are used to train neural networks. Given some training data $\\mathcal{D} := (x_i, y_i)_{i=1}^N$ of inputs $x_i$ and outputs $y_i$, we search for the weights of the neural network that minimise the negative log-likelihood of the data. \n",
    "\n",
    "For example, suppose we would like to train a neural network binary classifier. Then we have $y_i\\in\\{0, 1\\}$. Denote the neural network model as $f_\\theta:\\mathbb{R}^D\\mapsto [0, 1]$, where $\\theta$ are the parameters of the network. We can constrain the range of the network by using a sigmoid activation function on a single output neuron. Then we can interpret the output of the network as the probability that a given input $x_i$ has output $y_i = 1$:\n",
    "\n",
    "$$f_\\theta(x_i) = P_\\theta(y_i=1\\mid x_i).$$\n",
    "\n",
    "Therefore the network output is parameterising a Bernoulli distribution. The negative log-likelihood can be written\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\log \\mathcal{L}(\\theta\\mid\\mathcal{D}) &= -\\sum_{i=1}^N \\log \\mathcal{L}(\\theta \\mid y_i, x_i) \\\\\n",
    "&= -\\sum_{i=1}^N \\log\\left\\{ (1 - f_\\theta(x_i))^{(1 - y_i)} f_\\theta(x_i)^{y_i} \\right\\} \\\\\n",
    "&= -\\sum_{i=1}^N \\left\\{(1 - y_i) \\log (1 - f_\\theta(x_i)) + y_i \\log f_\\theta(x_i) \\right\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and we have derived the binary cross entropy loss function to be minimised. \n",
    "\n",
    "As another example, suppose we would like to train a network as a regression function, so that $y_i\\in\\mathbb{R}$. Again denote  the neural network model as $f_\\theta:\\mathbb{R}^D\\mapsto \\mathbb{R}$, where $\\theta$ are the parameters of the network. This time, we can use no activation function on the output neuron of the network. \n",
    "\n",
    "We can additionally define our model such that the network $f_\\theta(x_i)$ parameterises the mean of a normal distribution, so that\n",
    "\n",
    "$$\n",
    "y_i \\sim N(f_\\theta(x_i), \\sigma^2)\n",
    "$$\n",
    "\n",
    "for some fixed $\\sigma^2 > 0$. In this case, the negative log-likelihood can be written\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\log \\mathcal{L}(\\theta\\mid\\mathcal{D}) &= -\\sum_{i=1}^N \\log \\mathcal{L}(\\theta \\mid y_i, x_i) \\\\\n",
    "&= -\\sum_{i=1}^N \\log\\left\\{ \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left( -\\frac{1}{2\\sigma^2}(f_\\theta(x_i) - y_i)^2 \\right) \\right\\} \\\\\n",
    "&= \\sum_{i=1}^N \\frac{1}{2\\sigma^2} (f_\\theta(x_i) - y_i)^2 + \\log(\\sigma\\sqrt{2\\pi}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term $\\log(\\sigma\\sqrt{2\\pi})$ does not depend on the parameters $\\theta$ that are to be optimised, and so can be ignored for the purpose of finding $\\theta_\\text{MLE}$. Then we have derived the mean squared error loss, up to a scaling factor (which also does not affect $\\theta_{\\text{MLE}}$ as the minimizer).\n",
    "\n",
    "Note that importantly, these loss functions are differentiable with respect to the parameters $\\theta$, so long as the network function $f_\\theta$ itself is differentiable. This is important to allow the use of gradient-based optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a class=\"anchor\" id=\"backprop\"></a>\n",
    "## Error backpropagation\n",
    "\n",
    "Gradient-based neural network optimisation can be seen as iterating over the following two main steps:\n",
    "\n",
    "1. Computation of the (stochastic) gradient of the loss function with respect to the model parameters\n",
    "2. Use of the computed gradient to update the parameters\n",
    "\n",
    "We have already seen the parameter update rule according to stochastic gradient descent for a neural network model $f_\\theta:\\mathbb{R}^D\\mapsto Y$, where $Y$ is the target space (e.g. $\\mathbb{R}^{n_{L+1}}$ or $[0, 1]^{n_{L+1}}$):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_t L(\\theta_t; \\mathcal{D}_m),\\qquad t\\in\\mathbb{N}_0. \\tag{1} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In the above equation, the minibatch loss $L(\\theta_t; \\mathcal{D}_m)$ is calculated on a randomly drawn sample of data points from the training set,\n",
    "\n",
    "$$\n",
    "L(\\theta_t; \\mathcal{D}_m) = \\frac{1}{M} \\sum_{x_i, y_i\\in\\mathcal{D}_m} l(y_i, f_{\\theta_t}(x_i)). \\tag{2}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}_m$ is the randomly sampled minibatch, $M = |\\mathcal{D}_m| \\ll |\\mathcal{D}_{train}|$ is the size of the minibatch, and we will denote $L_i:Y\\times Y\\mapsto \\mathbb{R}$, given by $L_i := l(y_i, f_\\theta(x_i))$, as the per-example loss. \n",
    "\n",
    "The update $(1)$ requires computation of the term $\\nabla_t L(\\theta_t; \\mathcal{D}_m)$ (from here we drop the $\\mathcal{D}_m$ in this expresesion for brevity); that is, the gradient of the loss function with respect to all of the model parameters, evaluated at the current parameter settings $\\theta_t$.\n",
    "\n",
    "The computation of the derivatives is done through applying the chain rule of differentiation. The algorithm for computing these derivatives in an efficient manner is known as **backpropagation**, and was popularised for use in neural network optimisation in [Rumelhart et al 1986b](#Rumelhart86b) and [Rumelhart et al 1986c](#Rumelhart86c), although the technique dates back earlier, see e.g. [Werbos](#Werbos94) which includes Paul Werbos' 1974 dissertation.\n",
    "\n",
    "In this section, we will derive the important backpropagation algorithm for finding the loss function derivatives for a multilayer perceptron.\n",
    "\n",
    "First recall the layer transformations in the MLP:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(0)} &:= \\mathbf{x}, \\tag{3}\\\\\n",
    "\\mathbf{h}^{(k)} &= \\sigma\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, L,\\tag{4}\\\\\n",
    "\\hat{\\mathbf{y}} &= \\sigma_{out}\\left( \\mathbf{w}^{(L)}\\mathbf{h}^{(L)} + b^{(L)} \\right), \\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$, $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$, $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{n_k}$, $\\hat{\\mathbf{y}}\\in Y$, $\\sigma, \\sigma_{out}:\\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions that are applied element-wise, $n_0 := D$, and $n_k$ is the number of units in the $k$-th hidden layer. \n",
    "\n",
    "Also recall that we define the **pre-activations**\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(k)} = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\tag{6}\n",
    "$$\n",
    "\n",
    "and **post-activations**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k)} = \\sigma(\\mathbf{a}^{(k)}). \\tag{7}\n",
    "$$\n",
    "\n",
    "We will consider the gradient of the loss computed on a single data example $\\nabla_t {L}_i(\\theta_t)$, given the sum $(2)$. We first compute the **forward pass** $(3)--(5)$ and store the preactivations $\\mathbf{a}^{(k)}$ and post-activations $\\mathbf{h}^{(k)}$.\n",
    "\n",
    "<center><img src=\"figures/forward.png\" alt=\"Forward pass\" style=\"width: 800px;\"/></center>\n",
    "<center>Pre-activations, post-activations, weights and biases in the forward pass</center>\n",
    "<br>\n",
    "\n",
    "Consider the derivative of $L_i$ with respect to $w^{(k)}_{pq}$ and $b^{(k)}_p$. We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} &= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p} \\frac{\\partial a^{(k+1)}_p}{\\partial w^{(k)}_{pq}} \\\\\n",
    "&= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p} h^{(k)}_q,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the second line follows from $(6)$. Similarly,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} &= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p} \\frac{\\partial a^{(k+1)}_p}{\\partial b^{(k)}_{p}} \\\\\n",
    "&= \\frac{\\partial L_i}{\\partial a^{(k+1)}_p}. \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We introduce the notation $\\delta^{(k)}_p := \\frac{\\partial L_i}{\\partial a^{(k)}_p}$, called the **error**. We then write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} &= \\delta^{(k+1)}_p h^{(k)}_q \\tag{8}\\\\\n",
    "\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} &= \\delta^{(k+1)}_p. \\tag{9}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We therefore need to compute the quantity $\\delta^{(k+1)}_p$ for each hidden and output unit in the network. Again using the chain rule, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(k)}_p \\equiv \\frac{\\partial L_i}{\\partial a^{(k)}_p} &= \\sum_{j=1}^{n_{k+1}} \\frac{\\partial L_i}{\\partial a^{(k+1)}_j} \\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p} \\\\\n",
    "&= \\sum_{j=1}^{n_{k+1}} \\delta^{(k+1)}_j \\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p} \\tag{10}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining $(6)$ and $(7)$ we see that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(k+1)}_j &= \\sum_{l=1}^{n_k} w^{(k)}_{jl} \\sigma(a^{(k)}_l) + b^{(k)}_p \\tag{11}\\\\\n",
    "\\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p} &= w^{(k)}_{jp} \\sigma'(a^{(k)}_p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\sigma'$ is the derivative of the activation function. So from the above equation and $(10)$ we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(k)}_p \\equiv \\frac{\\partial L_i}{\\partial a^{(k)}_p} &= \\sum_{j=1}^{n_{k+1}} \\delta^{(k+1)}_p \\frac{\\partial a^{(k+1)}_j}{\\partial a^{(k)}_p}\\\\\n",
    "&=  \\sigma'(a^{(k)}_p) \\sum_{j=1}^{n_{k+1}} w^{(k)}_{jp}  \\delta^{(k+1)}_j \\tag{12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Equation $(12)$ is analogous to $(11)$, and describes the backpropagation of errors through the network. We can write it in the more concise form (analogous to $(7)$):\n",
    "\n",
    "$$\n",
    "\\mathbf{\\delta}^{(k)} = \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\sigma}'(\\mathbf{a}^{(k)}) = \\text{diag} ([\\mathbf{\\sigma}'(a^{(k)}_p)]_{p=1}^{n_k})$.\n",
    "\n",
    "<center><img src=\"figures/forward_backward_pass.png\" alt=\"Forward and backward passes\" style=\"width: 650px;\"/></center>\n",
    "<center>Forward and backward passes</center>\n",
    "<br>\n",
    "\n",
    "Now we can summarise the backpropagation algorithm as follows:\n",
    "\n",
    ">1. Propagate the signal forwards by passing an input vector $x_i$ through the network and computing all pre-activations and post-activations using $\\mathbf{a}^{(k)} = \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)}$\n",
    "> 2. Evaluate $\\mathbf{\\delta}^{(L+1)} = \\frac{\\partial L_i}{\\partial \\mathbf{a}^{(L+1)}}$ for the output neurons\n",
    "> 3. Backpropagate the errors to compute $\\mathbf{\\delta}^{(k)}$ for each hidden unit using $\\mathbf{\\delta}^{(k)} = \\mathbf{\\sigma}'(\\mathbf{a}^{(k)})(\\mathbf{W}^{(k)})^T \\mathbf{\\delta}^{(k+1)}$\n",
    "> 4. Obtain the derivatives of $L_i$ with respect to the weights and biases using $\\frac{\\partial L_i}{\\partial w^{(k)}_{pq}} = \\delta^{(k+1)}_p h^{(k)}_q,\\quad \n",
    "\\frac{\\partial L_i}{\\partial b^{(k)}_{p}} = \\delta^{(k+1)}_p$\n",
    "\n",
    "The backpropagation algorithm can easily be extended to apply to any directed acyclic graph, but we have presented it here in the case of MLPs for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"autodiff\"></a>\n",
    "## Automatic differentiation\n",
    "\n",
    "One of the major advantages of using deep learning frameworks like TensorFlow or PyTorch is the ability to automatically compute gradients of any differentiable operation. In both frameworks, the process consists of recording the operations that take place in the forward pass, and then in the backward pass the gradients are computed by traversing this sequence of operations in reverse.\n",
    "\n",
    "When using the Keras `model.fit` API, Keras applies the backpropagation equations automatically to compute gradients, and then uses the optimiser algorithm selected to update the parameters.\n",
    "\n",
    "In this section, we will see how lower-level tools in TensorFlow and PyTorch can be leveraged to compute gradients of differentiable expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow\n",
    "\n",
    "_NB: You should restart the jupyter kernel before running this section in order to ensure the backend is set correctly._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, operations that you want to take gradients with respect to need to be defined inside a `tf.GradientTape` context. This context can be thought of as setting a tape recording all operations that take place within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple operation and take the gradient\n",
    "\n",
    "x = tf.constant(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = 4 * tf.square(x) - x\n",
    "dydx = tape.gradient(y, x)\n",
    "dydx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tape.watch` line tells TensorFlow to track all operations that apply to the Tensor `x`. A new Tensor `y` is defined as a sequence of operations that include `x`, and so the gradient of `y` with respect to `x` can be computed with `tape.gradient`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take multiple derivatives\n",
    "\n",
    "x = tf.constant([[1.5, 2.], [0., 1.1]])\n",
    "y = tf.constant([-1., 0.5])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x, y])\n",
    "    a = tf.constant([0.3, 2.5])\n",
    "    z = tf.reduce_mean(x**2, axis=0) * a + y\n",
    "tape.gradient(z, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients can also be computed with respect to intermediate variables\n",
    "\n",
    "x = tf.random.normal((2, 3))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.math.sin(x)\n",
    "    a = tf.random.normal((3, 2))\n",
    "    z = tf.reduce_mean(tf.linalg.matmul(y, a))\n",
    "dzdx, dzdy = tape.gradient(z, [x, y])\n",
    "print(dzdx)\n",
    "print(dzdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients can be taken once by default\n",
    "\n",
    "x = tf.random.normal((2, 3))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.math.sin(x)\n",
    "    a = tf.random.normal((3, 2))\n",
    "    z = tf.reduce_mean(tf.matmul(y, a))\n",
    "dzdx = tape.gradient(z, x)\n",
    "dzdy = tape.gradient(z, y)\n",
    "del tape  \n",
    "print(dzdx)\n",
    "print(dzdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TensorFlow `Variable` objects are always automatically tracked and so there is no need to call `tape.watch` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable objects are tracked automatically\n",
    "\n",
    "x = tf.Variable(x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.math.sin(x)\n",
    "    a = tf.random.normal((3, 2))\n",
    "    z = tf.reduce_mean(tf.matmul(y, a))\n",
    "tape.gradient(z, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take gradients with respect to a layer operation\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "x = tf.random.normal((1, 4))\n",
    "dense_layer = Dense(1, activation='elu')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = dense_layer(x)\n",
    "grads = tape.gradient(y, dense_layer.trainable_variables)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take gradients with respect to a model operation\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(3, activation='sigmoid'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "x = tf.random.uniform((2, 2))\n",
    "y = tf.random.uniform((2, 1))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = tf.keras.losses.mse(y, model(x)) \n",
    "grads = tape.gradient(loss, model.trainable_weights)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on automatic differentiation in TensorFlow, see the guide [here](https://www.tensorflow.org/guide/autodiff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise.* Use `tf.GradientTape` to make a plot of the function $\\frac{dy}{dx}:[-5, 5]\\mapsto\\mathbb{R}$, where $y = \\sin (x^2) - \\frac{x^2}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch\n",
    "\n",
    "_NB: You should restart the jupyter kernel before running this section in order to ensure the backend is set correctly._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "import keras\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the computational graph, PyTorch will record all operations that apply to Tensors that are created with the `requires_grad` property set to `True`. `requires_grad` is contagious. This means whenever we use a Tensor with `requires_grad=True` in an operation, the resulting Tensor will also have `requires_grad` set to `True`. By default, newly created Tensors will have `requires_grad` set to `False`, unless they are created as a `torch.nn.Parameter`. (NB: an existing Tensor which has `requires_grad=False` can have this property set to `True` using the `requires_grad_()` method.)\n",
    "\n",
    "Each Tensor has an attribute called `grad_fn`, this defines the equation necessary to calculate the gradient. If `requires_grad` is set to `False`, `grad_fn` would be `None`.\n",
    "\n",
    "The `backward` function computes the gradient of the current Tensor with respect to the graph leaves, and stores the result in the respective Tensors’ `.grad` attribute. What is a graph leaf? Leaf nodes are the values from which the computation begins. When a Tensor is created and `requires_grad` is set to True, that will be a leaf node. Whenever a computation is executed on it, the resulting Tensor will not be a leaf anymore. Typically, in our deep learning models, model parameters are leaf nodes. To check if a Tensor is a leaf, simply check its `is_leaf` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create leaf and non-leaf Tensors\n",
    "\n",
    "a = torch.rand(10, requires_grad=True)\n",
    "print(a.is_leaf)\n",
    "\n",
    "b = torch.rand(10, requires_grad=True) + 4\n",
    "print(b.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple operation and take the gradient\n",
    "\n",
    "x = torch.tensor(3., requires_grad=True)\n",
    "print(x.grad)  # Initially set to None\n",
    "\n",
    "y = 4 * torch.square(x) - x\n",
    "print(y.requires_grad)  # True, requires_grad is contagious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now compute the gradients using backward\n",
    "\n",
    "dydx = y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()` can be called with no arguments only with scalar Tensors. If `backward` is called on a non-scalar Tensor `x` then you need to pass in a Tensor `gradient` of the same shape as `x` - you can think of the Tensor `gradient` as defining the gradient of a scalar valued function with respect to `x`. The resulting stored gradients `.grad` in the leaf Tensors are then the gradients of the scalar valued function with respect to those leaf Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call backward on a non-scalar Tensor\n",
    "\n",
    "x = torch.tensor([[1.5, 2.], [0., 1.1]], requires_grad=True)\n",
    "y = torch.tensor([-1., 0.5], requires_grad=False)\n",
    "y.requires_grad_()  # Can also manually set requires_grad after the Tensor is created\n",
    "\n",
    "a = torch.tensor([0.3, 2.5])\n",
    "z = torch.mean(x**2, dim=0) * a + y\n",
    "\n",
    "z.backward(torch.FloatTensor([1., 1.]))\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, PyTorch only stores gradients in leaf Tensors. To force it to store the gradient value, call `.retain_grad()` on the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients can also be computed with respect to intermediate variables\n",
    "\n",
    "x = torch.normal(mean=0, std=1, size=(2, 3), requires_grad=True)\n",
    "\n",
    "y = torch.sin(x)\n",
    "y.retain_grad()\n",
    "a = torch.normal(mean=0, std=1, size=(3, 2))\n",
    "z = torch.mean(torch.matmul(y, a))\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take gradients with respect to a layer operation\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "x = torch.normal(mean=0, std=1, size=(1, 4))\n",
    "dense_layer = Dense(1, activation='elu')\n",
    "y = dense_layer(x)\n",
    "y.backward()\n",
    "\n",
    "for param in dense_layer.trainable_variables:\n",
    "    print(param.name, param.value.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One subtlety in the above is that since the `Dense` layer is a Keras object, the parameters of the layer are Keras objects that wrap the underlying torch parameters. The torch parameters can be accessed using the `value` attribute as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take gradients with respect to a model operation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import ops\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(3, activation='sigmoid'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "x = torch.rand((2, 2))\n",
    "y = torch.rand((2, 1))\n",
    "\n",
    "loss = ops.mean(keras.losses.mean_squared_error(y, model(x)))\n",
    "loss.backward()\n",
    "\n",
    "for param in model.trainable_variables:\n",
    "    print(param.name, param.value.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on automatic differentiation in PyTorch, see the guide [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise.* Use `backward()` to make a plot of the function $\\frac{dy}{dx}:[-5, 5]\\mapsto\\mathbb{R}$, where $y = \\sin (x^2) - \\frac{x^2}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"datasets\"></a>\n",
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow and PyTorch both have tools for working with Datasets. \n",
    "\n",
    "In TensorFlow, the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) module can be used to load the training data into a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object, which is equipped with efficient data processing pipelines, and often improves overall performance. See [here](https://www.tensorflow.org/guide/data) for a guide on the use of these Dataset objects.\n",
    "\n",
    "In PyTorch, the [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) module can be used to store the training data into a [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) object, and iterate through the dataset using a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) object. This pipeline is further equipped with data processing functionality. See [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for a guide on the use of Dataset and DataLoader objects.\n",
    "\n",
    "One of the nice features of Keras is that either of these two data pipelines can be used to train models, regardless of the backend selected.\n",
    "\n",
    "In this section, we will get a working knowledge of TensorFlow Datasets and PyTorch Datasets and DataLoaders. In both cases we will use the Fashion-MNIST dataset as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow\n",
    "In TensorFlow, we will use the `tf.data` module, and the `tf.data.Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`from_tensor_slices`**\n",
    "\n",
    "If data is stored in NumPy arrays as above, then it is easy to create a `tf.data.Dataset` object using the `from_tensor_slices` method. If we pass in a tuple of arrays as below, then the Dataset will correspondingly return tuples of Tensors. The first dimension of the arrays needs to be the same, as this is assumed to index the elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a tf.data.Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the Dataset object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`map` and `filter`**\n",
    "\n",
    "`Dataset` objects come with `map` and `filter` methods for data preprocessing on the fly. For example, we can normalise the pixel values to the range $[0, 1]$ with the `map` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the pixel values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also filter out data examples according to some criterion with the `filter` method. For example, if we wanted to exclude all data examples with label $9$ from the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all examples with label 9 (ankle boot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a prefetch step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the batched and shuffled Dataset object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pattern is to create a `Dataset` object as above, that returns a tuple of `(inputs, outputs)`. As we will see below, this `Dataset` object can then be used in the `model.fit` call instead of passing in input and output arrays. Keras detects that a `Dataset` object has been passed in, and expects it to return tuples of inputs and outputs as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch\n",
    "In PyTorch, we will use the `torch.utils.data` module, and the `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the pixel values and change dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TensorDataset`**\n",
    "\n",
    "If data is stored in NumPy arrays as above, then it is easy to create a `torch.utils.data.Dataset` object using the `torch.utils.data.TensorDataset` class. If we pass in Tensors as below, then the Dataset will correspondingly return tuples of Tensors. The first dimension of the Tensors needs to be the same, as this is assumed to index the elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a torch.utils.data.TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom `Dataset`**\n",
    "\n",
    "For extra flexibility, it is possible to create custom `Dataset` classes that incorporate transformations. Your custom class should implement `__init__`, `__len__` and `__getitem__` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the NumPy arrays from the Keras API in custom Dataset, including normalisation transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`torchvision` datasets**\n",
    "\n",
    "PyTorch also has its own API for loading common datasets such as Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset using torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will download and store the dataset at the location `~/torchdata` (if it is not already there). The `train=True` argument specifies to download the training split (a test split is also available with `train=False`). If no transform is specified, the images are returned as `PIL.Image.Image` objects, but we need them represented as Tensor objects. The transform `ToTensor` accomplishes this for us, including scaling the pixel values to the interval $[0, 1]$. The final transform reshapes the images from `(1, 28, 28)` to `(28, 28)` (removes the channel axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also filter out data examples according to some criterion with the `Subset` class. For example, if we wanted to exclude all data examples with label $9$ from the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all examples with label 9 (ankle boot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically do not iterate directly through the Dataset. Instead, a `DataLoader` handles this, including constructing minibatches, reshuffling the data at every epoch, and speeding up performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataLoader` is an iterable, and we can pull minibatches from it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pattern is to create a `DataLoader` object as above, that returns a tuple of `(inputs, outputs)`. As we will see below, this `DataLoader` object can then be used in the `model.fit` call instead of passing in input and output arrays. Keras detects that a `DataLoader` object has been passed in, and expects it to return tuples of inputs and outputs as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show that a Keras model can be trained on either TensorFlow `Dataset` objects or PyTorch `DataLoader` objects, regardless of which backend is selected. Try running the following trainings with different backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Keras backend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss and optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a model and train on the TensorFlow `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the TensorFlow Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train again using the PyTorch `DataLoader`, we create a new model instance and also a new optimizer object, since the existing optimizer is tied to the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the PyTorch Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dropout\"></a>\n",
    "## Dropout\n",
    "\n",
    "Dropout was introduced by [Srivastava et al](#Srivastava14) in 2014 as a regularisation technique for neural networks, that also has the effect of modifying the behaviour of neurons within a network.\n",
    "\n",
    "The following is taken from the paper abstract:\n",
    "\n",
    "> Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods.\n",
    "\n",
    "The method of dropout is to randomly 'zero out' neurons (or equivalently, weight connections) in the network during training according to a Bernoulli mask whose values are independently sampled at every iteration. \n",
    "\n",
    "Suppose $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_{k}}$ is a weight matrix mapping neurons in layer $k$ to layer $k+1$:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k+1)} = \\sigma\\left( \\mathbf{W}^{(k)}\\mathbf{h}^{(k)} + \\mathbf{b}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "We can view dropout as randomly replacing each column of $\\mathbf{W}^{(k)}$ with zeros with probability $p_k$. We can write this as applying a Bernoulli mask:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{(k)} &\\leftarrow \\mathbf{W}^{(k)} \\cdot\\text{diag} ([\\mathbf{z}_{k, j}]_{j=1}^{n_{k-1}})\\\\\n",
    "\\mathbf{z}_{k, j} &\\sim \\text{Bernoulli}(p_k), \\qquad k=1,\\ldots, L,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_{k}}$. The following diagrams illustrate the effect of dropout on a neural network.\n",
    "\n",
    "<center><img src=\"figures/no_dropout.png\" alt=\"MLP with a two hidden layers\" style=\"width: 700px;\"/></center>\n",
    "<center>Neural network without dropout</center>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/dropout1.png\" alt=\"MLP with a two hidden layers\" style=\"width: 700px;\"/></center>\n",
    "<center>Neural network with dropout</center>\n",
    "<br>\n",
    "\n",
    "By randomly dropping out neurons in the network, one obvious effect is that the capacity of the model is reduced, and so there is a regularisation effect. Each randomly sampled Bernoulli mask defines a new 'sub-network' that is smaller than the original. \n",
    "\n",
    "In addition, a key motivation of dropout is that it prevents neurons from co-adapting too much. Any neuron in the network is no longer able to depend on any other specific neurons being present, and so each neuron learns features that are more robust, and generalise better.\n",
    "\n",
    "In the figure below (taken from the [original paper](#Srivastava14)) we see features that are learned on the MNIST dataset for a model trained without dropout (left) and one trained with dropout (right). We see that the dropout model learns features that are much less noisy and more meaningful (it is detecting edges, textures, spots etc.) and help the model to generalise better. The non-dropout model's features suggest a large degree of co-adaptation, where the neurons depend on the specific combination of features in order to make good predictions on the training data.\n",
    "\n",
    "<center><img src=\"figures/dropout_no_dropout.png\" alt=\"Features with and without dropout\" style=\"width: 700px;\"/></center>\n",
    "<center>Learned features in a neural network trained without dropout (left) and with dropout (right). From Srivastava et al 2014</center>\n",
    "<br>\n",
    "\n",
    "Typically, dropout is applied only in the training phase. When making predictions, all weight connections  $\\mathbf{W}^{(k)}$ are restored, but rescaled by a factor of $1 - p_k$ to take account for the fact that fewer connections were present at training.\n",
    "\n",
    "However, [Gal & Ghahramani](#Gal16) describe a Bayesian interpretation of dropout, and proposed that dropout is also applied at test time in order to obtain a Bayesian predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, dropout is available to include in your models as another layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "dropout_model = Sequential([\n",
    "    Dense(64, activation='sigmoid', input_shape=(784,)),\n",
    "    Dropout(rate=0.7),\n",
    "    Dense(64, activation='sigmoid'),\n",
    "    Dropout(rate=0.7),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "dropout_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"batchnorm\"></a>\n",
    "## Batch normalisation\n",
    "\n",
    "Batch normalisation ([Ioffe & Szegedy 2015](#Ioffe15)) is a widely used method in deep learning. It is used to normalise the distribution of internal activation values in the network, and greatly helps to stabilise learning especially in deep networks. \n",
    "\n",
    "The core issue is that of **covariate shift**, which is the change in distribution of input variables to a machine learning model. This can happen in datasets where there is some change in conditions in subsequent data collections, for example over time or location. Whilst the underlying target function might not have changed, the distribution of the input variables does change which means the model could perform poorly in the changed conditions.\n",
    "\n",
    "The following shows a simple example of a regression function that fails to generalise to new data points whose distribution has shifted from the training data, even though the underlying target function is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def target(x):\n",
    "    return x**3 - 15 * x - 12\n",
    "\n",
    "n_samples = 100\n",
    "x_train_all = np.linspace(-5, 5, n_samples)[..., np.newaxis]\n",
    "x_train_sub = x_train_all[30:]\n",
    "y_train_all = target(x_train_all) + 10 * np.random.randn(n_samples, 1)\n",
    "y_train_sub = y_train_all[30:]\n",
    "\n",
    "kernel_regressor_sub = KernelRidge(alpha=1e-2, kernel='rbf', gamma=0.5)\n",
    "kernel_regressor_sub.fit(x_train_sub, y_train_sub)\n",
    "mse1 = np.mean((kernel_regressor_sub.predict(x_train_sub) - y_train_sub)**2)\n",
    "\n",
    "kernel_regressor_all = KernelRidge(alpha=1e-2, kernel='rbf', gamma=0.5)\n",
    "kernel_regressor_all.fit(x_train_all, y_train_all)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(x_train_sub, target(x_train_sub), '--')\n",
    "plt.plot(x_train_all, kernel_regressor_sub.predict(x_train_all))\n",
    "plt.scatter(x_train_sub, y_train_sub, alpha=0.5)\n",
    "plt.title(\"Regression function and training data\")\n",
    "plt.legend(['Target function', 'Kernel regressor'])\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(x_train_all, target(x_train_all), '--')\n",
    "plt.plot(x_train_all, kernel_regressor_sub.predict(x_train_all))\n",
    "plt.scatter(x_train_all, y_train_all, alpha=0.5)\n",
    "plt.title(\"Same regression function on shifted data\")\n",
    "plt.legend(['Target function', 'Kernel regressor'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same phenomenon can occur during the course of training deep learning models on large datasets, where stochastic minibatches are used in the optimisation procedure. Furthermore, since deep learning models can be viewed as hierarchical feature extractors, we can encounter problems of **internal covariate shift**, where the activation values in hidden layers also undergo changes of distribution due to changes in parameter values and activations in earlier layers. \n",
    "\n",
    "<center><img src=\"figures/internal_covariate_shift.png\" alt=\"Internal Covariate Shift\" style=\"width: 750px;\"/></center>\n",
    "<center>Changes in weights and activations earlier in the network cause internal covariate shift in activations in later layers</center>\n",
    "<br>\n",
    "\n",
    "Batch normalisation reduces the internal covariate shift by normalising the mean and variance of the activation values in a layer. Intuitively speaking, this means that although layer inputs will change over the course of training, they won't change so much that learning becomes very slow or unstable. Batch normalisation also has a slight regularisation effect on the network.\n",
    "\n",
    "For a layer with $n_k$-dimensional input $\\mathbf{h}^{(k)} = (h^{(k)}_1,\\ldots,h^{(k)}_{n_k})$, we normalise each input feature\n",
    "\n",
    "$$\n",
    "\\hat{h}^{(k)}_j = \\frac{h^{(k)}_j - \\mathbb{E}[h^{(k)}_j]}{\\sqrt{\\text{Var}[h^{(k)}_j]}}.\n",
    "$$\n",
    "\n",
    "In order to maintain full expressive power of the network, we make sure the final transformation can represent the identity:\n",
    "\n",
    "$$\n",
    "z^{(k)}_j = \\gamma^{(k)}_j \\hat{h}^{(k)}_j + \\beta^{(k)}_j,\n",
    "$$\n",
    "\n",
    "where $\\gamma^{(k)}_j$ and $\\beta^{(k)}_j$ are learned parameters. Note that setting $\\gamma^{(k)}_j = \\sqrt{\\text{Var}[h^{(k)}_j]}$ and $\\beta^{(k)}_j = \\mathbb{E}[h^{(k)}_j]$ recovers the original activations. However, now the model can control the mean and variance of activations within the hidden layer $\\mathbf{h}^{(k)}$ by tuning the parameters $\\gamma^{(k)}_j$ and $\\beta^{(k)}_j$. $\\mathbf{z}^{(k)}$ then becomes the new input to the next layer in the network.\n",
    "\n",
    "Statistics $\\mathbb{E}[h^{(k)}_j]$ and $\\text{Var}[h^{(k)}_j]$ are estimated over each minibatch $\\mathcal{D}_m$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu^{(k)}_{jm} &= \\frac{1}{M} \\sum_{i=1}^M h^{(k)}_{ij}\\\\\n",
    "\\left(\\sigma^{(k)}_{jm}\\right)^2 &= \\frac{1}{M} \\sum_{i=1}^M (h^{(k)}_{ij} - \\mu^{(k)}_{jm})^2\\\\\n",
    "\\hat{h}^{(k)}_{j} &= \\frac{h^{(k)}_{j} - \\mu^{(k)}_{jm}}{\\sqrt{\\left(\\sigma^{(k)}_{jm}\\right)^2 + \\epsilon}}\\\\\n",
    "z^{(k)}_{j} &= \\gamma^{(k)}_j\\hat{h}^{(k)}_{j} + \\beta^{(k)}_j =: BN_{\\gamma^{(k)}, \\beta^{(k)}}\\left(h^{(k)}_{j}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $M = |\\mathcal{D}_m|$, and $h^{(k)}_{ij}$ is the activation value for the $j$-th neuron for input $x_i\\in\\mathcal{D}_m$ in layer $k$.\n",
    "\n",
    "At training time, the estimates $\\mu^{(k)}_{jm}$ and $\\sigma^{(k)}_{jm}$ are computed on the minibatch $\\mathcal{D}_m$. In practical implementations, a running average of these estimates over the training run is used at test time.\n",
    "\n",
    "The batch normalisation calculation is fully differentiable, and so gradients can be backpropagated through this calculation as normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch normalisation operation is implemented as a layer in Keras. In the following experiment we will recreate the first example from the [original paper](#Ioffe15) on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Dataset objects\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also replace the above with a PyTorch `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixel values in the Datasets\n",
    "\n",
    "def normalise_pixels(image, label):\n",
    "    return (tf.cast(image, tf.float32) / 255., label)\n",
    "\n",
    "train_dataset = train_dataset.map(normalise_pixels)\n",
    "test_dataset = test_dataset.map(normalise_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the dataset\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000).batch(60)\n",
    "test_dataset = test_dataset.batch(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define an MLP classifier model for the MNIST dataset. The following demonstrates how to build a model using [the functional API](https://keras.io/guides/functional_api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classifier model using the functional API\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "inputs = Input(shape=(28, 28))\n",
    "h = Flatten()(inputs)\n",
    "h = Dense(100, activation='sigmoid', kernel_initializer=RandomNormal())(h)\n",
    "h = Dense(100, activation='sigmoid', kernel_initializer=RandomNormal())(h)\n",
    "h = Dense(100, activation='sigmoid', kernel_initializer=RandomNormal())(h)\n",
    "outputs = Dense(10, activation='softmax')(h)\n",
    "\n",
    "no_bn_model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "no_bn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function, optimiser and metric\n",
    "\n",
    "no_bn_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "no_bn_history = no_bn_model.fit(train_dataset, epochs=50, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-build the model with batch normalisation\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(28, 28))\n",
    "h = Flatten()(inputs)\n",
    "h = Dense(100, activation='sigmoid', kernel_initializer=RandomNormal())(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Dense(100, activation='sigmoid', kernel_initializer=RandomNormal())(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Dense(100, activation='sigmoid', kernel_initializer=RandomNormal())(h)\n",
    "h = BatchNormalization()(h)\n",
    "outputs = Dense(10, activation='softmax')(h)\n",
    "\n",
    "bn_model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "\n",
    "bn_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "bn_history = bn_model.fit(train_dataset, epochs=50, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the progress of the test accuracy in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(bn_history.history['accuracy'])\n",
    "plt.plot(no_bn_history.history['accuracy'], '--')\n",
    "plt.legend(['With BN', 'Without BN'])\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Test accuracy vs epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see clearly in the above plot that the batch normalisation layers help the model to train faster, and to a higher accuracy.\n",
    "\n",
    "Batch normalisation reduces internal covariate shift, particularly early on in training. The distribution is more stable, making learning easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "### References\n",
    "\n",
    "<a class=\"anchor\" id=\"Gal16\"></a>\n",
    "* Gal, Y. & Ghahramani, Z. (2016), \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\", Proceedings of The 33rd International Conference on Machine Learning, **48**, 1050-1059.\n",
    "<a class=\"anchor\" id=\"Ioffe15\"></a>\n",
    "* Ioffe, S. & Szegedy, C., \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", in *Proceedings of the 32nd International Conference on International Conference on Machine Learning*, **37**, 448–456.\n",
    "<a class=\"anchor\" id=\"Rumelhart86b\"></a>\n",
    "* Rumelhart, D. E., Hinton, G., and Williams, R. (1986b), \"Learning representations by back-propagating errors\", Nature, **323**, 533-536.\n",
    "<a class=\"anchor\" id=\"Rumelhart86c\"></a>\n",
    "* Rumelhart, D. E., Hinton, G., and Williams, R. (1986c), \"Learning Internal Representations by Error Propagation\", in Rumelhart, D. E.; McClelland, J. L. (eds.), Parallel Distributed Processing : Explorations in the Microstructure of Cognition. Volume 1: Foundations, Cambridge, MIT Press.\n",
    "<a class=\"anchor\" id=\"Srivastava14\"></a>\n",
    "* Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014), \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", Journal of Machine Learning Research, **15**, 1929-1958.\n",
    "<a class=\"anchor\" id=\"Werbos94\"></a>\n",
    "* Werbos, P. J. (1994), \"The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting\", New York:, John Wiley & Sons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

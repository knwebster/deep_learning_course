{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da00255c-beeb-4ab4-873b-da6869253cb8",
   "metadata": {},
   "source": [
    "# Custom training loop in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60345334-a5a6-4d77-85ff-dfb013377ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada9d7a-33e6-439d-86be-6901213f60b5",
   "metadata": {},
   "source": [
    "One of the benefits of using the Keras API is that it handles the execution of the training loop behind the scenes, minimising the need for boiler plate code. Keras is also sufficiently flexible that it can take care of most custom training algorithms; all models and training pipelines in this module can be done in Keras.\n",
    "\n",
    "Nevertheless, for completeness, in this notebook we will see how a low-level training loop can be implemented directly in TensorFlow using the automatic differentiation tools we have covered. (Note that Keras is included as a submodule in TensorFlow, so many of the constructions below will be familiar.) This approach breaks down the training loop and can give you extra flexibility when you need it. \n",
    "\n",
    "We will demonstrate the implementation of the training loop using a classifier model on the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e866e34-cdcd-46fa-aefc-74fd0157efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1612b9-385e-47ae-82ae-23f4444cbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648b1ca-d6c9-4d49-a906-025c87d38523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a few training data examples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "random_inx = np.random.choice(x_train.shape[0], n_rows * n_cols, replace=False)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "\n",
    "for n, i in enumerate(random_inx):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(x_train[i])\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "    axes[row, col].text(10., -1.5, f'{classes[y_train[i]]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3c147-e772-4f9a-8aeb-e9b728839b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10)\n",
    "    ], name='fashion_mnist_classifier')\n",
    "    return model\n",
    "\n",
    "fashion_mnist_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457996c2-f75b-404f-815f-0d0308ad814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "fashion_mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d60f18-f386-42cc-bda8-753603025860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimiser\n",
    "\n",
    "rmsprop = tf.keras.optimizers.RMSprop(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31885cc-3573-4f21-aeb9-e9d1e7283dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa234477-ee72-4d73-979d-ed397752cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into tf.data.Dataset objects\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3fd31-1041-450e-ab0f-28c7e58f3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the dataset, and batch the validation dataset\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size=32)\n",
    "val_dataset = val_dataset.batch(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41089d4d-f37b-445f-aa36-3117d3188e69",
   "metadata": {},
   "source": [
    "The training loop consists of an outer loop than iterates through the epochs, and an inner loop that iterates through the dataset. At each inner iteration, we extract a batch of examples from the dataset, get the model predictions, compute the loss and apply a gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7262c5-ca69-4312-8f42-a561fe83f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the custom training loop\n",
    "\n",
    "import time\n",
    "\n",
    "epochs = 5\n",
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    losses = []\n",
    "    for images, labels in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = fashion_mnist_model(images)\n",
    "            batch_loss = loss_fn(labels, logits)\n",
    "        grads = tape.gradient(batch_loss, fashion_mnist_model.trainable_weights)\n",
    "        losses.append(batch_loss.numpy())\n",
    "        \n",
    "        rmsprop.apply_gradients(zip(grads, fashion_mnist_model.trainable_weights))\n",
    "\n",
    "    val_losses = []\n",
    "    for images, labels in val_dataset:\n",
    "        logits = fashion_mnist_model(images)\n",
    "        batch_loss = loss_fn(labels, logits)\n",
    "        val_losses.append(batch_loss.numpy())\n",
    "    \n",
    "    print(f\"End of epoch {epoch}, training loss: {np.mean(losses):.4f}, validation loss: {np.mean(val_losses):.4f}\")\n",
    "print(f\"End of training, time: {time.perf_counter() - start:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc7690-e8b9-48bc-82d6-d9d00d1b1442",
   "metadata": {},
   "source": [
    "A custom training loop such as the one above can often be sped up significantly by compiling the training update step into a computational graph. This allows TensorFlow to make optimisations and can lead to performance gains. The `@tf.function` decorator can be used for this purpose. Below we pull out the main computational step of running the forward and backward passes into a separate function, to which we then apply the decorator to tell TensorFlow to construct a computational graph for this program. See [here](https://www.tensorflow.org/guide/function) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f389dff-aaf6-4b77-8a69-e137911ff30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new model and create a new optimizer\n",
    "\n",
    "fashion_mnist_model = get_model()\n",
    "rmsprop = tf.keras.optimizers.RMSprop(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9dbd8-7aec-478a-872d-42625c53cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise the custom training loop by compiling the training step into a graph\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = fashion_mnist_model(images)\n",
    "        batch_loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(batch_loss, fashion_mnist_model.trainable_weights)\n",
    "    return batch_loss, grads\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    logits = fashion_mnist_model(images)\n",
    "    batch_loss = loss_fn(labels, logits)\n",
    "    return batch_loss\n",
    "\n",
    "epochs = 5\n",
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for images, labels in train_dataset:\n",
    "        batch_loss, grads = train_step(images, labels)\n",
    "        rmsprop.apply_gradients(zip(grads, fashion_mnist_model.trainable_weights))\n",
    "    losses.append(batch_loss.numpy())\n",
    "\n",
    "    val_losses = []\n",
    "    for images, labels in val_dataset:\n",
    "        batch_loss = test_step(images, labels)\n",
    "        val_losses.append(batch_loss.numpy())\n",
    "    \n",
    "    print(f\"End of epoch {epoch}, training loss: {np.mean(losses):.4f}, validation loss: {np.mean(val_losses):.4f}\")\n",
    "print(f\"End of training, time: {time.perf_counter() - start:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74956be-c04e-4592-a6b5-7f255c5f14dc",
   "metadata": {},
   "source": [
    "Note that the Keras API automatically optimises the training loop whenever you call `model.fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cee8c1-9af7-4428-87c0-d20bec03ef41",
   "metadata": {},
   "source": [
    "In many cases the data processing pipeline can also be optimised for performance gain, see [here](https://www.tensorflow.org/guide/data_performance) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

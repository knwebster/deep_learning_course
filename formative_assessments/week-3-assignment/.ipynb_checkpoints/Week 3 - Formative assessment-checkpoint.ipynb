{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4415b31e",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 3: Loss functions and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04209c02",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to train an MLP model with both the high-level Keras API and a custom training loop, using the automatic differentiation tools from TensorFlow and PyTorch, and a custom loss function.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e208f",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"figures/adelie.jpg\" title=\"AdÃ©lie\" style=\"width: 275px;\"/> </td>\n",
    "<td> <img src=\"figures/chinstrap.jpg\" title=\"Chinstrap\" style=\"width: 275px;\"/> </td>\n",
    "    <td> <img src=\"figures/gentoo.jpg\" title=\"Gentoo\" style=\"width: 275px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<center><font style=\"font-size:12px\">source: <a href=https://en.wikipedia.org/wiki/Penguin>wikipedia</a></font></center>\n",
    "\n",
    "#### The Palmer Penguins dataset\n",
    "In this formative assessment, you will use the [Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html). These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the [Palmer Station Long Term Ecological Research Program](https://pal.lternet.edu/), part of the [US Long Term Ecological Research Network](https://lternet.edu/). The dataset consists of measurements for three penguin species observed in the Palmer Archipelago, Antarctica.\n",
    "\n",
    "* Gorman, K.B., Williams, T.D. & Fraser, W.R. (2014), \"Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis)\", PLoS ONE **9** (3):e90081, https://doi.org/10.1371/journal.pone.0090081\n",
    "\n",
    "Your goal is to model the dataset using an MLP network, trained using the automatic differentiation tools in TensorFlow & PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df944d5",
   "metadata": {},
   "source": [
    "#### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72588990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and sample the data\n",
    "\n",
    "df = pd.read_csv(Path(\"./data/penguins.csv\"))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdf591",
   "metadata": {},
   "source": [
    "We will work with the following columns from the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of columns to use as input features from the DataFrame\n",
    "\n",
    "input_cols = ['Body Mass (g)', 'Culmen Depth (mm)', 'Culmen Length (mm)', 'Flipper Length (mm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the column to use for the target variable\n",
    "\n",
    "target_col = ['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c6f93",
   "metadata": {},
   "source": [
    "We will also use the `MinMaxScaler` from `sklearn` to scale the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc26098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MinMaxScaler and LabelEncoder objects\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb8dbb",
   "metadata": {},
   "source": [
    "You should now complete the following `get_inputs_and_targets` function, according to the following specifications:\n",
    "\n",
    "* The function takes `dataframe`, `input_columns`, `target_column`, `minmaxscaler`, `labelencoder` as arguments\n",
    "* Extract the inputs and target columns from the loaded DataFrame using `input_columns` and `target_column` lists\n",
    "* Remove any rows with `NaN` values\n",
    "* Scale the input features to the range $[0, 1]$ using the `minmaxscaler`\n",
    "* Convert the target variable strings to integers using the `labelencoder` according to the following mapping:</br>\n",
    "  `{\"Adelie\": 0, \"Chinstrap\": 1, \"Gentoo\": 2}`\n",
    "* The function should then return a tuple of constant Tensor objects `(input_variables, target_variable)`\n",
    "  * `input_variables` should be of type `float32`, with shape `(num_examples, num_features)` \n",
    "  * `target_variable` should be of type `int64`, with shape `(num_examples,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_inputs_and_targets(dataframe, input_columns, target_column, minmaxscaler, labelencoder):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame and column lists as above, and a\n",
    "    MinMaxScaler object. The function should extract the input and target features as \n",
    "    above, and return a tuple (input_variables, target_variable) of Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430175a-87ad-4619-a76b-467d5b52cd08",
   "metadata": {},
   "source": [
    "_NB: We choose to convert the string targets to integers before converting to Tensors to maximise compatibility with the different backends. While TensorFlow Tensors support string types, PyTorch Tensors do not._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235025a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run your function to get the input and target Tensors\n",
    "\n",
    "X, y = get_inputs_and_targets(df, input_cols, target_col, scaler, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e622b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ops.convert_to_numpy(X), ops.convert_to_numpy(y), test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2dd4cb",
   "metadata": {},
   "source": [
    "You should now complete the following `get_datasets` function according to the following specifications:\n",
    "\n",
    "* The function takes the NumPy arrays `X_train`, `y_train`, `X_test` and `y_test` as arguments\n",
    "* The function should return a nested tuple `((train_ds, test_ds), (train_dl, test_dl))`, where `train_ds` and `test_ds` are training and test `tf.data.Dataset` objects, and `train_dl` and `test_dl` are training and test `torch.utils.data.DataLoader` objects\n",
    "* The training Datasets/DataLoaders should be shuffled\n",
    "* The Datasets/DataLoaders should be batched with a batch size of 32\n",
    "* The resulting Datasets/DataLoaders should all return a tuple of `(inputs, targets)` Tensors, of types `float32` and `int32` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7394be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_datasets(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function takes the dataset as numpy arrays, and creates tf.data.Dataset\n",
    "    and torch.utils.data.DataLoader objects according to the above description.\n",
    "    The function should then return the Dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c526825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test Datasets\n",
    "\n",
    "(train_ds, test_ds), (train_dl, test_dl) = get_datasets(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0ef8c",
   "metadata": {},
   "source": [
    "#### MLP model\n",
    "\n",
    "You should now complete the following `get_model` function to build the MLP model in Keras, which we will use to train on the Palmer Penguins dataset.\n",
    "\n",
    "* The function takes `hidden_units`, `output_units`, `input_shape`, `rate` as arguments\n",
    "* You should build the model using the `Sequential` API\n",
    "* `hidden_units` is a list of integers, specifying the width of the hidden layers within the model\n",
    "  * Each hidden layer should use a sigmoid activation function\n",
    "  * Each fully connected layer should be followed by a batch normalization layer, and then a dropout layer with dropout rate equal to `rate`\n",
    "* The first layer in the model should be an `Input` layer that sets the input shape using the `input_shape` argument\n",
    "* `output_units` is an integer specifying the number of neurons in the final output layer\n",
    "  * The final output layer should not use an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08142e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_model(hidden_units, output_units, input_shape, rate):\n",
    "    \"\"\"\n",
    "    This function should create an MLP model according to the above description.\n",
    "    The function should then return the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to create a model and print the summary\n",
    "\n",
    "model = get_model(hidden_units=[10, 10], output_units=3, input_shape=(4,), rate=0.8)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe24fbf",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "We will train the model using a categorical cross entropy loss function. Since the final layer in the model does not use an activation function, it is returning the logits to be used in the computation of the loss function.\n",
    "\n",
    "The categorical cross entropy for a single data example $(x, y)$ is given by:\n",
    "\n",
    "$$\n",
    "l(y, \\hat{y}) = -\\sum_{j=1}^C y_{j} \\log \\hat{y}_{j}, \\label{cce}\\tag{1}\n",
    "$$\n",
    "\n",
    "where $C$ is the number of classes (in our case $C=3$), $y, \\hat{y}\\in\\mathbb{R}^C$, and $\\hat{y}_{j}$ is equal to the probability of the label $j$ as predicted by our neural network $f_\\theta$ with parameters $\\theta$, given the input $x$. In the above formulation the target label $y$ is represented as a one-hot vector. In our case, $y$ will be length three with two zeros and a single 1 in the place of the correct label.\n",
    "\n",
    "Note also that the our model defined above outputs logits $z_j$, not probabilities. The probabilities are computed using the softmax function:\n",
    "\n",
    "$$\n",
    "\\hat{y_j} = \\frac{\\exp(z_j)}{\\sum_{k=1}^3 \\exp(z_k)}.\n",
    "$$\n",
    "\n",
    "The loss function we want to minimise is the categorical cross entropy \\eqref{cce} averaged over all examples in the training data. In practice, we will estimate this loss function by sampling minibatches of data and computing the average categorical cross entropy over the minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74634f",
   "metadata": {},
   "source": [
    "You should now complete the following `loss_function` function, to compute the categorical cross entropy loss as above.\n",
    "\n",
    "In Keras, loss functions have the signature `loss(y_true, y_pred)`, where `y_true` is the ground truth Tensor and `y_pred` is the model prediction given the inputs. The `compute_loss` function follows this signature, so we would be able to pass it to the `loss` argument directly when calling `model.compile`.\n",
    "\n",
    "* The function takes `y_true` and `y_pred` as arguments\n",
    "  * `y_true` is a batch of ground truth inputs, of shape `(num_examples,)` and type `int32`\n",
    "  * `y_pred` is a batch of model predictions, of shape `(num_examples, 3)` and type `float32`\n",
    "* The function should compute the categorical cross entropy as above\n",
    "  * Bear in mind that `y_pred` will be a batch of logits, not probabilities\n",
    "  * `y_true` contains the integer-encoded labels (either 0, 1 or 2)\n",
    "* The function should average the categorical cross entropy over the minibatch, and return the result as a scalar Tensor\n",
    "\n",
    "_Hint: you might find the functions [`keras.ops.logsumexp`](https://keras.io/api/ops/core/#logsumexp-function) and [`keras.ops.take_along_axis`](https://keras.io/api/ops/numpy/#takealongaxis-function) useful._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function should compute the categorical cross entropy loss as described above.\n",
    "    The function should return a scalar Tensor with the computed loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a ground truth and predictions Tensor to test your function\n",
    "\n",
    "inputs, y_true = next(iter(train_ds))  # or use train_dl\n",
    "y_pred = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d34d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss on the batch of data using your function\n",
    "\n",
    "loss_function(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see that your computed loss agrees with the built-in Keras function\n",
    "\n",
    "ops.mean(keras.metrics.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4d44a",
   "metadata": {},
   "source": [
    "#### Train your model with the high-level Keras API\n",
    "\n",
    "You should now complete the following `train_model_keras` function to train the MLP model using the high-level Keras API.\n",
    "\n",
    "* The function takes `mlp_model`, `loss_fn`, `opt`, `training_dataset` and `epochs` as arguments\n",
    "* The function should use the high-level Keras API to compile and train the model\n",
    "  * Use the `compile` method to compile `mlp_model` using the loss function `loss_fn`, `opt` optimizer and accuracy metric\n",
    "  * Train with the `fit` method, using `training_dataset` for `epochs` epochs\n",
    "* The function should then return the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def train_model_keras(mlp_model, loss_fn, opt, training_dataset, epochs):\n",
    "    \"\"\"\n",
    "    This function should use the compile and fit methods to train the MLP model.\n",
    "    The function should return the history from the training.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00818ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SGD optimizer\n",
    "\n",
    "optimizer = keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4761d15-823d-4e7d-9623-c5a1dba7afc0",
   "metadata": {},
   "source": [
    "The Keras model can be trained with either the TensorFlow Dataset `train_ds` or the PyTorch DataLoader `train_dl`, regardless of which backend is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84685aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the MLP model\n",
    "\n",
    "model = get_model(hidden_units=[40, 20], output_units=3, input_shape=(4,), rate=0.5)\n",
    "history = train_model_keras(model, loss_fn=loss_function, opt=optimizer, epochs=200,\n",
    "                            training_dataset=train_dl)  # or pass training_dataset=train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be267cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c4b9d",
   "metadata": {},
   "source": [
    "#### Train your model with a custom training loop\n",
    "\n",
    "You will now implement a custom training loop to train an MLP model on the Palmer Penguins dataset, making use of the automatic differentiation tools in TensorFlow and PyTorch. See the examples [here](https://keras.io/guides/writing_a_custom_training_loop_in_tensorflow/) and [here](https://keras.io/guides/writing_a_custom_training_loop_in_torch/). Writing a custom training loop is one of the few cases where the implementation needs to be backend dependent. We will practice implementing this training loop in both PyTorch and TensorFlow.\n",
    "\n",
    "First you should complete the following `tf_train_step` and `pt_train_step` functions, for TensorFlow and PyTorch respectively. These will implement the core operations of computing the loss and gradients.\n",
    "\n",
    "* The functions both take the arguments `mlp_model`, `loss_fn` (a Keras loss function), `opt` (a Keras optimizer) and `train_batch`\n",
    "* `train_batch` is a tuple of `(inputs, targets)` Tensors yielded from the Dataset\n",
    "* The functions should compute the batch loss using `train_batch`, `mlp_model` and `loss_fn`\n",
    "  * The model should be run in training mode (see [these examples](https://www.tensorflow.org/api_docs/python/tf/keras/Model#call))\n",
    "* They should then compute the gradients `grads` of the loss function with respect to the `mlp_model`'s trainable parameters\n",
    "  * The trainable parameters can be accessed using the `trainable_variables` property\n",
    "* The code to perform these computations will depend on the chosen backend\n",
    "* The functions should return a tuple of four Tensors: `(loss, grads, y_true, y_pred)`\n",
    "  * `loss` is the scalar batch loss as computed by `loss_fn`\n",
    "  * `grads` is the list of Tensor gradients\n",
    "  * `y_true` is the ground truth Tensor for the batch\n",
    "  * `y_pred` is the model predictions Tensor for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following functions. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "@tf.function\n",
    "def tf_train_step(mlp_model, loss_fn, opt, train_batch):\n",
    "    \"\"\"\n",
    "    This function should perform the update step as described above.\n",
    "    The function should return a tuple of Tensors (loss, y_true, y_pred).\n",
    "    This function will only run with the TensorFlow backend.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def pt_train_step(mlp_model, loss_fn, opt, train_batch):\n",
    "    \"\"\"\n",
    "    This function should perform the update step as described above.\n",
    "    The function should return a tuple of Tensors (loss, y_true, y_pred).\n",
    "    This function will only run with the PyTorch backend.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f02162",
   "metadata": {},
   "source": [
    "You should now complete the following `train_model_custom` function to perform the custom training loop. We will use two Keras metric objects (defined below) to record the loss and accuracy values over the course of training. See the docs for the base [`Metric`](https://keras.io/api/metrics/base_metric/#metric-class) class to see the general usage.\n",
    "\n",
    "* The function takes `mlp_model`, `loss_fn`, `opt`, `training_dataset`, `train_step_fn`, `epochs`, `loss_metric` and `accuracy_metric` as arguments\n",
    "* The custom training loop should consist of an outer loop for the epochs, that runs for `epochs` number of times\n",
    "* At the start of each epoch, the metric states should be reset using the `reset_state` method\n",
    "* Within each epoch, the function should loop over `training_dataset` to pull batches of data\n",
    "* For each batch, it should use `train_step_fn` to calculate the loss and gradients\n",
    "  * This function returns a tuple of Tensors `(loss, grads, y_true, y_pred)`\n",
    "  * The optimizer `opt` should be used to update the model parameters using the list of gradients `grads`\n",
    "    * There are two ways to perform gradient updates with Keras optimizers:\n",
    "      * `opt.apply(grads, variables)`, where `grads` and `variables` are matching lists of gradient Tensors and Keras Variables\n",
    "      * `opt.apply_gradients(grads_and_vars)`, where `grads_and_vars` is an iterator, commonly constructed as `zip(grads, variables)`, where `grads` and `variables` are as above\n",
    "  * For each batch, the metrics should also be updated, using the `update_state` method\n",
    "* The average loss and accuracy over each epoch should each be stored in a list of floats\n",
    "  * The average loss and accuracy can be retrieved from the metrics at the end of the epoch using the `result` method\n",
    "* The function should return a tuple of the two lists `(epoch_losses, epoch_acc)` for average loss and accuracy scores per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388df053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss and accuracy metrics and optimizer\n",
    "\n",
    "loss_metric = keras.metrics.Mean()\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a722382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def train_model_custom(mlp_model, loss_fn, opt, training_dataset, train_step_fn,  epochs, \n",
    "                       loss_metric=loss_metric, accuracy_metric=accuracy_metric):\n",
    "    \"\"\"\n",
    "    This function should run the custom training loop as described above.\n",
    "    The function should return a tuple of two lists with the loss and accuracy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to run the custom training loop\n",
    "\n",
    "if keras.config.backend() == 'tensorflow':\n",
    "    print(\"Using TensorFlow train_step\")\n",
    "    train_step_fn = tf_train_step\n",
    "else:\n",
    "    assert keras.config.backend() == 'torch'\n",
    "    print(\"Using PyTorch train_step\")\n",
    "    train_step_fn = pt_train_step\n",
    "\n",
    "model = get_model(hidden_units=[40, 20], output_units=3, input_shape=(4,), rate=0.5)\n",
    "epoch_losses, epoch_acc = train_model_custom(model, loss_fn=loss_function, opt=optimizer,\n",
    "                                             training_dataset=train_dl,  # or pass train_ds\n",
    "                                             train_step_fn=train_step_fn, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_acc)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a90f10-cb4e-4599-b8a8-0052feba5049",
   "metadata": {},
   "source": [
    "#### Evaluate your model\n",
    "\n",
    "Finally, you will also implement custom code to evaluate your model. First you should complete the following `test_step` function. This will implement the evaluation of the model on a batch of data by computing the loss. This function does not perform gradient updates, and so will be backend independent.\n",
    "\n",
    "* The functions both take the arguments `mlp_model`, `loss_fn` (a Keras loss function), and `test_batch`\n",
    "* `test_batch` is a tuple of `(inputs, targets)` Tensors yielded from the Dataset\n",
    "* The functions should compute the batch loss using `test_batch`, `mlp_model` and `loss_fn`\n",
    "  * The model should be run in inference mode (see [these examples](https://www.tensorflow.org/api_docs/python/tf/keras/Model#call))\n",
    "* The functions should return a tuple of three Tensors: `(loss, y_true, y_pred)`\n",
    "  * `loss` is the scalar batch loss as computed by `loss_fn`\n",
    "  * `y_true` is the ground truth Tensor for the batch\n",
    "  * `y_pred` is the model predictions Tensor for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ca3e5-21e9-4a3f-bb90-d90d2321c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following functions. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def test_step(mlp_model, loss_fn, test_batch):\n",
    "    \"\"\"\n",
    "    This function should perform the evaluation step as described above.\n",
    "    The function should return a tuple of Tensors (loss, y_true, y_pred).\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da42c8f-5a83-42dd-8d27-aafad9a92bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the test_step function for TensorFlow\n",
    "\n",
    "tf_test_step = tf.function(test_step)\n",
    "pt_test_step = test_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a76f5",
   "metadata": {},
   "source": [
    "Now you should complete the following `test_model_custom` function that will evaluate the model on a test dataset. This will be similar to the `train_model_custom` function, except that no optimizer is used/needed and no parameter updates are made.\n",
    "\n",
    "* The function takes `mlp_model`, `loss_fn`, `test_dataset`, `test_step_fn`, `loss_metric` and `accuracy_metric` as arguments\n",
    "* The evaluation should make one complete iteration loop through `test_dataset`\n",
    "* At the start of the loop, the metric states should be reset using the `reset_state` method\n",
    "* For each batch, you should use `test_step_fn` to compute the loss and model prediction\n",
    "  * This function returns a tuple of Tensors `(loss, y_true, y_pred)`\n",
    "  * For each batch, the metrics should also be updated, using the `update_state` method\n",
    "* The average loss and accuracy should be retrieved from the metrics at the end of the loop using the `result` method\n",
    "* The function should return a tuple of two floats `(avg_loss, avg_acc)` for average loss and accuracy scores over the `test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7544a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def test_model_custom(mlp_model, loss_fn, test_dataset, test_step_fn, \n",
    "                      loss_metric=loss_metric, accuracy_metric=accuracy_metric):\n",
    "    \"\"\"\n",
    "    This function should run the custom evaluation loop as described above.\n",
    "    The function should return a tuple of two floats for the loss and accuracy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to evaluate the model\n",
    "\n",
    "if keras.config.backend() == 'tensorflow':\n",
    "    print(\"Using TensorFlow test_step\")\n",
    "    test_step_fn = tf_test_step\n",
    "else:\n",
    "    assert keras.config.backend() == 'torch'\n",
    "    print(\"Using PyTorch test_step\")\n",
    "    test_step_fn = pt_test_step\n",
    "    \n",
    "avg_loss, avg_acc = test_model_custom(model, loss_function, test_ds, test_step_fn)  # Could also use test_dl\n",
    "print(f\"Test loss: {avg_loss:.4f}\")\n",
    "print(f\"Test accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9ac0c",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now written custom code to implement a loss function, training loop and evaluation loop for an MLP model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

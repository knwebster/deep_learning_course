{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 1: Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement a linear regression model in Keras. You will implement the analytic solution, as well as a low-level training loop to update parameters using gradient descent.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/life_expectancy_wikipedia.png\" title=\"Life expectancy\" style=\"width: 450px;\"/></center>\n",
    "<center><font style=\"font-size:12px\">source: <a href=https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy>wikipedia</a></font></center>\n",
    "\n",
    "#### The WHO Life Expectancy dataset\n",
    "In this formative assessment, you will use the [WHO Life Expectancy dataset](https://www.kaggle.com/kumarajarshi/life-expectancy-who) from Kaggle. This dataset was collected from the Global Health Observatory (GHO) data repository under the World Health Organization (WHO), for the purpose of health data analysis. The dataset includes multiple factors affecting life expectancy across 133 countries, divided into the broad categories of immunization related factors, mortality factors, economical factors and social factors.\n",
    "\n",
    "Your goal is to use Keras to model the dataset using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to load and describe the data\n",
    "\n",
    "df = pd.read_csv(Path(\"./data/Life Expectancy Data.csv\"))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work the following columns from the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of columns to use from the DataFrame\n",
    "\n",
    "cols = ['Life expectancy ', 'Adult Mortality', 'Alcohol', ' BMI ',\n",
    "        'Polio', 'Total expenditure', 'Diphtheria ', ' HIV/AIDS', \n",
    "        'GDP', 'Income composition of resources', 'Schooling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now complete the following function, according to the following specifications:\n",
    "\n",
    "* Extract the columns above from the loaded DataFrame\n",
    "* Remove any rows with `NaN` values\n",
    "* Define a 1-D numpy array using the values in the `Life expectancy ` column. This will be the target variable\n",
    "* Define a 2-D numpy array using the values from all remaining columns. This array should have shape `(num_examples, num_features)`. These will be the input variables\n",
    "* The function should then return the tuple of Tensor objects `(input_variables, target_variable)` of type `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_inputs_and_targets(dataframe, columns):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame and column list as above, and extracts the\n",
    "    numpy arrays as described above.\n",
    "    Your function should return a tuple (input_variables, target_variable) of Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the input and target Tensors\n",
    "\n",
    "X, y = get_inputs_and_targets(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets and standardise the input scales\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(keras.ops.convert_to_numpy(X), \n",
    "                                                    keras.ops.convert_to_numpy(y), \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train = keras.ops.convert_to_tensor(X_train), keras.ops.convert_to_tensor(y_train)\n",
    "X_test, y_test = keras.ops.convert_to_tensor(X_test), keras.ops.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model\n",
    "\n",
    "We will fit a simple model of the form\n",
    "\n",
    "$$\n",
    "y = f_\\theta(\\mathbf{x}) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $y\\in\\mathbb{R}$ is the target variable, $\\mathbf{x}\\in\\mathbb{R}^{10}$ are the input features, $\\theta\\in\\mathbb{R}^{11}$ are the model parameters, $\\epsilon\\sim\\mathcal{N}(0, 1)$ is the observation noise random variable, and $f_\\theta:\\mathbb{R}^{10}\\mapsto\\mathbb{R}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\theta(\\mathbf{x}) &= \\theta_0 + \\sum_{m=1}^{10} \\theta_m x_m\\\\\n",
    "&= \\sum_{m=0}^{10} \\theta_m x_m.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the second line above we have defined $x_0=1$ to be the constant feature. The maximum likelihood solution is given by the normal equation\n",
    "\n",
    "$$\n",
    "\\theta_{ML} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}\\in\\mathbb{R}^{N\\times M}$ is the data matrix, $\\mathbf{y}\\in\\mathbb{R}^N$ are the targets, $N$ is the number of data examples, and $M$ are the number of features (including the constant feature).\n",
    "\n",
    "You should now complete the following function to implement the normal equation to compute the maximum likelhood solution. Your code should only use Keras functions. \n",
    "\n",
    "* The arguments to the function are an `inputs` Tensor of shape `(num_examples, num_features)`, and a `targets` Tensor of shape `(num_examples,)`\n",
    "* The function should add a column of ones as the first column to the `inputs` Tensor for the constant feature\n",
    "* The function should output a 1-D Tensor of parameters of length `(num_features + 1,)` (the first entry will be the bias)\n",
    "\n",
    "_Hint: check [the docs](https://keras.io/api/) for relevant Keras functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def normal_equation(inputs, targets):\n",
    "    \"\"\"\n",
    "    This function takes in inputs and targets Tensors, and implements the normal equation\n",
    "    as above, only using Keras functions.\n",
    "    Your function should return a Tensor for the maximum likelihood solution for the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to compute the ML estimate\n",
    "\n",
    "theta_ml = normal_equation(X_train, y_train)\n",
    "bias_ml, weights_ml = theta_ml[0], theta_ml[1:]\n",
    "print(\"MLE weights:\")\n",
    "print(weights_ml)\n",
    "print(\"MLE bias:\")\n",
    "print(bias_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "\n",
    "You will now implement the (batch) gradient descent algorithm to find the MLE using optimization. \n",
    "\n",
    "First, you should complete the following `get_variables` function to create Keras Variable objects for the weights and bias of the linear regression model.\n",
    "\n",
    "* The function takes `num_features` as an argument\n",
    "* The bias should be a Keras Variable with scalar shape, float32 type, and an initial value of zero\n",
    "* The weights Variable should be a 1-D Tensor of length `num_features`, float32 type, and with initial values sampled from a standard normal distribution\n",
    "* The function should return the tuple of Variables `(weights, bias)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_variables(num_features):\n",
    "    \"\"\"\n",
    "    This function takes in the number of features as an argument, and creates Tensors\n",
    "    for the linear regression model weights and bias, as well as an iteration counter Variable.\n",
    "    Your function should return a tuple of two Tensor objects (weights, bias).\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to create the Variables\n",
    "\n",
    "weights, bias = get_variables(num_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model itself by completing the following function. This function implements $f_\\theta(\\mathbf{x}) = \\theta_0 + \\sum_{m=1}^{10} \\theta_m x_m$ as above.\n",
    "\n",
    "* The function takes an `inputs` Tensor, `weights` and `bias` Variables as input\n",
    "* The `inputs` Tensor could be a batch of inputs of shape `(batch_size, num_features)`, or a single set of inputs of shape `(num_features,)`\n",
    "* The function should return the output Tensor $f_\\theta(\\mathbf{x})$\n",
    "* The output Tensor should have shape `(batch_size,)` (if passed a batch of inputs), or else should be a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def f(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    This function takes in an inputs Tensor, weights and bias Variables. It should compute and \n",
    "    return the output Tensor prediction. \n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on some dummy inputs\n",
    "\n",
    "inputs = keras.random.normal((3, 10), dtype='float32')\n",
    "print(f(inputs, weights, bias))\n",
    "\n",
    "inputs = keras.random.normal((10,), dtype='float32')\n",
    "print(f(inputs, weights, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to define the loss function to optimise. As we have assumed Gaussian noise $\\epsilon\\sim\\mathcal{N}(0, 1)$ and we are looking to find the maximum likelihood solution, this will be the mean squared error loss. The loss function that you should implement is therefore:\n",
    "\n",
    "$$\n",
    "{L}_{MSE}(\\theta) = \\frac{1}{M} \\sum_{\\mathbf{x}_i, y_i\\in\\mathcal{D}} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_i = f_\\theta(\\mathbf{x}_i)$, and $(\\mathbf{x}_i, y_i)$ is an example input and output from the training dataset $\\mathcal{D}$. The function specifications are as follows:\n",
    "\n",
    "* The `mse` function takes two Tensors as arguments: `y_true` and `y_pred`\n",
    "* These two Tensors will have shape `(num_examples,)`\n",
    "* The loss function should compute and return the mean squared error loss (MSE) as a scalar Tensor\n",
    "* Use only Keras functions inside your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function takes a batch of 'ground truth' values y_true and a corresponding batch\n",
    "    of model predictions y_pred, and computes the mean squared error.\n",
    "    Your function should return the MSE as a scalar Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the initial loss on the training data\n",
    "\n",
    "mse(y_train, f(X_train, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the train and test loss of the MLE\n",
    "\n",
    "print(f\"MLE train loss: {mse(y_train, f(X_train, weights_ml, bias_ml)):.4f}\")\n",
    "print(f\"MLE test loss: {mse(y_test, f(X_test, weights_ml, bias_ml)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements the update step of gradient descent, that we will use inside the training loop. Recall this update uses the gradient of the loss with respect to the model parameters to make the update:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_\\theta {L}_{MSE}(\\theta_t),\\qquad t\\in\\mathbb{N}_0,\n",
    "$$\n",
    "\n",
    "where $\\eta>0$ is the learning rate.\n",
    "\n",
    "* The `gradient_descent_update` function takes the following arguments:\n",
    "  * `model_fn` is the function that defines the predictive function (the function `f` above)\n",
    "  * `inputs` and `targets` are the inputs and targets Tensors, of shape `(num_examples, 10)` and `(num_examples,)` respectively\n",
    "  * `w` and `b` are the Variables that represent the model parameters\n",
    "  * The `learning_rate` is the gradient descent hyperparameter\n",
    "* The function should compute the gradient descent update step (assuming the mean squared error loss as above), updating the `w` and `b` Variables accordingly, using the `learning_rate` passed in\n",
    "* The function should not return anything. The weights and bias are updated in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def gradient_descent_update(model_fn, inputs, targets, w, b, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This function takes the model function, inputs batch, targets batch, weights Tensor,\n",
    "    bias Tensor and learning rate as arguments. It should update the Tensors w and b\n",
    "    using the SGD update rule above for the MSE loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your gradient descent update function\n",
    "\n",
    "print(\"Before the update:\")\n",
    "print(weights.value)\n",
    "print(bias.value)\n",
    "gradient_descent_update(f, X_train, y_train, weights, bias, learning_rate=0.05)\n",
    "print(\"\\nAfter the update:\")\n",
    "print(weights.value)\n",
    "print(bias.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to write the training loop in the following function. The training loop consists of a pre-defined number of iterations, where one iteration consists of the gradient descent update step which updates the weights and biases according to the gradient descent update rule. \n",
    "\n",
    "You should complete the following `training_loop` function according to the specifications:\n",
    "\n",
    "* The function takes the following arguments:\n",
    "  * `num_iterations`: a positive integer that defines the number of iterations to run the training loop\n",
    "  * `model_fn`: as before, the function that defines the predictive function\n",
    "  * `training_data`: a 2-tuple of Tensors `(inputs, targets)` for the complete training data\n",
    "  * `w`: the Variable that represents the model weights\n",
    "  * `b`: the Variable that represents the model bias\n",
    "  * `mse`: the loss function to evaluate the model (this will be your `mse` function above)\n",
    "  * `gradient_descent_update`: the function that implements the gradient descent update (this will be your `gradient_descent_update` function above)\n",
    "  * `learning_rate`: the learning rate for the gradient descent update\n",
    "* The function should perform the gradient descent update `num_iterations` times\n",
    "  * For each iteration, the parameters `w` and `b` should be updated according to `gradient_descent_update`, using the `learning_rate` provided\n",
    "* After every update, the model loss should be evaluated on the training data using the `mse` function and appended to a list as a scalar float\n",
    "* The list of losses `losses` should then be returned by the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def training_loop(num_iterations, model_fn, training_data, w, b, \n",
    "                  mse=mse, gradient_descent_update=gradient_descent_update, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This function executes the training loop according to the specifications above. \n",
    "    It should run for num_iterations, updating the model parameters using the \n",
    "    gradient_descent_update function at every iteration.\n",
    "    The function should return the list of losses computed at each iteration, \n",
    "    using the mse function.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise the model parameters and run the training loop\n",
    "\n",
    "weights, bias = get_variables(num_features=10)\n",
    "losses = training_loop(1000, f, (X_train, y_train), weights, bias, \n",
    "                       mse=mse, gradient_descent_update=gradient_descent_update, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the train and test loss of the learned weights\n",
    "\n",
    "print(f\"Model train loss: {mse(y_train, f(X_train, weights, bias)):.4f}\")\n",
    "print(f\"Model test loss: {mse(y_test, f(X_test, weights, bias)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your learned weights and bias to the exact solution computed earlier. They should be fairly close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the learned weights and bias\n",
    "\n",
    "print(\"Learned weights:\")\n",
    "print(keras.ops.convert_to_numpy(weights))\n",
    "print(\"Learned bias:\")\n",
    "print(keras.ops.convert_to_numpy(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the exact weights and bias\n",
    "\n",
    "print(\"Exact ML weights:\")\n",
    "print(keras.ops.convert_to_numpy(weights_ml))\n",
    "print(\"Exact ML bias:\")\n",
    "print(keras.ops.convert_to_numpy(bias_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now implemented the exact linear regression solution in Keras as well as the gradient descent algorithm for approximating the maximum likelihood parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de6e5c5",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 11: Bayesian neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d45565",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement and train a Bayesian neural network model.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd163c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell to import all required packages. \n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416166e",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/mnist_mnist_c.png\" title=\"MNIST & MNIST-C\" style=\"width: 650px;\"/></center>\n",
    "  \n",
    "#### The MNIST and MNIST-C dataset\n",
    "\n",
    "In this assignment, you will use the [MNIST](http://yann.lecun.com/exdb/mnist) and [MNIST-C](https://github.com/google-research/mnist-c) (MNIST-Corrupted) datasets. The MNIST-C dataset is a corruption benchmark dataset for out-of-distribution evaluation, where handwritten digits from the MNIST dataset are corrupted with various types of noise. Both datasets contain 60,000 examples for training and 10,000 examples for testing.\n",
    "\n",
    "* LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. (1998), \"Gradient-based learning applied to document recognition.\" _Proceedings of the IEEE_, **86** (11), 2278-2324.\n",
    "* LeCun, Y., Cortes, C. & Burges, C.J. (2010), \"MNIST handwritten digit database\", ATT Labs [Online](http://yann.lecun.com/exdb/mnist), **2**, 2010.\n",
    "* Mu, N. & Gilmer, J. (2019), \"MNIST-C: A Robustness Benchmark for Computer Vision\", arXiv preprint, abs/1906.02337.\n",
    "\n",
    "Your goal is to build and train a Bayesian neural network on the MNIST dataset, and test the robustness of the model on the MNIST-C dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb7f4b",
   "metadata": {},
   "source": [
    "#### Load and prepare the data\n",
    "For this assignment, you will load the MNIST and MNIST-C datasets from the TensorFlow Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52021a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the MNIST data and print the element_spec\n",
    "\n",
    "mnist_train_data = tfds.load(\"mnist_corrupted\", split=\"train\", \n",
    "                             read_config=tfds.ReadConfig(try_autocache=False))\n",
    "mnist_test_data = tfds.load(\"mnist_corrupted\", split=\"test\", \n",
    "                            read_config=tfds.ReadConfig(try_autocache=False))\n",
    "\n",
    "mnist_train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06966124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some samples\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 3))\n",
    "for i, example in enumerate(mnist_train_data.shuffle(200).take(6)):\n",
    "    axes[i].imshow(ops.convert_to_numpy(example['image']), cmap='gray_r')\n",
    "    axes[i].set_axis_off()\n",
    "    axes[i].set_title(f'Label: {ops.convert_to_numpy(example[\"label\"])}', fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the MNIST-C data and print the element_spec\n",
    "\n",
    "mnist_c_train_data = tfds.load(\"mnist_corrupted\", split=\"train\", builder_kwargs={\"config\": 'spatter'}, \n",
    "                               read_config=tfds.ReadConfig(try_autocache=False))\n",
    "mnist_c_test_data = tfds.load(\"mnist_corrupted\", split=\"test\", builder_kwargs={\"config\": 'spatter'},\n",
    "                              read_config=tfds.ReadConfig(try_autocache=False))\n",
    "\n",
    "mnist_c_train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5930d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some samples\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 3))\n",
    "for i, example in enumerate(mnist_c_train_data.shuffle(200).take(6)):\n",
    "    axes[i].imshow(ops.convert_to_numpy(example['image']), cmap='gray_r')\n",
    "    axes[i].set_axis_off()\n",
    "    axes[i].set_title(f'Label: {ops.convert_to_numpy(example[\"label\"])}', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3351a5",
   "metadata": {},
   "source": [
    "This version of the MNIST-C dataset adds 'spatters' to the images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01232714",
   "metadata": {},
   "source": [
    "First, you should write a `process_dataset` function to preprocess the data ready for training. \n",
    "\n",
    "* The function takes the arguments `dataset`, `batch_size`, and `shuffle_buffer`\n",
    "  * `dataset` is a `tf.data.Dataset` object as loaded above\n",
    "  * `batch_size` is a positive integer\n",
    "  * `shuffle_buffer` is a positive integer, or `None`\n",
    "* The `dataset` should be processed as follows:\n",
    "  * It should return tuples of Tensors `(image, label)`\n",
    "  * The image values should be scaled to the range $[0, 1]$, with dtype `tf.float32`\n",
    "  * The labels should be convert to one-hot vectors, with dtype `tf.float32`\n",
    "* If `shuffle_buffer` is not `None`, `dataset` should be shuffled with buffer size equal to `shuffle_buffer`\n",
    "* `dataset` should be batched using `batch_size`\n",
    "* Your function should end with a call to `prefetch` (using the argument `tf.data.AUTOTUNE`) and return the processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def process_dataset(dataset, batch_size, shuffle_buffer=None):\n",
    "    \"\"\"\n",
    "    This function takes a tf.data.Dataset, shuffle_buffer and batch_size arguments. \n",
    "    It should preprocess and return the Dataset as specified above.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to process the Datasets\n",
    "\n",
    "mnist_train_data = process_dataset(mnist_train_data, 128, shuffle_buffer=500)\n",
    "mnist_test_data = process_dataset(mnist_test_data, 128)\n",
    "mnist_c_train_data = process_dataset(mnist_c_train_data, 128, shuffle_buffer=500)\n",
    "mnist_c_test_data = process_dataset(mnist_c_test_data, 128)\n",
    "\n",
    "print(mnist_train_data.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc2c4d",
   "metadata": {},
   "source": [
    "#### Build and train the Bayesian MLP model\n",
    "\n",
    "We will now build and train a Bayesian MLP on the MNIST dataset. This model will learn distributions over the model weights, and will be able to quantify its own uncertainty on OOD (out-of-distribution) data.\n",
    "\n",
    "First you should complete the following `DenseVariational` custom layer. The layer is similar to the one from the lecture notes. It also should use an independent standard Normal distribution $N(0, 1)$ for each weight and bias parameter in the prior, and diagonal Gaussians with learnable means and variances for the posterior. \n",
    "\n",
    "* The initializer takes `units` as a required argument, and `activation`, `kl_weight`, `num_kl_mc_samples` as optional arguments\n",
    "* The initializer and `build` method are completed for you\n",
    "* You should complete the `call` method. The outputs computation has been completed, but you should add the KL loss term using the `add_loss` method\n",
    "  * The KL loss should be computed using the first form of the SGVB estimator. That is, the KL term should be approximated as\n",
    "$$\\frac{1}{K}\\sum_{j=1}^K  \\left[ \\log q_\\phi(\\theta^{(j)}) - \\log p(\\theta^{(j)}) \\right],$$\n",
    "where $\\theta^{(j)} = g_\\phi(\\epsilon^{(j)}),~\\epsilon^{(j)}\\sim p(\\epsilon)$ and $g_\\phi$ is the scale-and-shift transformation using the mean and standard deviation Variables for the kernel and mean parameters.\n",
    "  * $K$ in the above expression is defined by `num_kl_mc_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21545506-bc3c-4f8b-bade-f1fbd218653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Activation\n",
    "\n",
    "class DenseVariational(Layer):\n",
    "\n",
    "    def __init__(self, units, activation=None, kl_weight=None, num_kl_mc_samples=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kl_weight = kl_weight\n",
    "        self.num_kl_mc_samples = num_kl_mc_samples\n",
    "        self.pi = ops.array(np.pi)\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        in_units = inputs_shape[-1]\n",
    "        self.kernel_mean = self.add_weight(\n",
    "            name='kernel_mean',\n",
    "            shape=(in_units, self.units),\n",
    "            initializer='glorot_uniform'\n",
    "        )\n",
    "        self.kernel_logstd = self.add_weight(\n",
    "            name='kernel_logstd',\n",
    "            shape=(in_units, self.units),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        self.bias_mean = self.add_weight(\n",
    "            name='bias_mean',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        self.bias_logstd = self.add_weight(\n",
    "            name='bias_logstd',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        self.activation_fn = Activation(self.activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Add the KL loss using the add_loss method, using the kl_weight and num_kl_mc_samples attributes.\n",
    "        \"\"\"\n",
    "        kernel = self.kernel_mean + (keras.random.normal(self.kernel_mean.shape) * ops.exp(self.kernel_logstd))\n",
    "        bias = self.bias_mean + (keras.random.normal(self.bias_mean.shape) * ops.exp(self.bias_logstd))\n",
    "        outputs = self.activation_fn(inputs @ kernel + bias)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50badfb2",
   "metadata": {},
   "source": [
    "You should now define the BNN model with the following `get_bayesian_model` function.\n",
    "\n",
    "* The function takes the arguments `input_shape`, `hidden_units`, `kl_weight` and `num_kl_mc_samples`\n",
    "  * `input_shape` is a tuple of integers\n",
    "  * `hidden_units` is a list of integers, for the width of the Dense layers\n",
    "  * `kl_weight` is a float to use to weight the KL-term in the objective\n",
    "  * `num_kl_mc_samples` is an integer to define the number of MC samples to approximate the KL loss term\n",
    "* The first layer of the model should use the `input_shape` argument\n",
    "* The model should first flatten the input to a batch of 1-D Tensors\n",
    "* The `hidden_units` argument is a list of integers (of any length), containing the number of units to use in subsequent `DenseVariational` layers\n",
    "  * Each of these `DenseVariational` layers should use a ReLU activation\n",
    "  * The KL-divergence term should be weighted using `kl_weight`\n",
    "  * The KL loss should be evaluated using `num_kl_mc_samples`\n",
    "* There should then be one more `DenseVariational` layer with 10 units that output the logits of a categorical distribution with 10 categories\n",
    "  * This `DenseVariational` layer should also use `kl_weight` and `num_kl_mc_samples` as above\n",
    "* The function should then return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_bayesian_model(input_shape, hidden_units, kl_weight, num_kl_mc_samples):\n",
    "    \"\"\"\n",
    "    This function should define the BNN model as described above. \n",
    "    Your function should return this model.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to define the model\n",
    "\n",
    "model = get_bayesian_model(input_shape=(28, 28, 1), hidden_units=[200, 100], kl_weight=1/60000, num_kl_mc_samples=5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98251fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the negative log-likelihood loss\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=1e-2), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a4f31",
   "metadata": {},
   "source": [
    "The Bayes by Backprop algorithm can take a while to converge. We will use a learning rate schedule that uses the initial learning rate for 50 epochs, and then multiplies the learning rate by 0.1 every 25 epochs. We can use a [`LearningRateScheduler`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) callback to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2272fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 50:\n",
    "        return lr\n",
    "    elif (epoch % 25 == 0):\n",
    "        return lr * 0.1\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "learning_rate_scheduler = keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5355faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model on the MNIST data\n",
    "\n",
    "history = model.fit(mnist_train_data, epochs=100, callbacks=[learning_rate_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55816d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylim(0, 5)\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the MNIST test data\n",
    "\n",
    "mnist_results = model.evaluate(mnist_test_data, return_dict=True, verbose=False)\n",
    "print(f\"MNIST test loss: {mnist_results['loss']:.4f}, MNIST test accuracy: {mnist_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991222f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the MNIST-C test data\n",
    "\n",
    "mnist_c_results = model.evaluate(mnist_c_test_data, return_dict=True, verbose=False)\n",
    "print(f\"MNIST-C test loss: {mnist_c_results['loss']:.4f}, MNIST-C test accuracy: {mnist_c_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ff492",
   "metadata": {},
   "source": [
    "#### Predictive distribution\n",
    "\n",
    "We can use our posterior approximation $q_\\phi(\\theta)$ to approximate the predictive distribution, by marginalising out the parameters of the model. We can estimate this by drawing $K$ samples from our variational distribution $q_\\phi$, and computing the Monte Carlo estimate\n",
    "\n",
    "$$\n",
    "p(y^* \\mid x^*, \\mathcal{D}) \\approx \\frac{1}{K} \\sum_{k=1}^K p(y^* \\mid x^*,\\theta_k),\\quad \\theta_k \\sim q_\\phi(\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05a949",
   "metadata": {},
   "source": [
    "You should now complete the following `predictive_distribution` function to approximate the predictive distribution for a given batch of inputs.\n",
    "\n",
    "* The function takes the arguments `bayesian_model`, `inputs` and `num_samples`\n",
    "  * `bayesian_model` will be the trained Bayesian neural network from above\n",
    "  * `inputs` is a batch of images of shape `(batch_size, 28, 28, 1)`\n",
    "  * `num_samples` is a integer, for the number of Monte Carlo samples ($K$ in the above equation)\n",
    "* The function should compute the above approximation to the predictive distribution\n",
    "  * It should use `num_samples` Monte Carlo samples in the approximation\n",
    "* The function should then return a Tensor of shape `(batch_size, 10)` for the probabilities of the predictive distribution\n",
    "\n",
    "_Hint: you can access the logits of the categorical distribution with the `logits` attribute, which you can then use to compute the probabilities with `tf.nn.softmax`. You should not sample from the categorical distribution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db59b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def predictive_distribution(bayesian_model, inputs, num_samples):\n",
    "    \"\"\"\n",
    "    This function should compute the predictive distribution approximation as described\n",
    "    above. Your function should return the Tensor of probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on a batch of inputs\n",
    "\n",
    "for batch in mnist_train_data.take(1):\n",
    "    images, _ = batch\n",
    "\n",
    "y_pred = predictive_distribution(model, images, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21144a95",
   "metadata": {},
   "source": [
    "You should now complete the following `pd_loss_and_accuracy` function to compute the categorical cross-entropy loss and categorical accuracy on a dataset, using the predictive distribution approximation of the model.\n",
    "\n",
    "* The function takes the arguments `bayesian_model`, `dataset`, `num_samples` and `predictive_distribution_fn`\n",
    "  * `bayesian_model` will be the trained Bayesian neural network from above\n",
    "  * `dataset` is a tf.data.Dataset object, as used above for training or testing\n",
    "  * `num_samples` is a integer, for the number of Monte Carlo samples\n",
    "  * `predictive_distribution_fn` is a function used to compute the predictive distribution approximation. It has the signature defined above in the `predictive_distribution` function\n",
    "* The `pd_loss_and_accuracy` function should define two metric objects to compute the categorical cross-entropy loss and categorical accuracy\n",
    "* The function should iterate over `dataset`, compute the predictive distribution for the inputs using `predictive_distribution_fn`, and update the metrics\n",
    "* The function should then return a tuple of floats `(loss, accuracy)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def pd_loss_and_accuracy(bayesian_model, dataset, num_samples, \n",
    "                         predictive_distribution_fn=predictive_distribution):\n",
    "    \"\"\"\n",
    "    This function should compute the categorical cross-entropy loss and categorical accuracy\n",
    "    on a dataset as described above. Your function should return the loss and accuracy values.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea19bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to evaluate your model on the MNIST dataset using the predictive distribution\n",
    "\n",
    "final_loss, final_accuracy = pd_loss_and_accuracy(model, mnist_test_data, num_samples=5, \n",
    "                                                  predictive_distribution_fn=predictive_distribution)\n",
    "print(f\"MNIST test loss: {final_loss:.4f}, MNIST test accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8570298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to evaluate your model on the MNIST-C dataset using the predictive distribution\n",
    "\n",
    "final_loss, final_accuracy = pd_loss_and_accuracy(model, mnist_c_test_data, num_samples=5, \n",
    "                                                  predictive_distribution_fn=predictive_distribution)\n",
    "print(f\"MNIST-C test loss: {final_loss:.4f}, MNIST-C test accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105400c",
   "metadata": {},
   "source": [
    "You should see an improvement in these scores, compared with the evaluation of your model above (using `model.evaluate`) which uses a single Monte Carlo sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e2201",
   "metadata": {},
   "source": [
    "#### Uncertainty quantification\n",
    "\n",
    "One of the key advantages of using Bayesian neural networks is the ability to quantify the uncertainty in the predictions. Recall that the predictive entropy is calculated by computing the entropy of the predictive distribution, estimated using Monte Carlo samples:\n",
    "\n",
    "$$\n",
    "H(p(Y \\mid x,\\mathcal{D})) \\approx H\\left(\\frac{1}{K} \\sum_{k=1}^K p(y \\mid x, \\theta^{(k)}) \\right),\\qquad \\theta^{(k)}\\sim q_\\phi(\\theta),\\label{predictive_entropy}\\tag{1}\n",
    "$$\n",
    "\n",
    "where the entropy of a random variable $Y$ that can assume one of $n$ discrete states is given by\n",
    "\n",
    "$$\n",
    "H(Y) = \\mathbb{E}_p[-\\log p(y)] = -\\sum_{i=1}^n p(y_i) \\log p(y_i).\\label{entropy}\\tag{2}\n",
    "$$\n",
    "\n",
    "You should now complete the following `predictive_entropy` function, to compute the quantity given in equation \\eqref{predictive_entropy}.\n",
    "\n",
    "* The function takes the arguments `bayesian_model`, `inputs`, `num_samples` and `predictive_distribution_fn`\n",
    "  * `bayesian_model` will be the trained Bayesian neural network from above\n",
    "  * `inputs` is a batch of images of shape `(batch_size, 28, 28, 1)`\n",
    "  * `num_samples` is a integer, for the number of Monte Carlo samples ($K$ in the above equation)\n",
    "  * `predictive_distribution_fn` is a function used to compute the predictive distribution approximation. It has the signature defined above in the `predictive_distribution` function\n",
    "* The `predictive_entropy` function should use the `predictive_distribution_fn` to compute the entropy of the predictive distribution\n",
    "  * The predictive distribution should use `num_samples` Monte Carlo samples in the approximation\n",
    "  * The entropy should be computed according to equation \\eqref{entropy}\n",
    "* The function should then return a Tensor of shape `(batch_size,)` for the predictive entropy values for each input in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f92d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def predictive_entropy(bayesian_model, inputs, num_samples, \n",
    "                       predictive_distribution_fn=predictive_distribution):\n",
    "    \"\"\"\n",
    "    This function should compute the predictive entropy for a batch of inputs as described above.\n",
    "    Your function should return the Tensor of predictive entropy values.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b584853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on a batch of inputs\n",
    "\n",
    "for batch in mnist_train_data.take(1):\n",
    "    images, _ = batch\n",
    "\n",
    "pred_ent = predictive_entropy(model, images, num_samples=10, predictive_distribution_fn=predictive_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e24da",
   "metadata": {},
   "source": [
    "We have seen how the predictive entropy, conditional entropy and mutual information can be used to decompose the overall uncertainty into aleatoric and epistemic uncertainty.\n",
    "\n",
    "$$\n",
    "\\underbrace{H(p(Y \\mid x, \\mathcal{D}))}_{\\text{Predictive entropy}} = \\underbrace{I(\\theta; Y \\mid \\mathcal{D}, x)}_{\\substack{\\text{Mutual information/} \\\\ \\text{Epistemic uncertainty}}} + \\underbrace{\\mathbb{E}_{q_\\phi(\\theta)} H(p(Y \\mid x, \\theta))}_{\\substack{\\text{Expected entropy/} \\\\ \\text{Aleatoric uncertainty}}}.\\label{uncertainty_decomposition}\\tag{3}\n",
    "$$\n",
    "\n",
    "The aleatoric uncertainty is given by the expected entropy, which is calculated by computing an average entropy over output distributions:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q_\\phi(\\theta)} H(p(Y \\mid x, \\theta)) \\approx \\frac{1}{K} \\sum_{k=1}^K H\\left(p(Y \\mid x, \\theta^{(k)}) \\right),\\qquad \\theta^{(k)} \\sim q_\\phi(\\theta)\\label{expected_entropy}\\tag{4}\n",
    "$$\n",
    "\n",
    "You should now complete the following `expected_entropy` function, to compute the quantity given in equation \\eqref{expected_entropy}.\n",
    "\n",
    "* The function takes the arguments `bayesian_model`, `inputs` and `num_samples`\n",
    "  * `bayesian_model` will be the trained Bayesian neural network from above\n",
    "  * `inputs` is a batch of images of shape `(batch_size, 28, 28, 1)`\n",
    "  * `num_samples` is a integer, for the number of Monte Carlo samples ($K$ in the above equation)\n",
    "* The `expected_entropy` function should use `num_samples` Monte Carlo samples to compute the quantity in equation \\eqref{expected_entropy}\n",
    "* The function should then return a Tensor of shape `(batch_size,)` for the expected entropy values for each input in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82212ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def expected_entropy(bayesian_model, inputs, num_samples):\n",
    "    \"\"\"\n",
    "    This function should compute the predictive entropy for a batch of inputs as described above.\n",
    "    Your function should return the Tensor of predictive entropy values.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35844b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on a batch of inputs\n",
    "\n",
    "for batch in mnist_train_data.take(1):\n",
    "    images, _ = batch\n",
    "\n",
    "pred_ent = expected_entropy(model, images, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb5c28",
   "metadata": {},
   "source": [
    "The following cells analyse the model's uncertainty on a batch of inputs from the MNIST and the MNIST-C dataset.\n",
    "\n",
    "The cell plots histograms of the predictive entropy, expected entropy and mutual information values on the batch, and displays the images with the highest uncertainty values for each of these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse model predictions on a batch of inputs from the MNIST dataset\n",
    "\n",
    "for batch in mnist_train_data.take(1):\n",
    "    images, labels = batch\n",
    "    pe = predictive_entropy(model, images, num_samples=10,\n",
    "                            predictive_distribution_fn=predictive_distribution)\n",
    "    ee = expected_entropy(model, images, num_samples=10)\n",
    "    mi = pe - ee\n",
    "    pe, ee, mi = ops.convert_to_numpy(pe), ops.convert_to_numpy(ee), ops.convert_to_numpy(mi)\n",
    "    images = ops.convert_to_numpy(images)\n",
    "    valid_inx = np.logical_not(np.logical_or(np.isnan(pe), np.isnan(ee), np.isnan(mi)))\n",
    "    pe, ee, mi, images = pe[valid_inx], ee[valid_inx], mi[valid_inx], images[valid_inx]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(pe)\n",
    "plt.title(\"Predictive entropy\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(ee)\n",
    "plt.title(\"Expected entropy / Aleatoric uncertainty\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(mi)\n",
    "plt.title(\"Mutual information / Epistemic uncertainty\")\n",
    "plt.show()\n",
    "\n",
    "highest_pe = np.argsort(pe)[-1]\n",
    "highest_ee = np.argsort(ee)[-1]\n",
    "highest_mi = np.argsort(mi)[-1]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images[highest_pe])\n",
    "plt.gca().set_axis_off()\n",
    "plt.title(f\"Highest predictive entropy: {pe[highest_pe]:.4f}\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(images[highest_ee])\n",
    "plt.gca().set_axis_off()\n",
    "plt.title(f\"Highest aleatoric uncertainty: {ee[highest_ee]:.4f}\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(images[highest_mi])\n",
    "plt.gca().set_axis_off()\n",
    "plt.title(f\"Highest epistemic uncertainty: {mi[highest_mi]:.4f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d67da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse model predictions on a batch of inputs from the MNIST-C dataset\n",
    "\n",
    "for batch in mnist_c_train_data.take(1):\n",
    "    images, labels = batch\n",
    "    pe = predictive_entropy(model, images, num_samples=10,\n",
    "                            predictive_distribution_fn=predictive_distribution)\n",
    "    ee = expected_entropy(model, images, num_samples=10)\n",
    "    mi = pe - ee\n",
    "    pe, ee, mi = ops.convert_to_numpy(pe), ops.convert_to_numpy(ee), ops.convert_to_numpy(mi)\n",
    "    images = ops.convert_to_numpy(images)\n",
    "    valid_inx = np.logical_not(np.logical_or(np.isnan(pe), np.isnan(ee), np.isnan(mi)))\n",
    "    pe, ee, mi, images = pe[valid_inx], ee[valid_inx], mi[valid_inx], images[valid_inx]\n",
    "    \n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(pe)\n",
    "plt.title(\"Predictive entropy\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(ee)\n",
    "plt.title(\"Expected entropy / Aleatoric uncertainty\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(mi)\n",
    "plt.title(\"Mutual information / Epistemic uncertainty\")\n",
    "plt.show()\n",
    "\n",
    "highest_pe = np.argsort(pe)[-1]\n",
    "highest_ee = np.argsort(ee)[-1]\n",
    "highest_mi = np.argsort(mi)[-1]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images[highest_pe])\n",
    "plt.gca().set_axis_off()\n",
    "plt.title(f\"Highest predictive entropy: {pe[highest_pe]:.4f}\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(images[highest_ee])\n",
    "plt.gca().set_axis_off()\n",
    "plt.title(f\"Highest aleatoric uncertainty: {ee[highest_ee]:.4f}\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(images[highest_mi])\n",
    "plt.gca().set_axis_off()\n",
    "plt.title(f\"Highest epistemic uncertainty: {mi[highest_mi]:.4f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4a755",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! In this assignment you have developed a Bayesian neural network classifier for the MNIST and MNIST-C datasets, and compared overall performance and uncertainty quantification on both of these datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

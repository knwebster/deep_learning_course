{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 2: Multilayer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement and train a multilayer perceptron in Keras.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/bike_sharing.jpg\" title=\"Bike sharing\" style=\"width: 500px;\"/></center>\n",
    "<center><font style=\"font-size:12px\">source: <a href=https://www.visitlondon.com/traveller-information/getting-around-london/london-cycle-hire-scheme>visitlondon.com</a></font></center>\n",
    "\n",
    "#### The Bike Sharing dataset\n",
    "In this formative assessment, you will use the [Bike Sharing dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) from the UCI Machine Learning Repository. This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.\n",
    "\n",
    "* Fanaee-T, H,, & Gama, J. (2013), \"Event labeling combining ensemble detectors and background knowledge\", _Progress in Artificial Intelligence_, 1-15, Springer Berlin Heidelberg.\n",
    "\n",
    "Your goal is to use Keras to model the dataset using linear regression and MLP networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to load and sample the data\n",
    "\n",
    "df = pd.read_csv(Path(\"./data/bike_sharing.csv\"))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the dataset description](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) for more information on the attributes. There are two target variables: `casual` (number of casual users) and `registered` (number of registered users).\n",
    "\n",
    "You should first complete the following `get_inputs_and_targets` function, according to the following spec:\n",
    "\n",
    "* The function takes inputs `dataframe` and `target_variables`\n",
    "  * The `target_variables` is a list of column names that we will use for the targets\n",
    "* The function should return a tuple of DataFrames `(inputs_df, targets_df)`, where `targets_df` contains only the columns in `target_variables`, and `inputs_df` contains all the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_inputs_and_targets(dataframe, target_variables):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame and target_variables list as above, \n",
    "    and returns inputs and targets DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the input and target Tensors\n",
    "\n",
    "inputs_df, targets_df = get_inputs_and_targets(df, target_variables=['casual', 'registered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will need some preprocessing before it is ready to be used to train a deep learning model. \n",
    "\n",
    "Firstly, several of the attributes are categorical: `year`, `season`, `month`, `day_name`, `hr`, `holiday`, `weekday`, `workingday` and `weathersit`. We will represent each of these attributes with a one-hot encoding. For example, `year` takes one of the values `2011` or `2012` in the dataset. Will represent the year of a data example as either the one-hot vector `[1, 0]` or `[0, 1]`, corresponding to the year `2011` or `2012` respectively. In general, the length of the one-hot vector will equal the number of categories, and will be all zeros except for a single one in the place of the corresponding category for a particular data example.\n",
    "\n",
    "The final representation of our inputs will be the concatenation of all features, including one-hot vectors.\n",
    "\n",
    "You should now complete the following `convert_to_one_hot` function, according to the following specifications:\n",
    "\n",
    "* The function takes the inputs `inputs_dataframe` and `categorical_attributes`\n",
    "    * `categorical_attributes` will be a list of column names that are present in `inputs_dataframe`\n",
    "* The function should convert each categorical feature to a one-hot vector by replacing the column with a number of columns equal to the number of categories\n",
    "* The function should then return the updated DataFrame\n",
    "\n",
    "_Hint: see [`pd.get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html), and note that numerical columns can be converted to `category` type in order for this function to correctly the column as categorical._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def convert_to_one_hot(inputs_dataframe, categorical_attributes):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame and categorical_attributes list as above, \n",
    "    and converts the categorical features to one-hot encodings.\n",
    "    Your function should return the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to convert the categorical features to one-hot encodings\n",
    "\n",
    "cols = ['year', 'season', 'month', 'day_name', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "inputs_df = convert_to_one_hot(inputs_df, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second stage of preprocessing, we will scale the target values.\n",
    "\n",
    "You should now complete the following `scale_values` function, according to the following spec:\n",
    "\n",
    "* The function takes the `inputs_df` and `targets_df` DataFrames as arguments\n",
    "* The values in the `targets_df` are counts, and these should be converted as follows:\n",
    "$$\\text{count} \\mapsto \\log (1 + \\text{count})$$\n",
    "where the log is the natural logarithm.\n",
    "* The function should then return a tuple of Tensors `(inputs, targets)` of type `float32` with the transformed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def scale_values(inputs_df, targets_df):\n",
    "    \"\"\"\n",
    "    This function takes in the inputs and targets DataFrames and scales the values\n",
    "    in the target DataFrame according to the above description.\n",
    "    Your function should return a tuple of Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the scaled inputs and outputs Tensors\n",
    "\n",
    "inputs, targets = scale_values(inputs_df, targets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(ops.convert_to_numpy(inputs), \n",
    "                                                  ops.convert_to_numpy(targets), \n",
    "                                                  test_size=0.3)\n",
    "\n",
    "X_train, y_train = ops.convert_to_tensor(X_train), ops.convert_to_tensor(y_train)\n",
    "X_val, y_val = ops.convert_to_tensor(X_val), ops.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model\n",
    "\n",
    "We will first fit a simple linear regression model to the training data. Recall that this is a model of the form\n",
    "\n",
    "$$\n",
    "y = f_\\theta(\\mathbf{x}) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $y\\in\\mathbb{R}^C$ is the target variable, $\\mathbf{x}\\in\\mathbb{R}^{D}$ are the input features, $\\Theta\\in\\mathbb{R}^{C\\times D+1}$ are the model parameters, $\\epsilon\\in\\mathbb{R}^C$ with $(\\epsilon)_c\\sim\\mathcal{N}(0, 1)$ $(c=1,\\ldots,C)$ is the observation noise random variable, and $f_\\Theta:\\mathbb{R}^{D+1}\\mapsto\\mathbb{R}$ is given by\n",
    "\n",
    "$$\n",
    "f_\\Theta(\\hat{\\mathbf{x}}) = \\Theta\\hat{\\mathbf{x}},\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{D+1}$ is constructed by adding a constant 1 feature to $\\mathbf{x}$; that is, $\\hat{\\mathbf{x}}_0=1$.\n",
    "\n",
    "We will use the Keras API to implement a linear regression model, using the `Sequential` class.\n",
    "\n",
    "In the following function, you should build a `Sequential` model with just one `Dense` layer, which has two output units, one for each target variable, and no activation function. This is the same as the linear regression model above.\n",
    "\n",
    "* The function takes the `input_shape` as an argument, which should be used to specify the input shape with an `Input` layer\n",
    "* The function should build and return the `Sequential` object with one Dense layer with two output neurons, and no activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def sequential_linear_regression(input_shape):\n",
    "    \"\"\"\n",
    "    This function takes the input_shape as argument to build a Sequential model as \n",
    "    specified above. \n",
    "    The function should then return the Sequential model.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to build the model and print the model summary\n",
    "\n",
    "lr_model = sequential_linear_regression(input_shape=X_train.shape[1:])\n",
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now compile and fit the model to the training data. \n",
    "\n",
    "* The following function takes the following arguments:\n",
    "  * `sequential_model`: a Sequential model to fit to the training data\n",
    "  * `num_epochs`: a positive integer that defines the number of epochs to train the model\n",
    "  * `training_data`: a 2-tuple of Tensors (inputs, targets) for the training data\n",
    "  * `val_data`: a 2-tuple of Tensors (inputs, targets) for the validation data\n",
    "  * `batch_size`: a positive integer that defines the number of examples in each minibatch\n",
    "* The function should compile the model with the mean squared error loss and the SGD optimizer\n",
    "* The function should then fit the model to the training data for `num_epochs` epochs and save the returned history object\n",
    "* Your function should then return the history object\n",
    "\n",
    "_Hint: for the validation data, use the `validation_data` keyword argument (see [the docs](https://keras.io/api/models/model_training_apis/#fit-method))._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def compile_and_fit(sequential_model, num_epochs, training_data, val_data, batch_size):\n",
    "    \"\"\"\n",
    "    This function should compile and fit the sequential_model as described above. \n",
    "    The function should then return the history object that is returned from the fit method.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run your function to compile and fit the model\n",
    "\n",
    "history = compile_and_fit(lr_model, num_epochs=30, training_data=(X_train, y_train), \n",
    "                          val_data=(X_val, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves suggest that our linear regression model might be underfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the train and validation loss of the linear regression Sequential model\n",
    "\n",
    "print(\"Model train loss: {}\".format(lr_model.evaluate(X_train, y_train, verbose=0)))\n",
    "print(\"Model validation loss: {}\".format(lr_model.evaluate(X_val, y_val, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model above is equivalent to fitting separate linear regression models for each scalar target output (`casual` and `registered`). However, these two models are clearly very closely related, and there is likely to be shared features that would be helpful for both models. \n",
    "\n",
    "In addition, we would like to train a higher capacity model to attempt to alleviate the potential underfitting we might be seeing in the above linear regression model.\n",
    "\n",
    "Both of these reasons are motivation for training a deeper multilayer perceptron (MLP) model. This is a higher capacity model that simple linear regression, and we expect that the intermediate features represented by the hidden layers will learn features of the data that are useful for predicting both of the target variables.\n",
    "\n",
    "You should now complete the following `get_mlp` function to build an MLP model according to the following spec:\n",
    "\n",
    "* The function takes the arguments `input_shape` and `hidden_layers`\n",
    "* `hidden_layers` is a list of integers, corresponding to the number of neurons in the hidden layers\n",
    "* The function should build the MLP using the `Sequential` API\n",
    "  * It should use the `input_shape` argument in the first layer of the model\n",
    "  * The hidden layers should each use a ReLU activation function\n",
    "  * The output layer should have 2 neurons, and not use an activation function\n",
    "* The function should then return the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_mlp(input_shape, hidden_layers):\n",
    "    \"\"\"\n",
    "    This function takes the input_shape, hidden_layers and output_units as arguments \n",
    "    to build a Sequential model as specified above. \n",
    "    The function should then return the Sequential model.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to build the model and print the model summary\n",
    "\n",
    "mlp_model = get_mlp(input_shape=X_train.shape[1:], hidden_layers=[64, 32])\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run your compile_and_fit function on the MLP\n",
    "\n",
    "history = compile_and_fit(mlp_model, num_epochs=30, training_data=(X_train, y_train), \n",
    "                          val_data=(X_val, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the train and validation loss of the linear regression Sequential model\n",
    "\n",
    "print(\"Model train loss: {}\".format(mlp_model.evaluate(X_train, y_train, verbose=0)))\n",
    "print(\"Model validation loss: {}\".format(mlp_model.evaluate(X_val, y_val, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance has improved using the MLP instead of linear regression. However, there is still room for improvement and we could try further increasing the capacity. You should try re-building and training MLP models for different hyperparameter settings to see how much you are able to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now implemented linear regression using the Keras API, as well as an MLP model, and compared the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

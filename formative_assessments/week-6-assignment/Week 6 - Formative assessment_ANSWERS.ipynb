{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 6: Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will develop an RNN language model to generate text.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import TextVectorization\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/shakespeare.png\" title=\"Shakespeare\" style=\"width: 350px;\"/></center>\n",
    "  \n",
    "#### The Shakespeare dataset\n",
    "\n",
    "In this assignment, you will use a subset of the [Shakespeare dataset](http://shakespeare.mit.edu). This dataset consists of a single text file with several excerpts concatenated together. The data is in raw text form, and so far has not yet had any preprocessing. \n",
    "\n",
    "Your goal is to construct an unsupervised RNN model that can generate text according to a distribution learned from the dataset. This will be a character-level sequence model, that will predict text one character at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file into a string\n",
    "\n",
    "with open(Path('data/Shakespeare.txt'), 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of chunks of text\n",
    "\n",
    "text_chunks = text.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a feel for what the text looks like, we will print a few chunks from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some randomly selected text samples\n",
    "\n",
    "num_samples = 3\n",
    "inx = np.random.choice(len(text_chunks), num_samples, replace=False)\n",
    "for chunk in np.array(text_chunks)[inx]:\n",
    "    print(chunk)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Datasets\n",
    "\n",
    "The model will receive a sequence of characters and predict the next character in the sequence. At training time, the model can be passed an input sequence, with the target sequence shifted by one.\n",
    "\n",
    "For example, consider the expression `to be or not to be` from Shakespeare's play 'Hamlet'. Given the input `to be or not to b`, the correct prediction is `o be or not to be`. Notice that the prediction is the same length as the input.\n",
    "\n",
    "<center><img src=\"figures/to-be-or-not-to-be.png\" alt=\"Training procedure\" style=\"width: 750px;\"/></center>\n",
    "<center>Schematic diagram showing the training procedure for the character language model. The target sequence is the same as the input sequence, shifted by one.</center>\n",
    "<br>\n",
    "\n",
    "We will use PyTorch custom Datasets to handle the data processing for this task. These Datasets will do some filtering on the text chunks data, and tokenize the text at the character level. The corresponding DataLoaders will return zero-padded batches of integer tokens for both inputs and outputs. \n",
    "\n",
    "We will first do some preliminary processing on the text chunks and create training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip any whitespace at the beginning or end of the strings\n",
    "\n",
    "text_chunks = [s.strip() for s in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and validation splits\n",
    "\n",
    "train_split, valid_split = train_test_split(text_chunks, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now complete the following `ShakespeareDataset` class which we will use to create a custom Dataset object from one of the splits above. This class subclasses from the `torch.utils.data.Dataset` class. \n",
    "\n",
    "We will need to convert the sentence strings to integer tokens for processing by the recurrent neural network. This conversion should be performed by your custom Dataset class, and we will use the `TextVectorization` layer for this.\n",
    "\n",
    "* The class initializer takes `data_split` (one of the splits above), `min_len`, `max_len` and `textvectorization_layer` as arguments\n",
    "* Your class should implement the `__init__`, `__len__` and `__getitem__` methods\n",
    "* The Dataset should filter out any example that has less than `min_len` characters or more than `max_len` characters\n",
    "* The Dataset should tokenize the text using the `textvectorization_layer` (to be defined later)\n",
    "* For each tokenized example (with length `seq_len`), the Dataset should split the example into `input_tokens` and `target_tokens`\n",
    "  * Both `input_tokens` and `target_tokens` should have length `seq_len - 1`\n",
    "  * `input_tokens` should contain the first `seq_len - 1` tokens of each sequence \n",
    "  * `target_tokens` should contain the last `seq_len - 1` tokens of each sequence\n",
    "* The Dataset should return the tuple `(input_tokens, target_tokens)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL ####\n",
    "\n",
    "# Complete the following class.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This custom Dataset class takes data_split, min_len, max_len and textvectorization_layer \n",
    "    as arguments in the initializer, and returns tokenized text according to the spec above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_split, min_len, max_len, textvectorization_layer):\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.data_split = [s for s in data_split if self.min_len <= len(s) <= self.max_len]\n",
    "        self.textvectorization = textvectorization_layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_split)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data_split[index]\n",
    "        tokenized_text = self.textvectorization(text)\n",
    "        return tokenized_text[:-1], tokenized_text[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now complete the following `get_dataloaders` function to create DataLoaders from Dataset instances using your custom class above.\n",
    "\n",
    "* The function takes `train_text`, `valid_text`, `min_len`, `max_len` and `batch_size` as arguments\n",
    "  * `train_text` and `valid_text` are lists of text chunks, as defined above\n",
    "  * `min_len` and `max_len` are integers defining minimum (resp. maximum) character lengths, as described above in the custom Dataset pre-processing\n",
    "  * `batch_size` is an integer defining the batch size to be returned by the DataLoaders\n",
    "* Your function should create an instance of the `TextVectorization` layer\n",
    "  * This layer should be set up to allow unlimited number of tokens\n",
    "  * It should standardize the text by converting it to lower case\n",
    "  * It should split the input sentences at the character level\n",
    "  * The `TextVectorization` object should be configured using the `train_text` chunks\n",
    "* A Dataset should be created for both training and validation splits using the `ShakespeareDataset` class above\n",
    "* Training and validation DataLoaders should then be defined\n",
    "  * The training dataset should be shuffled, the validation dataset should not\n",
    "  * Both dataset should use a batch size of `batch_size`\n",
    "  * The DataLoaders should return tuples of integer tokens `(inputs, outputs)`, with each example padded with zeros up to the length of the longest sequence in the batch\n",
    "* The function should then return the DataLoaders and `TextVectorization` object in a tuple `(train_dl, valid_dl, text_vectorization)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_dataloaders(train_text, valid_text, min_len, max_len, batch_size):\n",
    "    \"\"\"\n",
    "    This function uses the training and validation text chunks and creates Dataset\n",
    "    objects and corresponding DataLoaders as described above. \n",
    "    It should then return the train_dataloader, valid_dataloader and TextVectorization object\n",
    "    \"\"\"\n",
    "    text_vectorization = TextVectorization(max_tokens=None, standardize='lower', split='character')\n",
    "    text_vectorization.adapt(train_text)\n",
    "\n",
    "    train_ds = ShakespeareDataset(train_text, min_len, max_len, text_vectorization)\n",
    "    valid_ds = ShakespeareDataset(valid_text, min_len, max_len, text_vectorization)\n",
    "\n",
    "    def padded_batch(batch):\n",
    "        inputs, outputs = zip(*batch)\n",
    "        \n",
    "        # The pad_sequence fn expects torch Tensors. The following conversion is only necessary for TF backend\n",
    "        inputs = [torch.tensor(ops.convert_to_numpy(t)) for t in inputs]\n",
    "        outputs = [torch.tensor(ops.convert_to_numpy(l)) for l in outputs]\n",
    "        \n",
    "        inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "        outputs = torch.nn.utils.rnn.pad_sequence(outputs, batch_first=True, padding_value=0)\n",
    "        return inputs, outputs\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=padded_batch)\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=padded_batch)\n",
    "    return train_dataloader, val_dataloader, text_vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to create the DataLoaders and TextVectorization object\n",
    "\n",
    "train_dl, valid_dl, text_vectorization = get_dataloaders(train_split, valid_split, \n",
    "                                                         min_len=10, max_len=400, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the training DataLoader\n",
    "\n",
    "inputs, outputs = next(iter(train_dl))\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train the recurrent neural network model\n",
    "\n",
    "You are now ready to build your RNN character-level language model. You should write the following function to build and compile the model. The function takes arguments `text_vectorization` (created earlier), `embedding_dim` (for the `Embedding` layer), and `gru_units` (for the GRU layer). \n",
    "\n",
    "Using the functional API, your function should build your model according to the following specifications:\n",
    "\n",
    "* The first layer should be an `Input` layer, with a single dimension for the (variable) sequence length\n",
    "* The second layer should be an Embedding layer with an embedding dimension of `embedding_dim`, and the vocabulary size set using the `text_vectorization` object\n",
    "  * *Hint: Use the `get_vocabulary` method of the* `TextVectorization` *object to determine the vocabulary size*\n",
    "  * The Embedding layer should also mask the zero padding in the input sequences\n",
    "* The next layer should be a (uni-directional) GRU layer with number of units set by the `gru_units` argument\n",
    "  * The GRU layer should return the full sequence, instead of just the output state at the final time step.\n",
    "  * It should also return its internal state\n",
    "* The output of the GRU layer should then be fed through a final `Dense` layer with number of units set to vocabulary size, and no activation function. Call this layer `preds`\n",
    "* The network should have multiple outputs consisting of the `Dense` layer output and the internal state of the GRU layer\n",
    "* The model should then be compiled.\n",
    "  * Use the Adam optimizer with the default arguments\n",
    "  * For the `loss` argument, you should pass a list of losses, one for each model output. The `Dense` layer output should have a cross entropy loss, and the GRU internal state loss can be `None`\n",
    "  * Similarly, use a sparse categorical accuracy metric, just for the `Dense` layer output\n",
    "* Your function should then return the compiled model\n",
    "\n",
    "_Hint: you might find [this Keras guide](https://keras.io/guides/functional_api/#manipulate-complex-graph-topologies) to be a useful example for working with multi-input and multi-output models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_model(text_vectorization, embedding_dim, gru_units):\n",
    "    \"\"\"\n",
    "    This function takes a vocabulary size and batch size, and builds and returns a \n",
    "    Sequential model according to the above specification.\n",
    "    \"\"\"\n",
    "    vocab_size = len(text_vectorization.get_vocabulary())\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(None,), name=\"token_input\")\n",
    "    embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                                          mask_zero=True, name='embedding')(inputs)\n",
    "    h, state = keras.layers.GRU(units=gru_units, return_sequences=True, \n",
    "                                   return_state=True, name='gru')(embedding)\n",
    "    preds = keras.layers.Dense(vocab_size, name='preds')(h)\n",
    "    model = keras.Model(inputs=inputs, outputs=[preds, state])\n",
    "    \n",
    "    losses = [\n",
    "        keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        None\n",
    "    ]\n",
    "    model.compile(optimizer='adam', \n",
    "                  metrics=[['sparse_categorical_accuracy'], []], \n",
    "                  loss=losses)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and print the model summary\n",
    "\n",
    "rnn_model = get_model(text_vectorization, 256, 1024)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = rnn_model.fit(train_dl, validation_data=valid_dl, epochs=15, \n",
    "                        callbacks=[keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot accuracy vs epoch and loss vs epoch\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['preds_sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_preds_sparse_categorical_accuracy'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(1 + np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(1 + np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a text generation algorithm\n",
    "\n",
    "An algorithm to generate text is as follows:\n",
    "\n",
    "1. Specify a seed string (e.g. `'ROMEO:'`) to get the network started, and a define number of characters for the model to generate, `num_generation_steps`.\n",
    "2. Tokenize the seed string to obtain a list containing a list of the integer tokens.\n",
    "3. Reset the initial state of the recurrent network layer to zeros. \n",
    "4. Convert the token list into a Tensor (or numpy array) and pass it to your model as a batch of size one.\n",
    "5. Get the model prediction (logits) for the last time step and extract the state of the recurrent layer.\n",
    "6. Use the logits to construct a categorical distribution and sample a token from it.\n",
    "7. Repeat the following for `num_generation_steps - 1` steps:\n",
    "\n",
    "    1. Use the saved state of the recurrent layer and the last sampled token to get new logit predictions\n",
    "    2. Use the logits to construct a new categorical distribution and sample a token from it.\n",
    "    3. Save the updated state of the recurrent layer.    \n",
    "\n",
    "8. Take the final list of tokens and convert to text using the TextVectorization layer vocabulary.\n",
    "\n",
    "Note that we have built our RNN model to return the internal state of the recurrent layer, as well as the logits output from the `Dense` layer. For the GRU layer, the internal state is a single Tensor of shape `(batch_size, gru_units)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's current recurrent state\n",
    "\n",
    "inputs, outputs = next(iter(train_dl))\n",
    "print(rnn_model(inputs)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will break the algorithm down into two steps. First, you should complete the following `sample_token` function that takes a sequence of tokens of any length and returns a token prediction for the last time step. The specification is as follows:\n",
    "\n",
    "* The function takes the `model` instance, `token_sequence` Tensor, and optional `initial_state` Tensor for the GRU layer\n",
    "* The `token_sequence` will be an integer Tensor with shape `(batch_size, seq_length)`\n",
    "  * The `seq_length` will be greater or equal to one\n",
    "* If the function argument `initial_state` is `None`, then the function should reset the state of the recurrent layer to zeros\n",
    "* Otherwise, if the function argument `initial_state` is a 2D Tensor or numpy array, it should be used as the initial state of the GRU layer\n",
    "* Get the model's prediction (logits) for the last time step only\n",
    "* Use the logits to form a categorical distribution and sample from it (*hint: you might find the* `keras.random.categorical` *function useful for this; see the documentation [here](https://keras.io/api/random/random_ops/#categorical-function)*)\n",
    "* The function should then return the sample as a 2D integer Tensor of shape `(batch_size, 1)` as well as an updated GRU layer state of shape `(batch_size, gru_units)` in a tuple `(samples, updated_state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def sample_token(model, token_sequence, initial_state=None):\n",
    "    \"\"\"\n",
    "    This function takes a model object, a token sequence and an optional initial\n",
    "    state for the recurrent layer. The function should return the logits prediction\n",
    "    for the final time step as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    h = token_sequence\n",
    "    updated_state = None\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.InputLayer):\n",
    "            continue  # skip the Input layer\n",
    "        elif isinstance(layer, keras.layers.GRU):\n",
    "            if initial_state is None:\n",
    "                initial_state = layer.get_initial_state(h.shape[0])\n",
    "            h, updated_state = layer(h, initial_state=initial_state)\n",
    "        else:\n",
    "            h = layer(h)\n",
    "    final_step = h[:, -1, :]  # (batch_size, num_tokens)\n",
    "    samples = keras.random.categorical(final_step, 1)  # (batch_size, 1)\n",
    "    return samples, updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function by passing in a dummy token sequence\n",
    "\n",
    "sample_token(rnn_model, ops.convert_to_tensor([[30, 2, 24], [16, 12, 33]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you should complete the following function to generate text from the model, given a seed string.\n",
    "\n",
    "* This function takes the `model` instance, `seed_string`, `text_vectorization`, `num_generation_steps` and `sample_token` function as arguments\n",
    "* The function should first convert the `seed_string` to integer tokens using the `text_vectorization` object, and store them in a 2D integer Tensor with batch size equal to one\n",
    "* The function should then run an internal loop for `num_generation_steps`:\n",
    "  * In the first iteration through the loop, the integer token sequence should be passed to the `sample_token` function (passed in as an argument), to get the next sample token and updated GRU state\n",
    "  * The `initial_state` can be set to `None` in the first iteration, in which case it is initialised to zeros\n",
    "  * For the remaining iterations, the `sample_token` function should be called using the sampled token (with batch size and sequence length of one) and updated internal GRU state\n",
    "* The `text_vectorization` object should then be used to convert the final sequence of integer tokens back to characters, and then concatenated to a single string\n",
    "  * The final string will have length given by `num_generation_steps` plus the length of the initial seed string\n",
    "* Your function should then return this final string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def generate_text(model, seed_string, text_vectorization, num_generation_steps, sample_token=sample_token):\n",
    "    \"\"\"\n",
    "    This function takes a model object, a seed string, a TextVectorization object and a \n",
    "    number of steps to generate characters as arguments. It should generate text \n",
    "    according to the above directions and return the extended string.\n",
    "    \"\"\"\n",
    "    token_sequence = text_vectorization(seed_string)[None, ...]  # (1, seq_length)\n",
    "    input_sequence = token_sequence\n",
    "    initial_state = None\n",
    "    for _ in range(num_generation_steps):\n",
    "        sample, updated_state = sample_token(model, input_sequence, initial_state=initial_state)\n",
    "        token_sequence = ops.concatenate((token_sequence, sample), axis=1)\n",
    "        input_sequence = sample\n",
    "        initial_state = updated_state\n",
    "    \n",
    "    inx_to_chars = {i: c for i, c in enumerate(text_vectorization.get_vocabulary())}\n",
    "    final_token_sequence = ops.convert_to_numpy(ops.squeeze(token_sequence))\n",
    "    final_char_sequence = [inx_to_chars[token] for token in final_token_sequence]\n",
    "    return ''.join(final_char_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text from the model\n",
    "\n",
    "You are now ready to generate text from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed string and number of generation steps\n",
    "\n",
    "init_string = 'ROMEO:'\n",
    "num_generation_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your model and function above to generate text\n",
    "\n",
    "print(generate_text(rnn_model, init_string, text_vectorization, num_generation_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now built and trained a character-level RNN language model on text data, and used it to generate new text examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

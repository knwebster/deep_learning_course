{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 5: Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to compute saliency maps for a pre-trained CNN model.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/imagenet.jpeg\" title=\"ImageNet\" style=\"width: 650px;\"/></center>\n",
    "  \n",
    "#### The ImageNet dataset\n",
    "\n",
    "In the first part of this assignment, you will use a subset of the [ImageNet dataset](http://www.image-net.org). This is a large-scale image classification dataset, that is organised according to the [WordNet](https://wordnet.princeton.edu) hierarchy. It contains over 20,000 categories, with hundreds of images per category. \n",
    "\n",
    "* Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. & Fei-Fei, L. (2009), \"ImageNet: A Large-Scale Hierarchical Image Database\", _IEEE Computer Vision and Pattern Recognition (CVPR)_.\n",
    "* Deng, J., Li, K., Do, M., Su, H., Fei-Fei, L. (2009), \"Construction and Analysis of a Large Scale Image Ontology\", in _Vision Sciences Society (VSS)_.\n",
    "* Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., & Fei-Fei, L. (2015), \"ImageNet Large Scale Visual Recognition Challenge\", *International Journal of Computer Vision (IJCV)*, **115** (3), 211-252.\n",
    "\n",
    "Your goal is to use a convolutional neural network pre-trained on the ImageNet dataset, to compute saliency maps that analyse the sensitivity of network predictions to pixels in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saliency maps\n",
    "\n",
    "Saliency maps are an important general concept in computer vision that refer to a group of techniques for highlighting specific features in an image (such as the pixels) that are the most conspicous (or salient) in relation to some property of the image (typically summarised as a scalar value). For example, consider how colour images can be converted to black-and-white to analyse colour intensity, or how night vision can be used to detect light sources.\n",
    "\n",
    "In the context of deep learning models, saliency maps have been used as a means to make network predictions more interpretable. They can be used to analyse the sensitivity of network predictions to pixels in the input. In other words, they highlight which pixels affect the network prediction the most - the underlying assumption being that such pixels will most likely correspond to the object being classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pre-trained CNN model\n",
    "\n",
    "For this part of the assignment, we will use a VGG16 convolutional neural network (CNN) that has been pre-trained on the ImageNet dataset. The VGG network is an important CNN architecture that was developed by the [Visual Geometry Group](https://www.robots.ox.ac.uk/~vgg/) at the University of Oxford. \n",
    "\n",
    "* Simonyan, K. & Zisserman, A. (2015), \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", *International Conference on Learning Representations (ICLR)*, San Diego, CA, USA, May 7-9, 2015.\n",
    "\n",
    "This model is available to download directly from `keras.applications`. You should now complete the following function to download this model. \n",
    "\n",
    "* The function should instantiate the VGG16 model from `keras.applications` (see the [docs](https://keras.io/api/applications/vgg/#vgg16-function))\n",
    "* The model should use weights learned from pre-training on ImageNet\n",
    "* It should include the final `Dense` layers of the architecture\n",
    "* It should return the logits in the final layer (i.e. the softmax activation should be removed)\n",
    "* Your function should then return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_vgg16():\n",
    "    \"\"\"\n",
    "    This function should download the VGG16 model as specified above.\n",
    "    The function should then return the model instance.\n",
    "    \"\"\"\n",
    "    vgg16 = keras.applications.VGG16(include_top=True, weights='imagenet', classifier_activation=None) \n",
    "    return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to instantiate the VGG16 model\n",
    "\n",
    "vgg16 = get_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess images and labels\n",
    "\n",
    "For this assignment, you will use a random sample of 1% of the images from the validation set of the ImageNet dataset. Note that the full dataset can be downloaded from [here](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description), but it is not necessary to download it for this assignment. \n",
    "\n",
    "In addition, the ground truth labels are already available in the json files `labels.json`, `synset.json` and `synset_words.json`.\n",
    "\n",
    "* The `labels.json` file is a dictionary mapping each filename in the (full) validation set to a label index. This index matches with the index of the network output\n",
    "* Labels in ImageNet are organised in 'synsets' (synonym sets), consisting of multiple words or phrases\n",
    "* The `synsets.json` file contains the codes for each of the 1000 synsets predicted by VGG16, organised in the same order as the network output indices\n",
    "* Finally, `synset_words.json` maps synset codes to human-readable text descriptions\n",
    "\n",
    "The following cell loads each of these json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label data\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        j = json.load(json_file)\n",
    "    return j\n",
    "\n",
    "labels = load_json(Path('./data/labels.json'))\n",
    "synsets = load_json(Path('./data/synsets.json'))\n",
    "synset_words = load_json(Path('./data/synset_words.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of image file paths\n",
    "\n",
    "img_filepaths = list(Path('./data/images').glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample image and label\n",
    "\n",
    "def sample_random_image_and_label():\n",
    "    i = np.random.choice(len(img_filepaths))\n",
    "    img_filepath = img_filepaths[i]\n",
    "    filename = img_filepath.name\n",
    "    label_inx = labels[filename]\n",
    "    synset = synsets[label_inx]\n",
    "    words_label = synset_words[synset]\n",
    "    return str(img_filepath), words_label\n",
    "\n",
    "img_file, label = sample_random_image_and_label()\n",
    "raw_img = keras.utils.load_img(img_file, color_mode=\"rgb\")\n",
    "img_array = keras.utils.img_to_array(raw_img, dtype='int32')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_array)\n",
    "plt.axis('off')\n",
    "plt.title(\"Label: {}\".format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the model summary, the VGG16 model expects an input image with shape `(224, 224, 3)` (unless the default `image_data_format` setting has been changed, see also [here](https://keras.io/api/applications/)). However, the images in the dataset come in varying sizes, as can be seen from the sample above. \n",
    "\n",
    "You should now complete the following function, that will load and preprocess an image from file.\n",
    "\n",
    "* The function takes a `filepath` and `shape` as arguments, where `shape` has the format `(H, W, C)`\n",
    "* It should load the image from file into a NumPy array (see the example code above)\n",
    "  * The image should be resized according to the `shape` argument\n",
    "* The function should then convert the array to a Tensor and return the resized image Tensor\n",
    "\n",
    "_Hint: Read the [docs](https://keras.io/api/data_loading/image/) for more details on the image loading functions available in Keras._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def load_and_resize_image(filepath, shape):\n",
    "    \"\"\"\n",
    "    This function should load an image from file, and resize it according to the\n",
    "    instructions above.\n",
    "    It should then return the resized image Tensor.\n",
    "    \"\"\"\n",
    "    raw_img = keras.utils.load_img(filepath, color_mode=\"rgb\", target_size=shape[:2], interpolation='nearest')\n",
    "    img_array = keras.utils.img_to_array(raw_img)\n",
    "    return ops.convert_to_tensor(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on a random image\n",
    "\n",
    "img_path, _ = sample_random_image_and_label()\n",
    "resized_img = load_and_resize_image(img_path, (224, 224, 3))\n",
    "resized_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get model predictions\n",
    "The VGG16 model requires images to be preprocessed before being passed through the model (see the [docs](https://keras.io/api/applications/vgg/)). The `keras.applications.vgg16` module provides a convenient function `preprocess_input` for this, as well as a `decode_predictions` function to help with interpreting model predictions.\n",
    "\n",
    "You should now complete the following function to get the model predictions from a resized image Tensor.\n",
    "\n",
    "* The function takes the `vgg16` model, `resized_img` as an argument, as output from the `load_and_resize_image` function\n",
    "* It should process the image Tensor using the `preprocess_input` function\n",
    "* The function should then compute the `vgg16` model predictions on the image Tensor, using a batch size of one\n",
    "* It should then collect the top 3 predictions using the `decode_predictions` function\n",
    "  * This will be returned as a list containing a list of 3-tuples `(synset_code, word_label, logit)` for each of the top 3 predictions\n",
    "* The function should then return the model's logit predictions, and the top 3 predictions as returned by `decode_predictions` above, output as a tuple `(preds, decoded_preds)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_model_predictions(vgg16, resized_img):\n",
    "    \"\"\"\n",
    "    This function should process a resized image and compute model predictions\n",
    "    as described above.\n",
    "    It should return a tuple of 2 elements (preds, decoded_predictions), where preds\n",
    "    is the model logit predictions, and decoded_predictions are the decoded top 3 predictions.\n",
    "    \"\"\"\n",
    "    processed_img = keras.applications.vgg16.preprocess_input(resized_img)\n",
    "    preds = vgg16(processed_img[None, ...])\n",
    "    decoded = keras.applications.vgg16.decode_predictions(preds, top=3)\n",
    "    return preds, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on a randomly selected image\n",
    "\n",
    "img_file, label = sample_random_image_and_label()\n",
    "resized_img = load_and_resize_image(img_file, (224, 224, 3))\n",
    "preds, decoded_preds = get_model_predictions(vgg16, resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the image, ground truth label and model predictions\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(ops.convert_to_numpy(resized_img) / 255.)\n",
    "plt.axis('off')\n",
    "plt.title(\"Ground truth label: {}\".format(label))\n",
    "plt.show()\n",
    "\n",
    "print(\"Model predictions:\\n{}\".format(decoded_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to compute the saliency map of the model on a randomly selected image. The saliency map should be computed by calculating the gradient of the model's top-scoring logit prediction with respect to the input image.\n",
    "\n",
    "You should now complete the following function to compute the saliency map.\n",
    "\n",
    "* The function takes the `vgg16` model and `resized_img` as input arguments, as well as the `get_model_predictions` function\n",
    "* It should compute the model predictions using `get_model_predictions`\n",
    "* The function should calculate which logit has the maximum score out of the model predictions\n",
    "* Using automatic differentiation, it should then compute the gradient of this logit with respect to the input image Tensor\n",
    "  * This implementation will be backend-specific. Your function should detect the current backend (`tensorflow` or `torch`) and implement the automatic differentiation accordingly\n",
    "* This will give gradients of shape `(224, 224, 3)`. To display the saliency map, the function should take the maximum absolute value of the gradient over the channel dimensions at each spatial location\n",
    "  * This will result in a Tensor of shape `(224, 224)`\n",
    "* The function should then return the resulting saliency map Tensor and the decoded predictions in a tuple `(sm, decoded_preds)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def saliency_map(vgg16, resized_img, get_model_predictions=get_model_predictions):\n",
    "    \"\"\"\n",
    "    This function should compute the saliency map as described above.\n",
    "    It should return the tuple (saliency_map, decoded_predictions).\n",
    "    \"\"\"\n",
    "    backend = keras.config.backend()\n",
    "    if backend == 'tensorflow':\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(resized_img)\n",
    "            preds, decoded = get_model_predictions(vgg16, resized_img)\n",
    "            preds = tf.squeeze(preds)\n",
    "            argmax = tf.math.argmax(preds)\n",
    "            max_logit = preds[argmax]\n",
    "        grads = tape.gradient(max_logit, resized_img)\n",
    "        sm = tf.reduce_max(tf.math.abs(grads), axis=-1)\n",
    "    elif backend == 'torch':\n",
    "        resized_img.requires_grad_()\n",
    "        # vgg16.zero_grad()\n",
    "        preds, decoded = get_model_predictions(vgg16, resized_img)\n",
    "        preds = preds.squeeze(dim=0)\n",
    "        score_max = preds.max()\n",
    "        score_max.backward()\n",
    "        sm, _ = torch.max(torch.abs(resized_img.grad), dim=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Only torch and tensorflow backends supported\")\n",
    "    return sm, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the saliency map for a randomly selected image\n",
    "\n",
    "img_file, label = sample_random_image_and_label()\n",
    "resized_img = load_and_resize_image(img_file, (224, 224, 3))\n",
    "sm, decoded_preds = saliency_map(vgg16, resized_img)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(ops.convert_to_numpy(resized_img) / 255.)\n",
    "plt.axis('off')\n",
    "plt.title(\"Ground truth label: {}\".format(label))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(ops.convert_to_numpy(sm), cmap='hot')\n",
    "plt.axis('off')\n",
    "plt.title(\"Saliency map\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Model predictions:\\n{}\".format(decoded_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the saliency maps highlight the regions of the image that give rise to the model's top prediction. Usually this is centred on the object location (particularly when the model prediction is correct), but can also sometimes show other features of the image that help the model to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now seen how to compute saliency maps for a pre-trained CNN model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

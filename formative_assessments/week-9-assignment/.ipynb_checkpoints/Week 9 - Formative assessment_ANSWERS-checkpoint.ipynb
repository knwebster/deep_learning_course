{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 10: Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement the variational autoencoder algorithm for an image dataset of celebrity faces. You will use the trained encoder and decoder networks to reconstruct and generate images. You will also see how the latent space encodes high-level information about the images.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Layer, Input, Dense, Flatten, Reshape, Conv2D,  Conv2DTranspose, BatchNormalization\n",
    "from keras.metrics import Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/celeba.png\" title=\"CelebA\" style=\"width: 650px;\"/></center>\n",
    "\n",
    "#### The Large-scale CelebFaces Attributes (CelebA) Dataset\n",
    "\n",
    "For this assignment you will use a subset of the [CelebFaces Attributes (CelebA) dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). The full dataset contains over 200K images CelebA contains thousands of colour images of the faces of celebrities, together with tagged attributes such as 'Smiling', 'Wearing glasses', or 'Wearing lipstick'. It also contains information about bounding boxes and facial part localisation. CelebA is a popular dataset that is commonly used for face attribute recognition, face detection, landmark (or facial part) localization, and face editing & synthesis. \n",
    "\n",
    "* Z. Liu, P. Luo, X. Wang, and X. Tang. \"Deep Learning Face Attributes in the Wild\", Proceedings of International Conference on Computer Vision (ICCV), 2015.\n",
    "\n",
    "Your goal is to implement the variational autoencoder algorithm for a subset of the CelebA dataset. For practical reasons we will keep the dataset and the network size relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the dataset\n",
    "\n",
    "For this assignment, you will use a subset of the CelebA dataset. Note that the full dataset can be downloaded from [the CelebA dataset webpage](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)), but this is not necessary for this assignment. \n",
    "\n",
    "In addition, attribute labels for the subset have been saved in the CSV file `list_attr_celeba.csv`. We will use this in the last part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now write the following `load_dataset` function to create a `tf.data.Dataset` object from the files saved in the images folder.\n",
    "\n",
    "* The function takes `split` as an argument, which will be equal to one of the strings `\"train\"`, `\"val\"` or  `\"test\"`, `batch_size`, an optional `shuffle_buffer` argument and `image_dir` argument\n",
    "* The function should create a Dataset containing the filepaths saved in the corresponding `split` subfolder of the `image_dir` directory\n",
    "* The function should include a nested/inner function used to map the Dataset\n",
    "  * This function will take the `filepath` as an argument\n",
    "  * It should read the contents of the file saved at `filepath` - this will be a jpeg image\n",
    "  * It should then decode the jpeg and scale the pixel values to lie in the range $[0, 1]$\n",
    "  * You should use the [`set_shape`](https://www.tensorflow.org/api_docs/python/tf/Tensor#set_shape) Tensor method to fix the (static) shape of the image Tensor to `(64, 64, 3)`\n",
    "  * It should then the image Tensor\n",
    "* The function should then apply the nested function using the `map` method\n",
    "* If `shuffle_buffer` is not None, then it should be used to shuffle the Dataset\n",
    "* It should then batch the Dataset using the `batch_size` argument\n",
    "* Finally, the function should prefetch the Dataset using the argument `tf.data.AUTOTUNE`\n",
    "* The function should then return the Dataset\n",
    "\n",
    "_Hint: The Dataset can be created using_ `tf.data.Dataset.list_files`, _and using a wildcard character_ `'*.jpg'`_. Make sure that you set_ `shuffle=False` _when calling this method._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def load_dataset(split, batch_size, shuffle_buffer=None, image_dir=str(Path('data', 'images'))):\n",
    "    \"\"\"\n",
    "    This function should create a tf.data.Dataset object for one of the train/valid/test\n",
    "    splits, according to the above specification.\n",
    "    It should then return the Dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.list_files(str(Path(image_dir, split, '*.jpg')), shuffle=False)\n",
    "    \n",
    "    def load_image(filepath):\n",
    "        raw_img = tf.io.read_file(filepath) \n",
    "        img_tensor = tf.image.decode_jpeg(raw_img, channels=3)\n",
    "        img_tensor.set_shape((64, 64, 3))\n",
    "        img_tensor = tf.image.convert_image_dtype(img_tensor, tf.float32)\n",
    "        return img_tensor\n",
    "    \n",
    "    dataset = dataset.map(load_image)\n",
    "    if shuffle_buffer is not None:\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to obtain the train, valid and test Datasets\n",
    "\n",
    "train_ds = load_dataset('train', 32, shuffle_buffer=500)\n",
    "valid_ds = load_dataset('val', 32)\n",
    "test_ds = load_dataset('test', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples\n",
    "\n",
    "n_rows, n_cols = 4, 8\n",
    "f, axs = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
    "\n",
    "for img_batch in train_ds.take(1):\n",
    "    img_batch = ops.convert_to_numpy(img_batch)\n",
    "    for n, image in enumerate(img_batch):\n",
    "        i = n // n_cols\n",
    "        j = n % n_cols\n",
    "        axs[i, j].imshow(image)\n",
    "        axs[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the encoder network\n",
    "\n",
    "We will now define the encoder network as part of the VAE. The approximate posterior $q_\\phi(z\\mid x)$ defined by the encoder will be a diagonal Gaussian distribution. You should complete the following function to define the encoder network, according to the following specification:\n",
    "\n",
    "* The function takes the `latent_dim` as an argument\n",
    "* Use the functional API to define the model, which has the following layers:\n",
    "  * An Input layer that sets the input shape to `(64, 64, 3)`\n",
    "  * A Conv2D layer with 32 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Conv2D layer with 64 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Conv2D layer with 128 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Conv2D layer with 256 filters, 3x3 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Flatten layer\n",
    "  * Dense layer with no activation function, and the right number of units to parameterise the means and log variance of a diagonal Gaussian distribution of dimension `latent_dim`\n",
    "  * The resulting Tensor should be split into `z_mean` and `z_log_var` Tensors\n",
    "  * The encoder Model should output the mean and log variance Tensors in a list `[z_mean, z_log_var]`\n",
    "* The function should then return the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_encoder(latent_dim):\n",
    "    \"\"\"\n",
    "    This function should build a CNN encoder model according to the above specification. \n",
    "    The function takes latent_dim as an argument, which should be used to define the model.\n",
    "    Your function should return the encoder model.\n",
    "    \"\"\"\n",
    "    encoder_inputs = Input(shape=(64, 64, 3))\n",
    "    h = Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)  # (32, 32, 32)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Conv2D(64, 3, activation='relu', strides=2, padding='same')(h)  # (16, 16, 64)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Conv2D(128, 3, activation='relu', strides=2, padding='same')(h)  # (8, 8, 128)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Conv2D(256, 3, activation='relu', strides=2, padding='same')(h)  # (4, 4, 256)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Flatten()(h)\n",
    "    h = Dense(2 * latent_dim)(h)\n",
    "    z_mean, z_log_var = ops.split(h, 2, axis=-1)\n",
    "    return Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var], name='encoder')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run your function to get the encoder\n",
    "\n",
    "encoder = get_encoder(latent_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the encoder summary\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the decoder network\n",
    "\n",
    "You should now define the decoder network for the VAE, using the functional API. This should be a neural network that returns a logits Tensor of shape `(64, 64, 3)` that will be used to parameterise independent Bernoulli distributions per pixel and colour channel.\n",
    "\n",
    "* The function takes the `latent_dim` as an argument\n",
    "* Use the functional API to define the model with the following layers:\n",
    "  * An Input layer that sets the input shape to `(latent_dim,)`\n",
    "  * A Dense layer with 4096 units and ReLU activation\n",
    "  * A Reshape layer, that reshapes its input to `(4, 4, 256)`\n",
    "  * BatchNormalization layer\n",
    "  * Conv2DTranspose layer with 128 filters, 3x3 kernel size, ReLU activation, stride of 2x2 and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Conv2DTranspose layer with 64 filters, 3x3 kernel size, ReLU activation, stride of 2x2 and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Conv2DTranspose layer with 32 filters, 3x3 kernel size, ReLU activation, stride of 2x2 and 'SAME' padding\n",
    "  * BatchNormalization layer\n",
    "  * Conv2DTranspose layer with 3 filters, 3x3 kernel size, no activation function, stride of 2x2 and 'SAME' padding\n",
    "* The Conv2DTranspose layers will need to be configured such that the final Conv2DTranspose layer outputs a Tensor of shape `(64, 64, 3)` in the final layer. You should pass in the `output_padding` argument to each of these layers.\n",
    "* The function should then return the decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_decoder(latent_dim):\n",
    "    \"\"\"\n",
    "    This function should build a CNN decoder model according to the above specification. \n",
    "    The function takes latent_dim as an argument, which should be used to define the model.\n",
    "    Your function should return the decoder model.\n",
    "    \"\"\"\n",
    "    event_shape = (64, 64, 3)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(latent_dim,))\n",
    "    h = Dense(4096, activation='relu')(decoder_inputs)\n",
    "    h = Reshape((4, 4, 256))(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same', output_padding=1)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same', output_padding=1)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same', output_padding=1)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    decoder_outputs = Conv2DTranspose(3, 3, strides=2, padding='same', output_padding=1)(h)\n",
    "\n",
    "    return Model(inputs=decoder_inputs, outputs=decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the decoder\n",
    "\n",
    "decoder = get_decoder(latent_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the decoder summary\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the end-to-end architecture\n",
    "\n",
    "Now that the encoder and decoder networks are defined, you should now complete the following `CelebAVAE` class to build the complete encoder-decoder architecture. \n",
    "\n",
    "* The `CelebAVAE` class subclasses from the base `Model` class\n",
    "* The function takes the `encoder` and `decoder` networks as arguments\n",
    "* You should complete the `_get_losses` method\n",
    "* The `_get_losses` method should compute and return the loss, KL divergence loss and negative log-likelihood loss as a tuple `(loss, kl_loss, nll_loss)`\n",
    "  * The prior distribution $p_\\theta(z)$ should be a zero-mean, isotropic Gaussian with identity covariance matrix\n",
    "  * You should use the following form of the SGVB estimator with $L=3$:\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x \\mid z^{(j)}) + \\log p_\\theta(z^{(j)}) − \\log q_\\phi(z^{(j)}|x)\n",
    "$$\n",
    "where $z^{(j)} = g_\\phi(\\epsilon^{(j)}, x)$, $\\epsilon^{(j)}\\sim p(\\epsilon)$ and $p(\\epsilon) = N(\\mathbf{0}, \\mathbf{I})$\n",
    "* The `train_step` method is completed for you. It computes the losses, perform the gradient update and update the metrics\n",
    "* The `test_step` method is completed for you. It computes the losses and update the metrics\n",
    "* The `call` method is completed for you. It passes a batch of inputs through the end-to-end encoder-decoder architecture. It uses a single Monte Carlo sample to evaluate the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "class CelebAVAE(Model):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_metric = Mean(name='loss')\n",
    "        self.nll_metric = Mean(name='nll')\n",
    "        self.kl_metric = Mean(name='kl')\n",
    "        self.pi = ops.array(np.pi)\n",
    "    \n",
    "    def _get_losses(self, data):\n",
    "        \"\"\"\n",
    "        This method should compute and return the loss, kl_loss and nll_loss.\n",
    "        It should use 3 Monte Carlo samples and the first form of the SGVB estimator.\n",
    "        \"\"\"\n",
    "        z_mean, z_log_var = self.encoder(data)\n",
    "        batch_size, latent_dim = ops.shape(z_mean)[0], ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal((3, batch_size, latent_dim))\n",
    "        z_std = ops.exp(0.5 * z_log_var)\n",
    "        posterior_samples = z_mean + (z_std * epsilon)  # (3, B, L)\n",
    "\n",
    "        log_Z = 0.5 * ops.log(2 * self.pi)\n",
    "        prior_log_prob = -0.5 * ops.square(posterior_samples) - log_Z\n",
    "        prior_log_prob = ops.mean(ops.sum(prior_log_prob, axis=-1))\n",
    "        \n",
    "        posterior_log_prob = -0.5 * ops.square((posterior_samples - z_mean) / z_std) - ops.log(z_std) - log_Z\n",
    "        posterior_log_prob = ops.mean(ops.sum(posterior_log_prob, axis=-1))\n",
    "        \n",
    "        kl_loss = posterior_log_prob - prior_log_prob\n",
    "        \n",
    "        posterior_samples = ops.reshape(posterior_samples, (3 * batch_size, latent_dim))\n",
    "        \n",
    "        x_logits = self.decoder(posterior_samples)  # (3*B, 64, 64, 3)\n",
    "        x_logits = ops.reshape(x_logits, (3, batch_size, 64, 64, 3, 1))\n",
    "        data = ops.reshape(data, (1, batch_size, 64, 64, 3, 1))\n",
    "        data = ops.repeat(data, 3, axis=0)\n",
    "        nll_loss = keras.losses.binary_crossentropy(data, x_logits, from_logits=True)  # (3, B, 64, 64, 3)\n",
    "        nll_loss = ops.mean(ops.sum(nll_loss, axis=[-1, -2, -3]))\n",
    "\n",
    "        loss = kl_loss + nll_loss\n",
    "        return loss, kl_loss, nll_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if keras.config.backend() == 'tensorflow':\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "            grads = tape.gradient(loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        else:\n",
    "            assert keras.config.backend() == 'torch'\n",
    "            self.zero_grad()\n",
    "            loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gradients = [v.value.grad for v in self.trainable_weights]    \n",
    "            with torch.no_grad():\n",
    "                self.optimizer.apply(gradients, self.trainable_weights)\n",
    "            \n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.nll_metric.update_state(nll_loss)\n",
    "        self.kl_metric.update_state(kl_loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.nll_metric.update_state(nll_loss)\n",
    "        self.kl_metric.update_state(kl_loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_std = self.encoder(inputs)\n",
    "        epsilon = keras.random.normal(ops.shape(z_mean))\n",
    "        z_std = ops.exp(z_log_std)\n",
    "        z_sample = z_mean + (z_std * epsilon)\n",
    "        return self.decoder(z_sample)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric, self.nll_metric, self.kl_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to define and compile the end-to-end architecture\n",
    "\n",
    "vae = CelebAVAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n",
    "vae.fit(train_ds, validation_data=valid_ds, epochs=40, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "vae.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute reconstructions of test images\n",
    "\n",
    "We will now take a look at some image reconstructions from the encoder-decoder architecture.\n",
    "\n",
    "You should complete the following function, that uses `encoder` and `decoder` to reconstruct images from the test dataset. \n",
    "\n",
    "* This function takes the `encoder`, `decoder` and a Tensor `batch_of_images` as arguments\n",
    "* It should then compute the reconstructions as follows:\n",
    "  * Compute the means of the encoding distributions from passing the batch of images into the encoder\n",
    "  * Pass these latent vectors through the decoder to get the Bernoulli distribution probabilities\n",
    "* The function should then return the resulting Tensor, which will be of shape `(batch_size, 64, 64, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def reconstruct(encoder, decoder, batch_of_images):\n",
    "    \"\"\"\n",
    "    This function should compute reconstructions of the batch_of_images according\n",
    "    to the above instructions.\n",
    "    The function takes the encoder, decoder and batch_of_images as inputs, which\n",
    "    should be used to compute the reconstructions.\n",
    "    The function should then return the reconstructions Tensor.\n",
    "    \"\"\"\n",
    "    mean_latent_vectors, _ = encoder(batch_of_images)\n",
    "    mean_reconstructions = ops.sigmoid(decoder(mean_latent_vectors))\n",
    "    return mean_reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to compute and visualise reconstructions\n",
    "\n",
    "for test_batch in test_ds.shuffle(100).take(1):\n",
    "    reconstructions = reconstruct(encoder, decoder, test_batch)\n",
    "\n",
    "test_batch_size = 8\n",
    "f, axs = plt.subplots(2, test_batch_size, figsize=(16, 6))\n",
    "axs[0, 0].set_title(\"Original test images\", loc='left')\n",
    "axs[1, 0].set_title(\"Reconstructed images\", loc='left')\n",
    "for j in range(test_batch_size):\n",
    "    axs[0, j].imshow(ops.convert_to_numpy(test_batch)[j])\n",
    "    axs[1, j].imshow(ops.convert_to_numpy(reconstructions)[j])\n",
    "    axs[0, j].axis('off')\n",
    "    axs[1, j].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate images in the latent space\n",
    "\n",
    "In this final section, we will see how the latent space encodes high-level information about the images, even though it has not been trained with any information apart from the images themselves.\n",
    "\n",
    "As mentioned earlier, each image in the CelebA dataset is labelled according to the attributes of the person pictured. The cell below will load these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the attribute labels\n",
    "\n",
    "labels = pd.read_csv(Path('./data/list_attr_celeba_subset.csv'))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, each image is labelled with a binary indicator (1 true, -1 false), according to whether it posseses the attribute. The list of attributes contained in the `labels` DataFrame is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the attributes contained in the DataFrame\n",
    "\n",
    "labels.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to perform some computations in the latent space, depending on the attribute values in the `labels` DataFrame. To do this, we will construct a new TensorFlow Dataset object, containing the images and attribute information.\n",
    "\n",
    "You should now complete the following `get_labelled_dataset` function to construct this new Dataset.\n",
    "\n",
    "* The function takes the arguments `split` (which again will be one of the strings `'train'`, `'val'` or `'test'`), an `attribute` string, the `labels` DataFrame and `image_dir` string\n",
    "  * The `attribute` will be one of the column headers listed above\n",
    "* As before, the function should create a Dataset containing the filepaths saved in the corresponding `split` subfolder of the `image_dir` directory\n",
    "* The function should include a nested function used to map the Dataset similar to before\n",
    "  * It should again read the contents of the file, decode the jpeg and scale the pixel values to lie in the range $[0, 1]$\n",
    "  * It should then look up the `attribute` value for the image from the `labels` DataFrame\n",
    "  * It should return a tuple containing the image Tensor, and scalar `tf.int32` label Tensor\n",
    "* The function should then apply the nested function using the `map` method\n",
    "* The function should then return the Dataset\n",
    "\n",
    "_Hint: convert the filenames and attribute columns of the_ `labels` _DataFrame into separate Tensor objects for use in the map function. The_ `tf.strings.split` _and_ `tf.where` _functions will be useful to extract the label for a given image._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_labelled_dataset(split, attribute, labels=labels, image_dir=str(Path('data', 'images'))):\n",
    "    \"\"\"\n",
    "    This function should create a tf.data.Dataset object for one of the train/valid/test\n",
    "    splits, according to the above specification.\n",
    "    It should then return the Dataset.\n",
    "    \"\"\"\n",
    "    filenames = tf.constant(labels['image_id'])\n",
    "    labels = tf.constant(labels[attribute], dtype=tf.int32)\n",
    "    dataset = tf.data.Dataset.list_files(str(Path(image_dir, split, '*.jpg')), shuffle=False)\n",
    "    \n",
    "    def load_image(filepath):\n",
    "        filename = tf.strings.split(filepath, '/')[-1]\n",
    "        i = tf.where(filenames == filename)\n",
    "        raw_img = tf.io.read_file(filepath) \n",
    "        img_tensor = tf.image.decode_jpeg(raw_img, channels=3)\n",
    "        img_tensor = tf.image.convert_image_dtype(img_tensor, tf.float32)\n",
    "        label = labels[tf.squeeze(i)]\n",
    "        return img_tensor, label\n",
    "    \n",
    "    dataset = dataset.map(load_image)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the labelled Dataset from the train split\n",
    "\n",
    "labelled_train_ds = get_labelled_dataset('train', 'Eyeglasses', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would like to compute the 'attribute vector' for the chosen attribute. This will be the average latent vector corresponding to all images that have the attribute, minus the average latent vector corresponding to all images that do not have the attribute. The intuition is that this vector will correspond the high-level property of adding the attribute to an image.\n",
    "\n",
    "You should now complete the following function to compute the attribute vector.\n",
    "\n",
    "* The function takes `labelled_dataset` as an argument, as well as the `encoder` network\n",
    "* The function should compute the encoding distribution mean (latent vector) for all images that have the attribute, and (separately) all the images that do not\n",
    "* It should then compute the average of each of these two sets of latent vectors\n",
    "* It should then compute `avg_latent_with_attribute - avg_latent_without_attribute`. This is the attribute vector\n",
    "* The function should then return the attribute vector as a numpy array of shape `(latent_dim,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_attribute_vector(labelled_dataset, encoder):\n",
    "    \"\"\"\n",
    "    This function should compute and return the attribute vector according \n",
    "    to the above specification.\n",
    "    \"\"\"\n",
    "    latent_dim = encoder.outputs[0].shape[1]\n",
    "    with_attribute_ds = labelled_dataset.filter(lambda i, l: tf.math.equal(l, 1)).batch(128)\n",
    "    without_attribute_ds = labelled_dataset.filter(lambda i, l: tf.math.equal(l, -1)).batch(128)\n",
    "    \n",
    "    avg_latent_with_attribute = np.empty(shape=(0, latent_dim), dtype=np.float32)\n",
    "    for images, _ in with_attribute_ds:\n",
    "        latents, _  = encoder(images)\n",
    "        avg_latent_with_attribute = np.concatenate((avg_latent_with_attribute, ops.convert_to_numpy(latents)), \n",
    "                                                   axis=0)\n",
    "    avg_latent_with_attribute = np.mean(avg_latent_with_attribute, axis=0)    \n",
    "    \n",
    "    avg_latent_without_attribute = np.empty(shape=(0, latent_dim), dtype=np.float32)\n",
    "    for images, _ in without_attribute_ds:\n",
    "        latents, _ = encoder(images)\n",
    "        avg_latent_without_attribute = np.concatenate((avg_latent_without_attribute, ops.convert_to_numpy(latents)), \n",
    "                                                      axis=0)\n",
    "    avg_latent_without_attribute = np.mean(avg_latent_without_attribute, axis=0)    \n",
    "    \n",
    "    return avg_latent_with_attribute - avg_latent_without_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attribute vector using your function\n",
    "\n",
    "attribute_vector = get_attribute_vector(labelled_train_ds, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view this attribute vector by decoding it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the decoded attribute vector\n",
    "\n",
    "decoded_a = ops.sigmoid(decoder(attribute_vector[np.newaxis, ...]))\n",
    "plt.imshow(ops.convert_to_numpy(decoded_a).squeeze())\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the attribute vector to add the attribute to an image reconstruction, where that attribute wasn't present before. To do this, we can just add the attribute vector to the latent vector encoding of the image, and then decode the result. We can also adjust the strength of the attribute vector by scaling with a multiplicative parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the attribute vector to a sample of images that don't have the attribute\n",
    "\n",
    "k = 2.5  # Weighting of attribute vector\n",
    "num_examples = 8\n",
    "labelled_test_ds = get_labelled_dataset('test', 'Eyeglasses', labels=labels).shuffle(100)\n",
    "images_without_attribute = []\n",
    "reconstructions = []\n",
    "modified_images = []\n",
    "for image, label in labelled_test_ds:\n",
    "    if label == 1:  # Only proceses images without the attribute\n",
    "        continue\n",
    "    else:\n",
    "        images_without_attribute.append(ops.convert_to_numpy(image))\n",
    "        encoding, _ = encoder(image[tf.newaxis, ...])\n",
    "        encoding = ops.convert_to_numpy(encoding)\n",
    "        decoded_image = ops.sigmoid(decoder(encoding))\n",
    "        reconstructions.append(np.squeeze(ops.convert_to_numpy(decoded_image)))\n",
    "        modified_encoding = encoding + (k * attribute_vector)\n",
    "        modified_reconstruction = ops.sigmoid(decoder(modified_encoding))\n",
    "        modified_images.append(np.squeeze(ops.convert_to_numpy(modified_reconstruction)))\n",
    "    if len(modified_images) >= num_examples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original images, their reconstructions, and modified reconstructions\n",
    "\n",
    "num_examples = 8\n",
    "f, axs = plt.subplots(3, num_examples, figsize=(16, 6))\n",
    "axs[0, 0].set_title(\"Original images\", loc='left')\n",
    "axs[1, 0].set_title(\"Reconstructed images\", loc='left')\n",
    "axs[2, 0].set_title(\"Images with added attribute\", loc='left')\n",
    "for j in range(num_examples):\n",
    "    axs[0, j].imshow(images_without_attribute[j])\n",
    "    axs[1, j].imshow(reconstructions[j])\n",
    "    axs[2, j].imshow(modified_images[j])\n",
    "    for ax in axs[:, j]: ax.axis('off')\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also try removing the attribute from images that possess the attribute, or experiment with a different attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! In this assignment you have developed the variational autoencoder algorithm for the CelebA dataset, and used the trained networks to compute reconstructions and modify dataset images with high-level semantic information extracted from the latent space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

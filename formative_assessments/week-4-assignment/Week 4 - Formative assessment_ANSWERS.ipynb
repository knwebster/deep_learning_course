{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 4: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement a binary classifier model in Keras. You will experiment with different sized models, datasets and regularisation techniques to validate your model and combat overfitting.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/lhc.jpg\" title=\"Large Hadron Collider\" style=\"width: 600px;\"/></center>\n",
    "<center><font style=\"font-size:12px\">source: flickr/Image Editor <a href=http://www.flickr.com/>http://www.flickr.com/</a></font></center>\n",
    "\n",
    "#### The HIGGS dataset\n",
    "In this formative assessment, you will use the [HIGGS dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS) from the UCI Machine Learning Repository. This dataset contains kinematic properties measured by the particle detectors in the accelerator, and a binary class label that distinguishes between a signal process which produces Higgs bosons and a background process which does not. For more information see the UCI website or the original paper:\n",
    "\n",
    "* Baldi, P., Sadowski, P. & Whiteson, D. (2014), \"Searching for Exotic Particles in High-energy Physics with Deep Learning\", *Nature Communications* **5** 4308.\n",
    "\n",
    "The full dataset contains 11,000,000 examples. We will be working with a small subset of the data in this assignment. Your goal is to develop a classifier to predict the presence of Higgs bosons using MLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare the data\n",
    "For this assignment, you are provided with a subset of the HIGGS dataset. Note that the full dataset can be downloaded from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00280/), but it is not necessary to download it for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and describe the data\n",
    "\n",
    "df = pd.read_csv(Path(\"./data/HIGGS-sample.csv\"), header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of the data\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the binary label, and the remaining columns are the features. \n",
    "\n",
    "In this assignment, we will use TensorFlow Datasets. You should now complete the following function to build training, validation and test Datasets, according to the following specifications:\n",
    "\n",
    "* Create a random train/validation/test data partition with a 80/10/10 percentage split\n",
    "* Your function should be able to operate on a numeric `DataFrame` of any shape\n",
    "* Load the separate splits into separate `tf.data.Dataset` objects\n",
    "* Each Dataset should have an `element_spec` containing a single Tensor (of type `float32`) that represents an entire row of the CSV file\n",
    "* The function should then return the tuple of `tf.data.Dataset` objects `(train_ds, valid_ds, test_ds)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_datasets(dataframe):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame, and builds training, validation\n",
    "    and test Dataset objects as described above.\n",
    "    Your function should return a tuple (train_ds, valid_ds, test_ds) of Datasets.\n",
    "    \"\"\"\n",
    "    dataset_size = dataframe.shape[0]\n",
    "    df = dataframe.sample(dataset_size)\n",
    "    num_train = int(dataset_size * 0.8)\n",
    "    num_valid = int(dataset_size * 0.1)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(df[:num_train].values.astype(np.float32))\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices(df[num_train:num_train + num_valid].values.astype(np.float32))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(df[num_train + num_valid:].values.astype(np.float32))\n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to create the Datasets\n",
    "\n",
    "train_ds, valid_ds, test_ds = get_datasets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Dataset element_spec\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now further process the Datasets, ready for training. The following functions will shuffle and batch the Datasets, and extract the input features and targets. \n",
    "\n",
    "First you should complete the following function to shuffle and batch the Datasets.\n",
    "\n",
    "* The function takes `dataset` (a `tf.data.Dataset` object), `batch_size` and `shuffle_buffer` as inputs\n",
    "* If `shuffle_buffer` is `None` (the default), then the Dataset should not be shuffled\n",
    "* If `shuffle_buffer` is an integer, then it should be used to shuffle the Dataset\n",
    "* The function should then batch the Dataset using `batch_size`\n",
    "* Your function should then return the (maybe) shuffled and batched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def shuffle_and_batch_dataset(dataset, batch_size, shuffle_buffer=None):\n",
    "    \"\"\"\n",
    "    This function is used to shuffle and batch the dataset, using shuffle_buffer\n",
    "    and batch_size.\n",
    "    Your function should return the shuffled and batched Dataset.\n",
    "    \"\"\"\n",
    "    if shuffle_buffer is not None:\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to shuffle and batch the Datasets\n",
    "\n",
    "train_ds = shuffle_and_batch_dataset(train_ds, 500, shuffle_buffer=1000)\n",
    "valid_ds = shuffle_and_batch_dataset(valid_ds, 500)\n",
    "test_ds = shuffle_and_batch_dataset(test_ds, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `map_dataset` function should now extract the input features and targets.\n",
    "\n",
    "Inside this function you should define an auxiliary function that you will use with the `map` method of the Dataset object. This auxiliary function should take the Tensor (as in the element_spec of the shuffled and batched Dataset), and return a tuple of two elements, with the input features in the first element, and the binary label in the second element. \n",
    "\n",
    "* The function takes `dataset` as an input (a `tf.data.Dataset` object)\n",
    "* The function should define an inner function to extract the inputs and targets\n",
    "* The inner function should be used to `map` over the Dataset\n",
    "* The `map_dataset` should then return the mapped Dataset\n",
    "* The resulting `element_spec` of the mapped Dataset should be a 2-tuple where the elements have shape `(batch_size, 28)` and `(batch_size, 1)` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def map_dataset(dataset):\n",
    "    \"\"\"\n",
    "    This function is used to map over the Dataset object to extract the input features \n",
    "    and target variable. The function takes a Dataset object, and maps over the \n",
    "    Dataset to create input features and targets.\n",
    "    Your function should return the mapped Dataset.\n",
    "    \"\"\"\n",
    "    def extract_inputs_and_targets(batch_of_features):\n",
    "        features = batch_of_features[..., 1:]\n",
    "        targets = batch_of_features[..., :1]\n",
    "        return features, targets\n",
    "    return dataset.map(extract_inputs_and_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to map over the Datasets\n",
    "\n",
    "train_ds = map_dataset(train_ds)\n",
    "valid_ds = map_dataset(valid_ds)\n",
    "test_ds = map_dataset(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetch the Datasets\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Dataset element_spec\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train the small MLP model\n",
    "\n",
    "You should now complete the following function to build an MLP classifier. We will experiment with different sized models, so this function needs to be able to build an MLP with different numbers of layers and units. \n",
    "\n",
    "The function should use the Sequential API, and build the model according to the following specifications:\n",
    "\n",
    "* The function has `input_shape` and `hidden_units` arguments\n",
    "* The `input_shape` should be used to define the Input layer\n",
    "* The `hidden_units` argument is a list of integers (of any length), containing the number of units to use in subsequent `Dense` hidden layers\n",
    "* Each `Dense` hidden layer should use a `selu` activation function\n",
    "* There should also be a final output `Dense` layer with one unit and a linear (no) activation\n",
    "* The function should then return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_mlp(input_shape, hidden_units):\n",
    "    \"\"\"\n",
    "    This function is used to build the MLP model. It takes input_shape and hidden_units\n",
    "    as arguments, which should be used to build the model as described above.\n",
    "    Your function should return the model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "    ])\n",
    "    for units in hidden_units:\n",
    "        model.add(Dense(units, activation='selu'))\n",
    "    model.add(Dense(1, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the first (small) MLP\n",
    "\n",
    "model = get_mlp(input_shape=(28,), hidden_units=[16, 16])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines the optimizer, loss function and metrics to use to compile the model. \n",
    "\n",
    "* The function should create an Adam optimizer object from the `keras.optimizers` module, with learning rate 0.0005\n",
    "* It should also create an instance of the binary cross entropy loss from the `keras.losses` module, with the option `from_logits=True`, as the final layer of our model has a linear activation\n",
    "* It should also create a binary accuracy metric object from the `keras.metrics` module, with the default settings\n",
    "* The function should then return a tuple of the three objects `(optimizer, loss, metric)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_metrics():\n",
    "    \"\"\"\n",
    "    This function is used to create the optimizer, loss, metric objects. \n",
    "    Each of these should be created as instances from the corresponding classes in the\n",
    "    optimizers, losses and metrics modules respectively, with the options as above.\n",
    "    The function should then return the tuple (optimizer, loss, metric)\n",
    "    \"\"\"\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    acc = keras.metrics.BinaryAccuracy()\n",
    "    return opt, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines the `EarlyStopping` and `ModelCheckpoint` callbacks used to fit the model.\n",
    "\n",
    "* The function takes a single input argument, `filepath`\n",
    "* It should create the following callbacks:\n",
    "  * An `EarlyStopping` callback, with patience set to 200\n",
    "  * A `ModelCheckpoint` callback that saves the best model only (according to the validation loss), and saves weights only, using the filename `filepath`\n",
    "* The function should then return a tuple of the two callback objects `(earlystopping, modelckpt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_callbacks(filepath):\n",
    "    \"\"\"\n",
    "    This function is used to create the callback objects. \n",
    "    Each of these should be created as instances from the corresponding classes in the\n",
    "    callbacks modules respectively, with the options as above.\n",
    "    The function should then return the tuple (earlystopping, modelckpt)\n",
    "    \"\"\"\n",
    "    earlystopping = keras.callbacks.EarlyStopping(patience=200)\n",
    "    modelckpt = keras.callbacks.ModelCheckpoint(filepath, save_best_only=True, \n",
    "                                                save_weights_only=True, monitor=\"val_loss\")\n",
    "    return earlystopping, modelckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required Keras extension when saving model weights only is `.weights.h5` (for a full Keras model, it is `.keras`). The filepath can be passed into the `ModelCheckpoint` initialiser as a string or a `pathlib.Path` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your functions to get the optimizer, loss, metric and callbacks\n",
    "\n",
    "adam, bce_loss, bin_acc = get_metrics()\n",
    "early_stopping, ckpt = get_callbacks(Path(\"./models/small.weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to complete the following function to compile and fit the model.\n",
    "\n",
    "* The function takes `model`, `optimizer`, `loss`, `num_epochs`, `train_dataset`, `validation_dataset`, `metrics` and `callbacks` arguments\n",
    "* It should compile the `model` using the `optimizer`, `loss` and `metrics` list\n",
    "* It should then fit the `model` using the `train_dataset`, `validation_dataset`, `num_epochs` arguments and `callbacks` list\n",
    "* The `fit` method should be passed `verbose=0`, as there will be many epochs\n",
    "* The function should then return the `History` object returned by the `fit` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def compile_and_fit(model, optimizer, loss, num_epochs, train_dataset, \n",
    "                    validation_dataset=None, metrics=None, callbacks=None):\n",
    "    \"\"\"\n",
    "    This function should compile and fit the model according to the above specifications.\n",
    "    It should then return the History object returned by the fit method\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    history = model.fit(train_dataset, epochs=num_epochs, verbose=0,\n",
    "                        validation_data=validation_dataset, callbacks=callbacks)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the small MLP model\n",
    "\n",
    "small_history = compile_and_fit(model, adam, bce_loss, 2000, train_ds, \n",
    "                                validation_dataset=valid_ds, metrics=[bin_acc],\n",
    "                                callbacks=[early_stopping, ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.plot(small_history.history['loss'], label='train', color='C0', linestyle='-')\n",
    "plt.plot(small_history.history['val_loss'], label='valid', color='C0', linestyle=':')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(small_history.history['binary_accuracy'], label='train', color='C0', linestyle='-')\n",
    "plt.plot(small_history.history['val_binary_accuracy'], label='valid', color='C0', linestyle=':')\n",
    "plt.title(\"Binary accuracy vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train the medium and large MLP models\n",
    "We will now see if we can improve the model performance by increasing the model capacity. We will reuse the `get_mlp` function to build these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a medium-sized MLP model\n",
    "\n",
    "model = get_mlp(input_shape=(28,), hidden_units=[64, 64, 64])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fresh compile and fit arguments\n",
    "\n",
    "adam, bce_loss, bin_acc = get_metrics()\n",
    "early_stopping, ckpt = get_callbacks(Path(\"./models/medium.weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have silenced the printout from the `fit` method, we will create a custom callback to print the model progress less frequently than every epoch.\n",
    "\n",
    "You should now complete the following class, which subclasses the base `Callback` class.\n",
    "\n",
    "* The class initialiser takes one required argument, `num_epochs`, that defines the frequency to print logs\n",
    "* After every `num_epochs` epochs of training, the class should print out a single line with the epoch number, training and validation loss and metric values\n",
    "  * Make sure to account for the zero-indexing of python, e.g. the first epoch is numbered 0, so your class should print `epoch + 1`\n",
    "* The loss and metric values should be printed to 4 decimal places (_hint: use_ `f\"{value:.4f}\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following class. \n",
    "# Make sure to not change the class name or provided methods and signatures.\n",
    "\n",
    "class PrintProgress(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, num_epochs, **kwargs):\n",
    "        \"\"\"\n",
    "        The initializer should call the base class initializer, passing in any \n",
    "        optional keyword arguments passed in\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.num_epochs == 0:\n",
    "            loss_and_metrics = ', '.join([f'{k}: {v:.4f}' for k, v in logs.items()])\n",
    "            print(f\"Epoch: {epoch + 1}, {loss_and_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your callback class\n",
    "\n",
    "print_progress = PrintProgress(num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the medium MLP model\n",
    "\n",
    "medium_history = compile_and_fit(model, adam, bce_loss, 2000, train_ds, \n",
    "                                 validation_dataset=valid_ds, metrics=[bin_acc],\n",
    "                                 callbacks=[early_stopping, print_progress, ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will also build and train a large MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a large-sized MLP model\n",
    "\n",
    "model = get_mlp(input_shape=(28,), hidden_units=[512, 512, 512, 512])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fresh compile and fit arguments\n",
    "\n",
    "adam, bce_loss, bin_acc = get_metrics()\n",
    "early_stopping, ckpt = get_callbacks(Path(\"./models/large.weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the large MLP model\n",
    "\n",
    "large_history = compile_and_fit(model, adam, bce_loss, 2000, train_ds, \n",
    "                                validation_dataset=valid_ds, metrics=[bin_acc],\n",
    "                                callbacks=[early_stopping, print_progress, ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the performance of each model by plotting the training and validation loss and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for all models\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.plot(small_history.history['loss'], label='small (train)', color='C0', linestyle='-')\n",
    "plt.plot(small_history.history['val_loss'], label='small (valid)', color='C0', linestyle=':')\n",
    "plt.plot(medium_history.history['loss'], label='medium (train)', color='C1', linestyle='-')\n",
    "plt.plot(medium_history.history['val_loss'], label='medium (valid)', color='C1', linestyle=':')\n",
    "plt.plot(large_history.history['loss'], label='large (train)', color='C2', linestyle='-')\n",
    "plt.plot(large_history.history['val_loss'], label='large (valid)', color='C2', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(small_history.history['binary_accuracy'], label='small (train)', color='C0', linestyle='-')\n",
    "plt.plot(small_history.history['val_binary_accuracy'], label='small (valid)', color='C0', linestyle=':')\n",
    "plt.plot(medium_history.history['binary_accuracy'], label='medium (train)', color='C1', linestyle='-')\n",
    "plt.plot(medium_history.history['val_binary_accuracy'], label='medium (valid)', color='C1', linestyle=':')\n",
    "plt.plot(large_history.history['binary_accuracy'], label='large (train)', color='C2', linestyle='-')\n",
    "plt.plot(large_history.history['val_binary_accuracy'], label='large (valid)', color='C2', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Binary accuracy vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularise the large model\n",
    "As we can see clearly in the above plots, the large model achieves a low loss and high accuracy, but severely overfits the training data. We will now look to regularise this model.\n",
    "\n",
    "First, you should write a new function, `get_regularised_mlp`, to build regularised MLP models.\n",
    "\n",
    "The function should use the Sequential API, and build the model according to the following specifications:\n",
    "\n",
    "* The function has `input_shape`, `hidden_units`, `l2_reg_coeff` and `dropout_rate` arguments\n",
    "* The `input_shape` should be used to define the `Input` layer\n",
    "* The `hidden_units` argument is a list of integers (of any length), containing the number of units to use in subsequent `Dense` hidden layers\n",
    "* Each `Dense` hidden layer should use a `selu` activation function\n",
    "* Each `Dense` layer should use the `l2_reg_coeff` argument to set kernel $l^2$ regularisation\n",
    "* After each `Dense` hidden layer, there should be a `Dropout` layer with rate `dropout_rate`\n",
    "* There should also be a final output `Dense` layer with one unit and a linear (no) activation\n",
    "* The function should then return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_regularised_mlp(input_shape, hidden_units, l2_reg_coeff, dropout_rate):\n",
    "    \"\"\"\n",
    "    This function is used to build the MLP model. It takes input_shape and hidden_units\n",
    "    as arguments, which should be used to build the model as described above.\n",
    "    Your function should return the model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "    ])\n",
    "    for units in hidden_units:\n",
    "        model.add(Dense(units, activation='selu', kernel_regularizer=keras.regularizers.l2(l2_reg_coeff)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a regularised version of the large MLP model\n",
    "\n",
    "model = get_regularised_mlp(input_shape=(28,), hidden_units=[512, 512, 512, 512],\n",
    "                            l2_reg_coeff=0.0001, dropout_rate=0.5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fresh compile and fit arguments\n",
    "\n",
    "adam, bce_loss, bin_acc = get_metrics()\n",
    "early_stopping, ckpt = get_callbacks(Path(\"./models/reg_large.weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the regularised large MLP model\n",
    "\n",
    "reg_large_history = compile_and_fit(model, adam, bce_loss, 2000, train_ds, \n",
    "                                    validation_dataset=valid_ds, metrics=[bin_acc],\n",
    "                                    callbacks=[early_stopping, print_progress, ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for all models\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.plot(small_history.history['loss'], label='small (train)', color='C0', linestyle='-')\n",
    "plt.plot(small_history.history['val_loss'], label='small (valid)', color='C0', linestyle=':')\n",
    "plt.plot(medium_history.history['loss'], label='medium (train)', color='C1', linestyle='-')\n",
    "plt.plot(medium_history.history['val_loss'], label='medium (valid)', color='C1', linestyle=':')\n",
    "plt.plot(large_history.history['loss'], label='large (train)', color='C2', linestyle='-')\n",
    "plt.plot(large_history.history['val_loss'], label='large (valid)', color='C2', linestyle=':')\n",
    "plt.plot(reg_large_history.history['loss'], label='reg large (train)', color='C3', linestyle='-')\n",
    "plt.plot(reg_large_history.history['val_loss'], label='reg large (valid)', color='C3', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(small_history.history['binary_accuracy'], label='small (train)', color='C0', linestyle='-')\n",
    "plt.plot(small_history.history['val_binary_accuracy'], label='small (valid)', color='C0', linestyle=':')\n",
    "plt.plot(medium_history.history['binary_accuracy'], label='medium (train)', color='C1', linestyle='-')\n",
    "plt.plot(medium_history.history['val_binary_accuracy'], label='medium (valid)', color='C1', linestyle=':')\n",
    "plt.plot(large_history.history['binary_accuracy'], label='large (train)', color='C2', linestyle='-')\n",
    "plt.plot(large_history.history['val_binary_accuracy'], label='large (valid)', color='C2', linestyle=':')\n",
    "plt.plot(reg_large_history.history['binary_accuracy'], label='reg large (train)', color='C3', linestyle='-')\n",
    "plt.plot(reg_large_history.history['val_binary_accuracy'], label='reg large (valid)', color='C3', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Binary accuracy vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the regularisation has helped to prevent overfitting in the large model.\n",
    "\n",
    "#### Regularise with more data\n",
    "Finally, we will demonstrate the regularising effect of more data. We will significantly increase the capacity of the network by making it wider and deeper.\n",
    "\n",
    "You should now write the following function to build this model. The function should again use the Sequential API, and build the model according to the following specifications:\n",
    "\n",
    "* The function has `input_shape`, `hidden_units`, `l2_reg_coeff` and `dropout_rate` arguments\n",
    "* The `input_shape` should be used to define the `Input` layer\n",
    "* The `hidden_units` argument is a list of integers (of any length), containing the number of units to use in subsequent `Dense` hidden layers\n",
    "* Each `Dense` hidden layer should use a `selu` activation function\n",
    "* Each `Dense` layer should use the `l2_reg_coeff` argument to set kernel $l^2$ regularisation\n",
    "* After each `Dense` hidden layer, there should be a `BatchNormalization` layer\n",
    "* After each `BatchNormalization` layer, there should be a `Dropout` layer with rate `dropout_rate`\n",
    "* There should also be a final output `Dense` layer with one unit and a linear (no) activation\n",
    "* The function should then return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_regularised_bn_mlp(input_shape, hidden_units, l2_reg_coeff, dropout_rate):\n",
    "    \"\"\"\n",
    "    This function is used to build the MLP model. It takes input_shape and hidden_units\n",
    "    as arguments, which should be used to build the model as described above, using the\n",
    "    functional API.\n",
    "    Your function should return the model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "    ])\n",
    "    for units in hidden_units:\n",
    "        model.add(Dense(units, activation='selu', kernel_regularizer=keras.regularizers.l2(l2_reg_coeff)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a huge MLP model\n",
    "\n",
    "model = get_regularised_bn_mlp(input_shape=(28,), hidden_units=[1024, 1024, 1024, 512, 512, 512],\n",
    "                               l2_reg_coeff=0.0001, dropout_rate=0.5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fresh compile and fit arguments\n",
    "\n",
    "adam, bce_loss, bin_acc = get_metrics()\n",
    "early_stopping, ckpt = get_callbacks(Path(\"./models/huge.weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the huge MLP model\n",
    "\n",
    "huge_history = compile_and_fit(model, adam, bce_loss, 2000, train_ds, \n",
    "                               validation_dataset=valid_ds, metrics=[bin_acc],\n",
    "                               callbacks=[early_stopping, print_progress, ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance of the huge model and large regularised model\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.plot(reg_large_history.history['loss'], label='reg large (train)', color='C3', linestyle='-')\n",
    "plt.plot(reg_large_history.history['val_loss'], label='reg large (valid)', color='C3', linestyle=':')\n",
    "plt.plot(huge_history.history['loss'], label='huge (train)', color='C4', linestyle='-')\n",
    "plt.plot(huge_history.history['val_loss'], label='huge (valid)', color='C4', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(reg_large_history.history['binary_accuracy'], label='reg large (train)', color='C3', linestyle='-')\n",
    "plt.plot(reg_large_history.history['val_binary_accuracy'], label='reg large (valid)', color='C3', linestyle=':')\n",
    "plt.plot(huge_history.history['binary_accuracy'], label='huge (train)', color='C4', linestyle='-')\n",
    "plt.plot(huge_history.history['val_binary_accuracy'], label='huge (valid)', color='C4', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Binary accuracy vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this huge model is again overfitting to the training data. We will now retrain the same model on a much larger dataset to see the regularising effect of more data.\n",
    "\n",
    "There is an additional CSV saved at `'./data/HIGGS-sample-extra.csv'` that contains an extra 50,000 data examples. You should now complete the following function to construct training, validation and test datasets using this CSV.\n",
    "\n",
    "* The function takes the `csv_path`, `batch_size`, `map_dataset` function and `shuffle_buffer` size as arguments\n",
    "* Your function should read in the CSV to a pandas `DataFrame`, using the `csv_path`. Make sure to use the option `header=None`\n",
    "* Your function should then be able to operate on a numeric `DataFrame` of any shape\n",
    "* It should then randomly shuffle the `DataFrame` and as before, create a train/validation/test data partition with a 80/10/10 percentage split\n",
    "* It should load these data splits into `tf.data.Dataset` objects\n",
    "* It should shuffle the training Dataset using the `shuffle_buffer` size, if it is not `None`\n",
    "* It should then batch the training, validation and test Datasets using the `batch_size`\n",
    "* It should the use your `map_dataset` function as above to parse the data into input features and targets\n",
    "* Finally, it should then make a call to `prefetch`, with the argument `tf.data.AUTOTUNE` for each Dataset\n",
    "* Your function should return the tuple of Dataset objects `(train_ds, valid_ds, test_ds)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_more_data(csv_path, batch_size, map_dataset=map_dataset, shuffle_buffer=None):\n",
    "    \"\"\"\n",
    "    This function takes in the CSV filepath, batch_size, map_dataset function, and\n",
    "    shuffle_buffer size. It should create train/valid/test Datasets according to the\n",
    "    above specifications.\n",
    "    Your function should then return the tuple (train_ds, valid_ds, test_ds) of Datasets.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    dataset_size = df.shape[0]\n",
    "    df = df.sample(dataset_size)\n",
    "    \n",
    "    num_train = int(dataset_size * 0.8)\n",
    "    num_valid = int(dataset_size * 0.1)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(df[:num_train].values.astype(np.float32))\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices(df[num_train:num_train + num_valid].values.astype(np.float32))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(df[num_train + num_valid:].values.astype(np.float32))\n",
    "    \n",
    "    if shuffle_buffer is not None:\n",
    "        train_ds = train_ds.shuffle(shuffle_buffer)\n",
    "    \n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    valid_ds = valid_ds.batch(batch_size)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    \n",
    "    train_ds = map_dataset(train_ds)\n",
    "    valid_ds = map_dataset(valid_ds)\n",
    "    test_ds = map_dataset(test_ds)\n",
    "    \n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the extra Datasets\n",
    "\n",
    "train_ds_extra, valid_ds_extra, test_ds_extra = get_more_data(Path('./data/HIGGS-sample-extra.csv'), 500,\n",
    "                                                              map_dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the new Datasets with the existing ones\n",
    "\n",
    "train_ds_full = train_ds_extra.concatenate(train_ds)\n",
    "valid_ds_full = valid_ds_extra.concatenate(valid_ds)\n",
    "test_ds_full = test_ds_extra.concatenate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build another instance of the huge MLP model\n",
    "\n",
    "model = get_regularised_bn_mlp(input_shape=(28,), hidden_units=[1024, 1024, 1024, 512, 512, 512],\n",
    "                               l2_reg_coeff=0.0001, dropout_rate=0.5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fresh compile and fit arguments\n",
    "\n",
    "adam, bce_loss, bin_acc = get_metrics()\n",
    "early_stopping, ckpt = get_callbacks(Path(\"./models/huge_full.weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a very large model trained on a much bigger dataset, so will take some time to train - you might want to go make yourself a cup of tea or coffee while it's running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the huge MLP model on the expanded dataset\n",
    "\n",
    "huge_full_history = compile_and_fit(model, adam, bce_loss, 2000, train_ds_full, \n",
    "                                    validation_dataset=valid_ds_full, metrics=[bin_acc],\n",
    "                                    callbacks=[early_stopping, print_progress, ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance of the huge models and large regularised model\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.plot(reg_large_history.history['loss'], label='reg large (train)', color='C3', linestyle='-')\n",
    "plt.plot(reg_large_history.history['val_loss'], label='reg large (valid)', color='C3', linestyle=':')\n",
    "plt.plot(huge_history.history['loss'], label='huge (train)', color='C4', linestyle='-')\n",
    "plt.plot(huge_history.history['val_loss'], label='huge (valid)', color='C4', linestyle=':')\n",
    "plt.plot(huge_full_history.history['loss'], label='huge full (train)', color='C5', linestyle='-')\n",
    "plt.plot(huge_full_history.history['val_loss'], label='huge full (valid)', color='C5', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(reg_large_history.history['binary_accuracy'], label='reg large (train)', color='C3', linestyle='-')\n",
    "plt.plot(reg_large_history.history['val_binary_accuracy'], label='reg large (valid)', color='C3', linestyle=':')\n",
    "plt.plot(huge_history.history['binary_accuracy'], label='huge (train)', color='C4', linestyle='-')\n",
    "plt.plot(huge_history.history['val_binary_accuracy'], label='huge (valid)', color='C4', linestyle=':')\n",
    "plt.plot(huge_full_history.history['binary_accuracy'], label='huge full (train)', color='C5', linestyle='-')\n",
    "plt.plot(huge_full_history.history['val_binary_accuracy'], label='huge full (valid)', color='C5', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Binary accuracy vs epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conclude by evaluating each model on the held-out test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect evaluation loss and metrics for each model\n",
    "\n",
    "saved_models = {\n",
    "    'small': {\"build_fn\": get_mlp, \"args\": {\"input_shape\": (28,), \"hidden_units\": [16, 16]}},\n",
    "    'medium': {\"build_fn\": get_mlp, \"args\": {\"input_shape\": (28,), \"hidden_units\": [64, 64, 64]}},\n",
    "    'large': {\"build_fn\": get_mlp, \"args\": {\"input_shape\": (28,), \"hidden_units\": [512, 512, 512, 512]}},\n",
    "    'reg_large': {\"build_fn\": get_regularised_mlp, \"args\": {\n",
    "        \"input_shape\": (28,), \"hidden_units\": [512, 512, 512, 512], \n",
    "        \"l2_reg_coeff\": 0.0001, \"dropout_rate\": 0.5\n",
    "    }},\n",
    "    'huge': {\"build_fn\": get_regularised_bn_mlp, \"args\": {\n",
    "        \"input_shape\": (28,), \"hidden_units\": [1024, 1024, 1024, 512, 512, 512], \n",
    "        \"l2_reg_coeff\": 0.0001, \"dropout_rate\": 0.5\n",
    "    }},\n",
    "    'huge_full': {\"build_fn\": get_regularised_bn_mlp, \"args\": {\n",
    "        \"input_shape\": (28,), \"hidden_units\": [1024, 1024, 1024, 512, 512, 512], \n",
    "        \"l2_reg_coeff\": 0.0001, \"dropout_rate\": 0.5\n",
    "    }}\n",
    "}\n",
    "\n",
    "evaluation = {\"Model\": [], \"Test loss\": [], \"Test accuracy\": []}\n",
    "for model_size, options in saved_models.items():\n",
    "    model = options['build_fn'](**options['args'])\n",
    "    adam, bce_loss, bin_acc = get_metrics()\n",
    "    model.compile(loss=bce_loss, optimizer=None, metrics=[bin_acc])\n",
    "    model.load_weights(Path(f\"./models/{model_size}.weights.h5\"))\n",
    "    results = model.evaluate(test_ds_full, return_dict=True, verbose=0)\n",
    "    evaluation[\"Model\"].append(model_size)\n",
    "    evaluation[\"Test loss\"].append(results['loss'])\n",
    "    evaluation[\"Test accuracy\"].append(results['binary_accuracy'])\n",
    "    \n",
    "pd.DataFrame(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "! rm -r ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! In this assignment you have experimented with model capacity and various forms of regularisation, and seen their effects on the model training and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
